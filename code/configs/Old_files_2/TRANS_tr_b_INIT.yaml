name: "TRANS_tr_b_INIT"

data:
    src: "tr_b"
    trg: "en_b"
    train: "../2DL4NLP/all_data/tr_b.en_b/train.bpe"
    dev: "../2DL4NLP/all_data/tr_b.en_b/val.bpe"
    test: "../2DL4NLP/all_data/tr_b.en_b/test.bpe"
    level: "bpe"
    lowercase: False
    max_sent_length: 50                 # Luong
    src_voc_min_freq: 0
    src_voc_limit: 10000                # I took the minimum of the 3 papers
    trg_voc_min_freq: 0
    trg_voc_limit: 10000                # here as well
    src_vocab: "../2DL4NLP/all_data/tr_b.en_b/tr_b.en_b.vocab.txt"
    trg_vocab: "../2DL4NLP/all_data/tr_b.en_b/tr_b.en_b.vocab.txt"


testing:
    beam_size: 5
    alpha: 1.0

training:
    random_seed: 42
    optimizer: "adam"                    # all
    #learning_rate: 0.5                  # Zoph
    #learning_rate_min: 0.0000005
    weight_decay: 0.0
    #clip_grad_norm: 5.0                 # all
    batch_size: 80                      # Gulcehre
    scheduling: "plateau"
    patience: 1                         # Zoph
    #decrease_factor: 0.9                # Zoph
    early_stopping_metric: "eval_metric"
    epochs: 2                         # Zoph
    validation_freq: 1000
    logging_freq: 1000
    eval_metric: "bleu"
    model_dir: "models/TRANS_tr_b_INIT"
    overwrite: True
    shuffle: True
    use_cuda: True
    max_output_length: 60
    print_valid_sents: [0, 1, 2]
    keep_best_ckpts: 1
    save_latest_ckpt: True

model:
    initializer: "xavier"
    bias_initializer: "zeros"
    init_gain: 1.0
    embed_initializer: "xavier"
    embed_init_gain: 1.0
    tied_embeddings: True
    tied_softmax: True
    encoder:
        type: "transformer"
        num_layers: 6
        num_heads: 4             # TODO: Increase to 8 for larger data.
        embeddings:
            embedding_dim: 512   # TODO: Increase to 512 for larger data.
            scale: True
            dropout: 0.2
        # typically ff_size = 4 x hidden_size
        hidden_size: 512         # TODO: Increase to 512 for larger data.
        ff_size: 1024            # TODO: Increase to 2048 for larger data.
        dropout: 0.3
    decoder:
        type: "transformer"
        num_layers: 6
        num_heads: 4              # TODO: Increase to 8 for larger data.
        embeddings:
            embedding_dim: 512    # TODO: Increase to 512 for larger data.
            scale: True
            dropout: 0.2
        # typically ff_size = 4 x hidden_size
        hidden_size: 512         # TODO: Increase to 512 for larger data.
        ff_size: 1024            # TODO: Increase to 2048 for larger data.
        dropout: 0.3
