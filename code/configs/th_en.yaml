name: "th_en_model"

data:
    src: "th"
    trg: "en"
    train: "../2DL4NLP/all_data/train"
    dev: "../2DL4NLP/all_data/val"
    test: "../2DL4NLP/all_data/test"
    level: "bpe"
    lowercase: False
    max_sent_length: 50                 # Luong
    src_voc_min_freq: 0
    src_voc_limit: 10000                # I took the minimum of the 3 papers
    trg_voc_min_freq: 0
    trg_voc_limit: 10000                # here as well

testing:
    beam_size: 5
    alpha: 1.0

training:
    random_seed: 42
    optimizer: "sgd"                    # all
    learning_rate: 0.5                  # Zoph
    learning_rate_min: 0.0000005
    weight_decay: 0.0
    clip_grad_norm: 5.0                 # all
    batch_size: 80                      # Gulcehre
    scheduling: "plateau"
    patience: 1                         # Zoph
    decrease_factor: 0.9                # Zoph
    early_stopping_metric: "eval_metric"
    epochs: 100                         # Zoph
    validation_freq: 7362
    logging_freq: 1000
    eval_metric: "bleu"
    model_dir: "models/th_en_model"
    overwrite: True
    shuffle: True
    use_cuda: True
    max_output_length: 60
    print_valid_sents: [0, 1, 2]

model:
    encoder:
        rnn_type: "lstm"
        embeddings:
            embedding_dim: 620          # Gulcehre
            scale: False
        hidden_size: 1000               # Luong & Zoph
        bidirectional: True            # all
        dropout: 0.2
        num_layers: 1
    decoder:
        rnn_type: "lstm"
        embeddings:
            embedding_dim: 620          # uncertain
            scale: False
        emb_scale: False
        hidden_size: 1000               # all
        dropout: 0.2
        hidden_dropout: 0.2
        num_layers: 1
        input_feeding: True             # wss beter om aan te laten maar de authors hebben het hier niet over
        init_hidden: "bridge"
        attention: "bahdanau"
