2021-10-11 19:20:18,522 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-11 19:20:18,584 - INFO - joeynmt.data - Loading training data...
2021-10-11 19:20:19,167 - INFO - joeynmt.data - Building vocabulary...
2021-10-11 19:20:19,624 - INFO - joeynmt.data - Loading dev data...
2021-10-11 19:20:19,694 - INFO - joeynmt.data - Loading test data...
2021-10-11 19:20:19,778 - INFO - joeynmt.data - Data loaded.
2021-10-11 19:20:19,779 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-11 19:20:20,346 - INFO - joeynmt.model - Enc-dec model built.
2021-10-11 19:20:20,353 - INFO - joeynmt.training - Total params: 24859648
2021-10-11 19:20:24,481 - INFO - joeynmt.helpers - cfg.name                           : FINAL_pl_INIT
2021-10-11 19:20:24,481 - INFO - joeynmt.helpers - cfg.data.src                       : pl
2021-10-11 19:20:24,481 - INFO - joeynmt.helpers - cfg.data.trg                       : en
2021-10-11 19:20:24,481 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/pl.en/train.bpe
2021-10-11 19:20:24,481 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/pl.en/val.bpe
2021-10-11 19:20:24,481 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/pl.en/test.bpe
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 100000
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 100000
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/pl.en/pl.en.vocab.txt
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/pl.en/pl.en.vocab.txt
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 5e-07
2021-10-11 19:20:24,482 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.clip_grad_norm        : 1.0
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.patience              : 10
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.5
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.epochs                : 1
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 100
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/FINAL_pl_INIT
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-10-11 19:20:24,483 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.encoder.rnn_type         : lstm
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : False
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.encoder.bidirectional    : False
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.2
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 1
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.decoder.rnn_type         : lstm
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : False
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.decoder.emb_scale        : False
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 1024
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.2
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_dropout   : 0.2
2021-10-11 19:20:24,484 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 1
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - cfg.model.decoder.input_feeding    : True
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - cfg.model.decoder.init_hidden      : bridge
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - cfg.model.decoder.attention        : bahdanau
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - Data set sizes: 
	train 21154,
	valid 4109,
	test 5011
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - First training example:
	[SRC] pl
	[TRG] en
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) to (9) a
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) to (9) a
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - Number of Src words (types): 4195
2021-10-11 19:20:24,485 - INFO - joeynmt.helpers - Number of Trg words (types): 4195
2021-10-11 19:20:24,486 - INFO - joeynmt.training - Model(
	encoder=RecurrentEncoder(LSTM(512, 512, batch_first=True)),
	decoder=RecurrentDecoder(rnn=LSTM(1536, 1024, batch_first=True), attention=BahdanauAttention),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4195),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4195))
2021-10-11 19:20:24,495 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-11 19:20:24,496 - INFO - joeynmt.training - EPOCH 1
2021-10-11 19:21:05,538 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:21:05,539 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:21:05,539 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:21:05,565 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:21:06,489 - INFO - joeynmt.training - Example #0
2021-10-11 19:21:06,490 - INFO - joeynmt.training - 	Source:     pl
2021-10-11 19:21:06,490 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:21:06,490 - INFO - joeynmt.training - 	Hypothesis: I &apos;s .
2021-10-11 19:21:06,490 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0142' in position 89: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"Gdy', 'mi@@', 'a\u0142am', '11', 'lat', ',', 'pew@@', 'nego', 'ran@@', 'ka', 'ob@@', 'u@@', 'dzi@@', '\u0142', 'mnie', 'ok@@', 'rzy@@', 'k', 'ra@@', 'do@@', '\u015bci', 'w', 'moim', 'domu', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0142' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Gdy mia\u0142am 11 lat , pewnego ranka obudzi\u0142 mnie okrzyk rado\u015bci w moim domu ."',)
2021-10-11 19:21:06,494 - INFO - joeynmt.training - 	Source:     "Gdy mia\u0142am 11 lat , pewnego ranka obudzi\u0142 mnie okrzyk rado\u015bci w moim domu ."
2021-10-11 19:21:06,494 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:21:06,495 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s , , , , , , , , , , , , , the
2021-10-11 19:21:06,495 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0142' in position 109: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"To', 'mój', 'oj@@', 'cie@@', 'c', 's\u0142uch@@', 'a\u0142', 'wiadom@@', 'o\u015bci', 'B@@', 'B@@', 'C', 'w', 'swoim', 'ma\u0142@@', 'ym', ',', 'sz@@', 'ar@@', 'ym', 'radi@@', 'u', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0142' in position 81: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"To mój ojciec s\u0142ucha\u0142 wiadomo\u015bci BBC w swoim ma\u0142ym , szarym radiu ."',)
2021-10-11 19:21:06,495 - INFO - joeynmt.training - 	Source:     "To mój ojciec s\u0142ucha\u0142 wiadomo\u015bci BBC w swoim ma\u0142ym , szarym radiu ."
2021-10-11 19:21:06,496 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:21:06,496 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s , , , , , , , , , , , , the
2021-10-11 19:21:06,496 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      100: bleu:   3.78, loss: 758109.1250, ppl: 559.9268, duration: 20.7253s
2021-10-11 19:21:56,838 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:21:56,839 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:21:56,839 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:21:56,882 - INFO - joeynmt.training - Example #0
2021-10-11 19:21:56,882 - INFO - joeynmt.training - 	Source:     pl
2021-10-11 19:21:56,883 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:21:56,883 - INFO - joeynmt.training - 	Hypothesis: ( &apos;s a .
2021-10-11 19:21:56,883 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0142' in position 89: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"Gdy', 'mi@@', 'a\u0142am', '11', 'lat', ',', 'pew@@', 'nego', 'ran@@', 'ka', 'ob@@', 'u@@', 'dzi@@', '\u0142', 'mnie', 'ok@@', 'rzy@@', 'k', 'ra@@', 'do@@', '\u015bci', 'w', 'moim', 'domu', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0142' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Gdy mia\u0142am 11 lat , pewnego ranka obudzi\u0142 mnie okrzyk rado\u015bci w moim domu ."',)
2021-10-11 19:21:56,885 - INFO - joeynmt.training - 	Source:     "Gdy mia\u0142am 11 lat , pewnego ranka obudzi\u0142 mnie okrzyk rado\u015bci w moim domu ."
2021-10-11 19:21:56,886 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:21:56,886 - INFO - joeynmt.training - 	Hypothesis: "And I I I I I I I I I I I I I I I I I I I I I I I I I I I they you a a a the the the the the the the the the the the the the the the the the the the the the the the the the the the the ."
2021-10-11 19:21:56,886 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0142' in position 109: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"To', 'mój', 'oj@@', 'cie@@', 'c', 's\u0142uch@@', 'a\u0142', 'wiadom@@', 'o\u015bci', 'B@@', 'B@@', 'C', 'w', 'swoim', 'ma\u0142@@', 'ym', ',', 'sz@@', 'ar@@', 'ym', 'radi@@', 'u', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0142' in position 81: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"To mój ojciec s\u0142ucha\u0142 wiadomo\u015bci BBC w swoim ma\u0142ym , szarym radiu ."',)
2021-10-11 19:21:56,886 - INFO - joeynmt.training - 	Source:     "To mój ojciec s\u0142ucha\u0142 wiadomo\u015bci BBC w swoim ma\u0142ym , szarym radiu ."
2021-10-11 19:21:56,886 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:21:56,887 - INFO - joeynmt.training - 	Hypothesis: "And I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I they you a a a the the the the the the the the the ."
2021-10-11 19:21:56,887 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      200: bleu:   0.49, loss: 718090.0625, ppl: 400.9248, duration: 27.7449s
2021-10-11 19:22:13,699 - INFO - joeynmt.training - Epoch   1: total training loss 37205.66
2021-10-11 19:22:13,699 - INFO - joeynmt.training - Training ended after   1 epochs.
2021-10-11 19:22:13,699 - INFO - joeynmt.training - Best validation result (greedy) at step      100:   3.78 eval_metric.
2021-10-11 19:22:13,725 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-11 19:22:13,725 - INFO - joeynmt.prediction - Loading model from models/FINAL_pl_INIT/100.ckpt
2021-10-11 19:22:14,043 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-11 19:22:14,560 - INFO - joeynmt.model - Enc-dec model built.
2021-10-11 19:22:14,614 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/pl.en/val.bpe.en)...
2021-10-11 19:22:52,252 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:22:52,253 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:22:52,253 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:22:52,279 - INFO - joeynmt.prediction -  dev bleu[13a]:   3.86 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-11 19:22:52,284 - INFO - joeynmt.prediction - Translations saved to: models/FINAL_pl_INIT/00000100.hyps.dev
2021-10-11 19:22:52,284 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/pl.en/test.bpe.en)...
2021-10-11 19:23:35,370 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:23:35,370 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:23:35,371 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:23:35,404 - INFO - joeynmt.prediction - test bleu[13a]:   4.21 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-11 19:23:35,410 - INFO - joeynmt.prediction - Translations saved to: models/FINAL_pl_INIT/00000100.hyps.test
