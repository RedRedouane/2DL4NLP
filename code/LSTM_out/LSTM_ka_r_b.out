2021-10-05 01:41:26,401 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-05 01:41:26,465 - INFO - joeynmt.data - Loading training data...
2021-10-05 01:41:26,766 - INFO - joeynmt.data - Building vocabulary...
2021-10-05 01:41:27,214 - INFO - joeynmt.data - Loading dev data...
2021-10-05 01:41:27,224 - INFO - joeynmt.data - Loading test data...
2021-10-05 01:41:27,243 - INFO - joeynmt.data - Data loaded.
2021-10-05 01:41:27,243 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-05 01:41:27,785 - INFO - joeynmt.model - Enc-dec model built.
2021-10-05 01:41:27,791 - INFO - joeynmt.training - Total params: 24765440
2021-10-05 01:41:32,423 - INFO - joeynmt.helpers - cfg.name                           : LSTM_ka_r_b
2021-10-05 01:41:32,424 - INFO - joeynmt.helpers - cfg.data.src                       : ka_r_b
2021-10-05 01:41:32,424 - INFO - joeynmt.helpers - cfg.data.trg                       : en_b_ka
2021-10-05 01:41:32,424 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/ka_r_b.en_b_ka/train.bpe
2021-10-05 01:41:32,424 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/ka_r_b.en_b_ka/val.bpe
2021-10-05 01:41:32,424 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/ka_r_b.en_b_ka/test.bpe
2021-10-05 01:41:32,424 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-05 01:41:32,424 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 100000
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 100000
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/ka_r_b.en_b_ka/ka_r_b.en_b_ka.vocab.txt
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/ka_r_b.en_b_ka/ka_r_b.en_b_ka.vocab.txt
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 5e-07
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.training.clip_grad_norm        : 1.0
2021-10-05 01:41:32,425 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.patience              : 10
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.5
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.epochs                : 20
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 100
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/LSTM_ka_r_b
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-05 01:41:32,426 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.encoder.rnn_type         : lstm
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : False
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.encoder.bidirectional    : False
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.2
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 1
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.rnn_type         : lstm
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : False
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.emb_scale        : False
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 1024
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.2
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_dropout   : 0.2
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 1
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.input_feeding    : True
2021-10-05 01:41:32,427 - INFO - joeynmt.helpers - cfg.model.decoder.init_hidden      : bridge
2021-10-05 01:41:32,428 - INFO - joeynmt.helpers - cfg.model.decoder.attention        : bahdanau
2021-10-05 01:41:32,428 - INFO - joeynmt.helpers - Data set sizes: 
	train 11723,
	valid 655,
	test 944
2021-10-05 01:41:32,428 - INFO - joeynmt.helpers - First training example:
	[SRC] k@@ a
	[TRG] en
2021-10-05 01:41:32,428 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) ." (6) . (7) the (8) is (9) a
2021-10-05 01:41:32,428 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) ." (6) . (7) the (8) is (9) a
2021-10-05 01:41:32,428 - INFO - joeynmt.helpers - Number of Src words (types): 4149
2021-10-05 01:41:32,428 - INFO - joeynmt.helpers - Number of Trg words (types): 4149
2021-10-05 01:41:32,429 - INFO - joeynmt.training - Model(
	encoder=RecurrentEncoder(LSTM(512, 512, batch_first=True)),
	decoder=RecurrentDecoder(rnn=LSTM(1536, 1024, batch_first=True), attention=BahdanauAttention),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4149),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4149))
2021-10-05 01:41:32,439 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-05 01:41:32,439 - INFO - joeynmt.training - EPOCH 1
2021-10-05 01:41:57,646 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:41:57,647 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:41:57,647 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:41:57,653 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:41:58,580 - INFO - joeynmt.training - Example #0
2021-10-05 01:41:58,580 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:41:58,580 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:41:58,581 - INFO - joeynmt.training - 	Hypothesis: I &apos;s .
2021-10-05 01:41:58,581 - INFO - joeynmt.training - Example #1
2021-10-05 01:41:58,581 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:41:58,581 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:41:58,581 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s , , , , , , , , , , , , , , , , , , , , , , , ."
2021-10-05 01:41:58,581 - INFO - joeynmt.training - Example #2
2021-10-05 01:41:58,582 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:41:58,582 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:41:58,582 - INFO - joeynmt.training - 	Hypothesis: "And I , , , , , , , , , , , , , , , , , , , , , , , , ."
2021-10-05 01:41:58,582 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      100: bleu:   2.02, loss: 107858.8359, ppl: 482.4597, duration: 4.6552s
2021-10-05 01:42:09,856 - INFO - joeynmt.training - Epoch   1: total training loss 20417.80
2021-10-05 01:42:09,857 - INFO - joeynmt.training - EPOCH 2
2021-10-05 01:42:25,214 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:42:25,215 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:42:25,215 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:42:25,220 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:42:26,115 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/100.ckpt
2021-10-05 01:42:26,190 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/100.ckpt
2021-10-05 01:42:26,191 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/100.ckpt')
2021-10-05 01:42:26,193 - INFO - joeynmt.training - Example #0
2021-10-05 01:42:26,193 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:42:26,193 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:42:26,193 - INFO - joeynmt.training - 	Hypothesis: ( &apos;s the .
2021-10-05 01:42:26,193 - INFO - joeynmt.training - Example #1
2021-10-05 01:42:26,193 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:42:26,193 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:42:26,193 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s the the the the the the the the the the the the the the the the the the the the the the the the the the ."
2021-10-05 01:42:26,193 - INFO - joeynmt.training - Example #2
2021-10-05 01:42:26,194 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:42:26,194 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:42:26,194 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s the the the the the the the the the the the the the the the the the the the the the the the the the the ."
2021-10-05 01:42:26,194 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      200: bleu:   2.33, loss: 102904.2109, ppl: 363.2395, duration: 5.0529s
2021-10-05 01:42:47,534 - INFO - joeynmt.training - Epoch   2: total training loss 18707.40
2021-10-05 01:42:47,535 - INFO - joeynmt.training - EPOCH 3
2021-10-05 01:42:52,948 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:42:52,948 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:42:52,948 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:42:52,955 - INFO - joeynmt.training - Example #0
2021-10-05 01:42:52,956 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:42:52,956 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:42:52,956 - INFO - joeynmt.training - 	Hypothesis: ( Applause )
2021-10-05 01:42:52,956 - INFO - joeynmt.training - Example #1
2021-10-05 01:42:52,957 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:42:52,957 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:42:52,957 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s the a , I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I
2021-10-05 01:42:52,957 - INFO - joeynmt.training - Example #2
2021-10-05 01:42:52,958 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:42:52,958 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:42:52,958 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s the a the , I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I
2021-10-05 01:42:52,958 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step      300: bleu:   0.49, loss: 97474.8750, ppl: 266.1427, duration: 4.1339s
2021-10-05 01:43:22,788 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:43:22,789 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:43:22,789 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:43:22,794 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:43:23,711 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/200.ckpt
2021-10-05 01:43:23,782 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/200.ckpt
2021-10-05 01:43:23,782 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/200.ckpt')
2021-10-05 01:43:23,784 - INFO - joeynmt.training - Example #0
2021-10-05 01:43:23,785 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:43:23,785 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:43:23,785 - INFO - joeynmt.training - 	Hypothesis: Thank you you you .
2021-10-05 01:43:23,785 - INFO - joeynmt.training - Example #1
2021-10-05 01:43:23,785 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:43:23,785 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:43:23,785 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s the lot of the world , and the lot , and the lot , and we &apos;re a lot of the world ."
2021-10-05 01:43:23,785 - INFO - joeynmt.training - Example #2
2021-10-05 01:43:23,785 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:43:23,785 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:43:23,786 - INFO - joeynmt.training - 	Hypothesis: "So , I &apos;m a lot of the world , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:43:23,786 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step      400: bleu:   2.33, loss: 92736.4766, ppl: 202.8739, duration: 5.3802s
2021-10-05 01:43:35,799 - INFO - joeynmt.training - Epoch   3: total training loss 17233.36
2021-10-05 01:43:35,799 - INFO - joeynmt.training - EPOCH 4
2021-10-05 01:43:52,551 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:43:52,551 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:43:52,551 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:43:52,557 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:43:53,460 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/400.ckpt
2021-10-05 01:43:53,536 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/400.ckpt
2021-10-05 01:43:53,536 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/400.ckpt')
2021-10-05 01:43:53,538 - INFO - joeynmt.training - Example #0
2021-10-05 01:43:53,538 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:43:53,539 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:43:53,539 - INFO - joeynmt.training - 	Hypothesis: Thank you do .
2021-10-05 01:43:53,539 - INFO - joeynmt.training - Example #1
2021-10-05 01:43:53,539 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:43:53,539 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:43:53,539 - INFO - joeynmt.training - 	Hypothesis: "And I &apos;m a way , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:43:53,539 - INFO - joeynmt.training - Example #2
2021-10-05 01:43:53,539 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:43:53,539 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:43:53,539 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to be a way to be a lot of the world .
2021-10-05 01:43:53,539 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step      500: bleu:   2.48, loss: 89800.8906, ppl: 171.4710, duration: 5.1188s
2021-10-05 01:44:15,478 - INFO - joeynmt.training - Epoch   4: total training loss 16293.61
2021-10-05 01:44:15,478 - INFO - joeynmt.training - EPOCH 5
2021-10-05 01:44:22,080 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:44:22,081 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:44:22,081 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:44:22,086 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:44:23,027 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/500.ckpt
2021-10-05 01:44:23,100 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/500.ckpt
2021-10-05 01:44:23,100 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/500.ckpt')
2021-10-05 01:44:23,102 - INFO - joeynmt.training - Example #0
2021-10-05 01:44:23,103 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:44:23,103 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:44:23,103 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-05 01:44:23,103 - INFO - joeynmt.training - Example #1
2021-10-05 01:44:23,103 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:44:23,103 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:44:23,103 - INFO - joeynmt.training - 	Hypothesis: "And I think I think , I &apos;m going to be a lot of the world , I &apos;m going to be ."
2021-10-05 01:44:23,103 - INFO - joeynmt.training - Example #2
2021-10-05 01:44:23,104 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:44:23,104 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:44:23,104 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to be a lot of the world , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:44:23,104 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step      600: bleu:   2.51, loss: 87632.7266, ppl: 151.4426, duration: 5.1109s
2021-10-05 01:44:52,035 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:44:52,035 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:44:52,035 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:44:52,040 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:44:52,938 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/600.ckpt
2021-10-05 01:44:53,014 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/600.ckpt
2021-10-05 01:44:53,015 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/600.ckpt')
2021-10-05 01:44:53,017 - INFO - joeynmt.training - Example #0
2021-10-05 01:44:53,017 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:44:53,017 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:44:53,017 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot .
2021-10-05 01:44:53,017 - INFO - joeynmt.training - Example #1
2021-10-05 01:44:53,017 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:44:53,017 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:44:53,017 - INFO - joeynmt.training - 	Hypothesis: "I &apos;m going to be a lot of the world , I &apos;m going to be a lot of the world ."
2021-10-05 01:44:53,017 - INFO - joeynmt.training - Example #2
2021-10-05 01:44:53,018 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:44:53,018 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:44:53,018 - INFO - joeynmt.training - 	Hypothesis: "I &apos;m going to be a lot of the world , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:44:53,018 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step      700: bleu:   3.93, loss: 85851.3828, ppl: 136.7506, duration: 5.1857s
2021-10-05 01:45:04,056 - INFO - joeynmt.training - Epoch   5: total training loss 15638.85
2021-10-05 01:45:04,056 - INFO - joeynmt.training - EPOCH 6
2021-10-05 01:45:22,080 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:45:22,080 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:45:22,080 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:45:22,085 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:45:23,085 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/700.ckpt
2021-10-05 01:45:23,157 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/700.ckpt
2021-10-05 01:45:23,158 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/700.ckpt')
2021-10-05 01:45:23,160 - INFO - joeynmt.training - Example #0
2021-10-05 01:45:23,160 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:45:23,160 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:45:23,160 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very much .
2021-10-05 01:45:23,160 - INFO - joeynmt.training - Example #1
2021-10-05 01:45:23,161 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:45:23,161 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:45:23,161 - INFO - joeynmt.training - 	Hypothesis: "And I was a little bit , I was a little bit , I was going to be a little bit of the world ."
2021-10-05 01:45:23,161 - INFO - joeynmt.training - Example #2
2021-10-05 01:45:23,161 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:45:23,161 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:45:23,161 - INFO - joeynmt.training - 	Hypothesis: And I was a very bit to be a little bit of the world .
2021-10-05 01:45:23,161 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step      800: bleu:   4.39, loss: 84630.6719, ppl: 127.5142, duration: 5.2008s
2021-10-05 01:45:41,945 - INFO - joeynmt.training - Epoch   6: total training loss 15121.82
2021-10-05 01:45:41,946 - INFO - joeynmt.training - EPOCH 7
2021-10-05 01:45:49,879 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:45:49,879 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:45:49,879 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:45:49,886 - INFO - joeynmt.training - Example #0
2021-10-05 01:45:49,887 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:45:49,887 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:45:49,887 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very .
2021-10-05 01:45:49,887 - INFO - joeynmt.training - Example #1
2021-10-05 01:45:49,887 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:45:49,887 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:45:49,887 - INFO - joeynmt.training - 	Hypothesis: "And I was a little bit , I was a little bit of the world , I was a little bit of the world ."
2021-10-05 01:45:49,887 - INFO - joeynmt.training - Example #2
2021-10-05 01:45:49,888 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:45:49,888 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:45:49,888 - INFO - joeynmt.training - 	Hypothesis: "I &apos;m a little bit of the AD , and &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:45:49,888 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step      900: bleu:   3.58, loss: 83651.8125, ppl: 120.5606, duration: 4.0654s
2021-10-05 01:46:14,357 - INFO - joeynmt.training - Epoch   7, Step:     1000, Batch Loss:    99.404793, Tokens per Sec:     7378, Lr: 0.000300
2021-10-05 01:46:18,347 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:46:18,347 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:46:18,347 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:46:18,454 - INFO - joeynmt.training - Example #0
2021-10-05 01:46:18,455 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:46:18,455 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:46:18,455 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very very .
2021-10-05 01:46:18,455 - INFO - joeynmt.training - Example #1
2021-10-05 01:46:18,456 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:46:18,456 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:46:18,456 - INFO - joeynmt.training - 	Hypothesis: "And I was a lot of my first , I was a lot of the world , I was a lot of the world ."
2021-10-05 01:46:18,457 - INFO - joeynmt.training - Example #2
2021-10-05 01:46:18,457 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:46:18,457 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:46:18,457 - INFO - joeynmt.training - 	Hypothesis: I was a lot of my first . &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:46:18,457 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     1000: bleu:   4.38, loss: 82413.2500, ppl: 112.3028, duration: 4.0993s
2021-10-05 01:46:27,836 - INFO - joeynmt.training - Epoch   7: total training loss 14669.38
2021-10-05 01:46:27,837 - INFO - joeynmt.training - EPOCH 8
2021-10-05 01:46:46,891 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:46:46,892 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:46:46,892 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:46:46,899 - INFO - joeynmt.training - Example #0
2021-10-05 01:46:46,899 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:46:46,899 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:46:46,899 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very much .
2021-10-05 01:46:46,899 - INFO - joeynmt.training - Example #1
2021-10-05 01:46:46,900 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:46:46,900 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:46:46,900 - INFO - joeynmt.training - 	Hypothesis: "And I was a little bit of my , I was a little bit of the world , I was a little bit of the world ."
2021-10-05 01:46:46,900 - INFO - joeynmt.training - Example #2
2021-10-05 01:46:46,900 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:46:46,900 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:46:46,900 - INFO - joeynmt.training - 	Hypothesis: &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:46:46,900 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     1100: bleu:   4.03, loss: 81743.9844, ppl: 108.0786, duration: 4.0065s
2021-10-05 01:47:06,151 - INFO - joeynmt.training - Epoch   8: total training loss 14270.02
2021-10-05 01:47:06,152 - INFO - joeynmt.training - EPOCH 9
2021-10-05 01:47:15,254 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:47:15,254 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:47:15,254 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:47:15,262 - INFO - joeynmt.training - Example #0
2021-10-05 01:47:15,262 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:47:15,262 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:47:15,262 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very very .
2021-10-05 01:47:15,262 - INFO - joeynmt.training - Example #1
2021-10-05 01:47:15,262 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:47:15,262 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:47:15,262 - INFO - joeynmt.training - 	Hypothesis: "And I was a little bit , I was a little bit of the world , I was a little bit of the world ."
2021-10-05 01:47:15,263 - INFO - joeynmt.training - Example #2
2021-10-05 01:47:15,263 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:47:15,263 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:47:15,263 - INFO - joeynmt.training - 	Hypothesis: "I said , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:47:15,263 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     1200: bleu:   3.91, loss: 81232.5859, ppl: 104.9582, duration: 4.0223s
2021-10-05 01:47:43,978 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:47:43,979 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:47:43,979 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:47:43,986 - INFO - joeynmt.training - Example #0
2021-10-05 01:47:43,987 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:47:43,987 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:47:43,987 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very good .
2021-10-05 01:47:43,987 - INFO - joeynmt.training - Example #1
2021-10-05 01:47:43,988 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:47:43,988 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:47:43,988 - INFO - joeynmt.training - 	Hypothesis: "And I was a first , I was a couple of my year , I was a couple of my year ."
2021-10-05 01:47:43,988 - INFO - joeynmt.training - Example #2
2021-10-05 01:47:43,989 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:47:43,989 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:47:43,989 - INFO - joeynmt.training - 	Hypothesis: "I &apos;m going to tell me , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:47:43,989 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     1300: bleu:   4.15, loss: 80415.5859, ppl: 100.1590, duration: 4.1350s
2021-10-05 01:47:52,030 - INFO - joeynmt.training - Epoch   9: total training loss 13916.20
2021-10-05 01:47:52,030 - INFO - joeynmt.training - EPOCH 10
2021-10-05 01:48:12,164 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:48:12,164 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:48:12,164 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:48:12,169 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:48:13,091 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/800.ckpt
2021-10-05 01:48:13,166 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/800.ckpt
2021-10-05 01:48:13,167 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/800.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/800.ckpt')
2021-10-05 01:48:13,169 - INFO - joeynmt.training - Example #0
2021-10-05 01:48:13,169 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:48:13,170 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:48:13,170 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very much .
2021-10-05 01:48:13,170 - INFO - joeynmt.training - Example #1
2021-10-05 01:48:13,170 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:48:13,170 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:48:13,170 - INFO - joeynmt.training - 	Hypothesis: "And I was a father , I was a little bit of my year , I was a little bit of the world ."
2021-10-05 01:48:13,170 - INFO - joeynmt.training - Example #2
2021-10-05 01:48:13,170 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:48:13,170 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:48:13,170 - INFO - joeynmt.training - 	Hypothesis: &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; No . &quot; &quot; And it &apos;s a good . &quot; &quot; And it &apos;s a good .
2021-10-05 01:48:13,170 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     1400: bleu:   4.84, loss: 79937.6641, ppl:  97.4540, duration: 4.9004s
2021-10-05 01:48:29,595 - INFO - joeynmt.training - Epoch  10: total training loss 13610.45
2021-10-05 01:48:29,596 - INFO - joeynmt.training - EPOCH 11
2021-10-05 01:48:39,884 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:48:39,884 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:48:39,884 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:48:39,890 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:48:40,790 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/1400.ckpt
2021-10-05 01:48:40,861 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/1400.ckpt
2021-10-05 01:48:40,861 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/1400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/1400.ckpt')
2021-10-05 01:48:40,863 - INFO - joeynmt.training - Example #0
2021-10-05 01:48:40,863 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:48:40,863 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:48:40,863 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very much .
2021-10-05 01:48:40,863 - INFO - joeynmt.training - Example #1
2021-10-05 01:48:40,864 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:48:40,864 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:48:40,864 - INFO - joeynmt.training - 	Hypothesis: "When I was a couple of years ago , I was a couple of my year , I was a couple of years ago ."
2021-10-05 01:48:40,864 - INFO - joeynmt.training - Example #2
2021-10-05 01:48:40,864 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:48:40,864 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:48:40,864 - INFO - joeynmt.training - 	Hypothesis: &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; No .
2021-10-05 01:48:40,864 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step     1500: bleu:   5.01, loss: 79471.6250, ppl:  94.8866, duration: 4.8601s
2021-10-05 01:49:08,153 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:49:08,153 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:49:08,153 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:49:08,160 - INFO - joeynmt.training - Example #0
2021-10-05 01:49:08,161 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:49:08,161 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:49:08,161 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very much .
2021-10-05 01:49:08,161 - INFO - joeynmt.training - Example #1
2021-10-05 01:49:08,161 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:49:08,161 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:49:08,161 - INFO - joeynmt.training - 	Hypothesis: "And I went to my mother , I was a very girg , I was a very girg , I was a couple of years ago ."
2021-10-05 01:49:08,161 - INFO - joeynmt.training - Example #2
2021-10-05 01:49:08,162 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:49:08,162 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:49:08,162 - INFO - joeynmt.training - 	Hypothesis: "He said , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:49:08,162 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step     1600: bleu:   5.00, loss: 79113.6797, ppl:  92.9607, duration: 3.8080s
2021-10-05 01:49:15,173 - INFO - joeynmt.training - Epoch  11: total training loss 13303.72
2021-10-05 01:49:15,174 - INFO - joeynmt.training - EPOCH 12
2021-10-05 01:49:36,897 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:49:36,897 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:49:36,897 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:49:36,905 - INFO - joeynmt.training - Example #0
2021-10-05 01:49:36,905 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:49:36,906 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:49:36,906 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very good .
2021-10-05 01:49:36,906 - INFO - joeynmt.training - Example #1
2021-10-05 01:49:36,906 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:49:36,906 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:49:36,906 - INFO - joeynmt.training - 	Hypothesis: "And I was married , I was a father , I was a couple of years ago , I was a couple of years ago ."
2021-10-05 01:49:36,906 - INFO - joeynmt.training - Example #2
2021-10-05 01:49:36,906 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:49:36,906 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:49:36,906 - INFO - joeynmt.training - 	Hypothesis: "He said , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:49:36,907 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step     1700: bleu:   4.97, loss: 78746.0391, ppl:  91.0233, duration: 3.9866s
2021-10-05 01:49:53,647 - INFO - joeynmt.training - Epoch  12: total training loss 13036.96
2021-10-05 01:49:53,647 - INFO - joeynmt.training - EPOCH 13
2021-10-05 01:50:05,682 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:50:05,682 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:50:05,682 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:50:05,689 - INFO - joeynmt.training - Example #0
2021-10-05 01:50:05,690 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:50:05,690 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:50:05,690 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very good .
2021-10-05 01:50:05,690 - INFO - joeynmt.training - Example #1
2021-10-05 01:50:05,690 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:50:05,690 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:50:05,690 - INFO - joeynmt.training - 	Hypothesis: "I was a first , I was a very few years ago , and I was a very few years ago ."
2021-10-05 01:50:05,690 - INFO - joeynmt.training - Example #2
2021-10-05 01:50:05,691 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:50:05,691 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:50:05,691 - INFO - joeynmt.training - 	Hypothesis: "He said , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; I &apos;m a good career ."
2021-10-05 01:50:05,691 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step     1800: bleu:   4.96, loss: 78309.8594, ppl:  88.7771, duration: 4.3143s
2021-10-05 01:50:32,545 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:50:32,546 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:50:32,546 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:50:32,551 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:50:33,459 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/1500.ckpt
2021-10-05 01:50:33,534 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/1500.ckpt
2021-10-05 01:50:33,534 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/1500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/1500.ckpt')
2021-10-05 01:50:33,537 - INFO - joeynmt.training - Example #0
2021-10-05 01:50:33,537 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:50:33,537 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:50:33,538 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very much .
2021-10-05 01:50:33,538 - INFO - joeynmt.training - Example #1
2021-10-05 01:50:33,538 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:50:33,538 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:50:33,538 - INFO - joeynmt.training - 	Hypothesis: "And I was a little woman , I was a little bit of my year , and I was a little bit of my year ."
2021-10-05 01:50:33,538 - INFO - joeynmt.training - Example #2
2021-10-05 01:50:33,539 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:50:33,539 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:50:33,539 - INFO - joeynmt.training - 	Hypothesis: &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-05 01:50:33,539 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step     1900: bleu:   5.40, loss: 78203.4922, ppl:  88.2377, duration: 5.0438s
2021-10-05 01:50:39,030 - INFO - joeynmt.training - Epoch  13: total training loss 12765.90
2021-10-05 01:50:39,031 - INFO - joeynmt.training - EPOCH 14
2021-10-05 01:50:57,971 - INFO - joeynmt.training - Epoch  14, Step:     2000, Batch Loss:    77.542732, Tokens per Sec:     8313, Lr: 0.000300
2021-10-05 01:51:02,023 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:51:02,024 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:51:02,024 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:51:02,029 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:51:03,021 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/1900.ckpt
2021-10-05 01:51:03,094 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/1900.ckpt
2021-10-05 01:51:03,094 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/1900.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/1900.ckpt')
2021-10-05 01:51:03,096 - INFO - joeynmt.training - Example #0
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Hypothesis: It &apos;s very much .
2021-10-05 01:51:03,097 - INFO - joeynmt.training - Example #1
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Hypothesis: "I was married , I was a woman , I was a very good career ."
2021-10-05 01:51:03,097 - INFO - joeynmt.training - Example #2
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:51:03,097 - INFO - joeynmt.training - 	Hypothesis: "&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; Awe ."
2021-10-05 01:51:03,097 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step     2000: bleu:   5.54, loss: 77662.0938, ppl:  85.5431, duration: 5.1262s
2021-10-05 01:51:16,954 - INFO - joeynmt.training - Epoch  14: total training loss 12506.32
2021-10-05 01:51:16,955 - INFO - joeynmt.training - EPOCH 15
2021-10-05 01:51:29,581 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:51:29,582 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:51:29,582 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:51:29,587 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:51:30,619 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/2000.ckpt
2021-10-05 01:51:30,697 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/2000.ckpt
2021-10-05 01:51:30,698 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/2000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/2000.ckpt')
2021-10-05 01:51:30,700 - INFO - joeynmt.training - Example #0
2021-10-05 01:51:30,700 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:51:30,700 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:51:30,700 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a good .
2021-10-05 01:51:30,701 - INFO - joeynmt.training - Example #1
2021-10-05 01:51:30,701 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:51:30,701 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:51:30,701 - INFO - joeynmt.training - 	Hypothesis: "When I was married , I was a couple of years ago , I was a couple of years ago , I was a couple of years ago ."
2021-10-05 01:51:30,701 - INFO - joeynmt.training - Example #2
2021-10-05 01:51:30,701 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:51:30,701 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:51:30,701 - INFO - joeynmt.training - 	Hypothesis: "He said , &quot; &quot; I &apos;m going to get a man of my school , and he &apos;s a good career ."
2021-10-05 01:51:30,701 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step     2100: bleu:   5.60, loss: 77753.5938, ppl:  85.9926, duration: 4.7986s
2021-10-05 01:51:57,061 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:51:57,061 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:51:57,061 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:51:57,066 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:51:57,995 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/2100.ckpt
2021-10-05 01:51:58,073 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/2100.ckpt
2021-10-05 01:51:58,073 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/2100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/2100.ckpt')
2021-10-05 01:51:58,076 - INFO - joeynmt.training - Example #0
2021-10-05 01:51:58,076 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:51:58,076 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:51:58,076 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very good .
2021-10-05 01:51:58,076 - INFO - joeynmt.training - Example #1
2021-10-05 01:51:58,076 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:51:58,076 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:51:58,076 - INFO - joeynmt.training - 	Hypothesis: "And I was a few years ago , I was a little bit of my year , I was a little bit of my year ."
2021-10-05 01:51:58,077 - INFO - joeynmt.training - Example #2
2021-10-05 01:51:58,077 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:51:58,077 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:51:58,077 - INFO - joeynmt.training - 	Hypothesis: "&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; I &apos;m a good career ."
2021-10-05 01:51:58,077 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step     2200: bleu:   5.88, loss: 77298.3125, ppl:  83.7788, duration: 4.6234s
2021-10-05 01:52:00,796 - INFO - joeynmt.training - Epoch  15: total training loss 12254.97
2021-10-05 01:52:00,797 - INFO - joeynmt.training - EPOCH 16
2021-10-05 01:52:24,715 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:52:24,715 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:52:24,715 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:52:24,723 - INFO - joeynmt.training - Example #0
2021-10-05 01:52:24,724 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:52:24,724 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:52:24,724 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very good .
2021-10-05 01:52:24,724 - INFO - joeynmt.training - Example #1
2021-10-05 01:52:24,724 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:52:24,725 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:52:24,725 - INFO - joeynmt.training - 	Hypothesis: "When I was a girl , I was a couple of months ago , I was a very few years ago ."
2021-10-05 01:52:24,725 - INFO - joeynmt.training - Example #2
2021-10-05 01:52:24,725 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:52:24,725 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:52:24,726 - INFO - joeynmt.training - 	Hypothesis: "&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; is a golf man ."
2021-10-05 01:52:24,726 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     2300: bleu:   5.64, loss: 77241.9453, ppl:  83.5087, duration: 3.8854s
2021-10-05 01:52:37,380 - INFO - joeynmt.training - Epoch  16: total training loss 12016.16
2021-10-05 01:52:37,380 - INFO - joeynmt.training - EPOCH 17
2021-10-05 01:52:51,508 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:52:51,508 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:52:51,508 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:52:51,515 - INFO - joeynmt.training - Example #0
2021-10-05 01:52:51,516 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:52:51,516 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:52:51,516 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a good .
2021-10-05 01:52:51,516 - INFO - joeynmt.training - Example #1
2021-10-05 01:52:51,516 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:52:51,516 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:52:51,516 - INFO - joeynmt.training - 	Hypothesis: "And I was a woman , I was a woman , I was a woman , and I was a hrip of the U.K. I had a free ."
2021-10-05 01:52:51,516 - INFO - joeynmt.training - Example #2
2021-10-05 01:52:51,517 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:52:51,517 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:52:51,517 - INFO - joeynmt.training - 	Hypothesis: "&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; I &apos;m not a dream ."
2021-10-05 01:52:51,517 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step     2400: bleu:   5.72, loss: 77372.5547, ppl:  84.1359, duration: 3.9646s
2021-10-05 01:53:14,448 - INFO - joeynmt.training - Epoch  17: total training loss 11756.95
2021-10-05 01:53:14,449 - INFO - joeynmt.training - EPOCH 18
2021-10-05 01:53:18,490 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:53:18,491 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:53:18,491 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:53:18,496 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-05 01:53:19,394 - INFO - joeynmt.helpers - delete models/LSTM_ka_r_b/2200.ckpt
2021-10-05 01:53:19,470 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/LSTM_ka_r_b/2200.ckpt
2021-10-05 01:53:19,470 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/LSTM_ka_r_b/2200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/LSTM_ka_r_b/2200.ckpt')
2021-10-05 01:53:19,472 - INFO - joeynmt.training - Example #0
2021-10-05 01:53:19,472 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:53:19,472 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:53:19,472 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very good .
2021-10-05 01:53:19,472 - INFO - joeynmt.training - Example #1
2021-10-05 01:53:19,473 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:53:19,473 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:53:19,473 - INFO - joeynmt.training - 	Hypothesis: "When I was a few years ago , I was a little bit of my year , I was a little bit of the first time ."
2021-10-05 01:53:19,473 - INFO - joeynmt.training - Example #2
2021-10-05 01:53:19,473 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:53:19,473 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:53:19,473 - INFO - joeynmt.training - 	Hypothesis: "He said , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; the world ."
2021-10-05 01:53:19,473 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     2500: bleu:   6.40, loss: 76896.6875, ppl:  81.8733, duration: 4.7984s
2021-10-05 01:53:47,438 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:53:47,438 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:53:47,439 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:53:47,446 - INFO - joeynmt.training - Example #0
2021-10-05 01:53:47,447 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:53:47,447 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:53:47,447 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very good .
2021-10-05 01:53:47,447 - INFO - joeynmt.training - Example #1
2021-10-05 01:53:47,448 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:53:47,448 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:53:47,448 - INFO - joeynmt.training - 	Hypothesis: "When I was a couple of years ago , I was a couple of months ago , I was a couple of months ago ."
2021-10-05 01:53:47,448 - INFO - joeynmt.training - Example #2
2021-10-05 01:53:47,449 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:53:47,449 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:53:47,449 - INFO - joeynmt.training - 	Hypothesis: "&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; I &apos;m a certificate ."
2021-10-05 01:53:47,449 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step     2600: bleu:   5.82, loss: 77005.3594, ppl:  82.3845, duration: 3.6487s
2021-10-05 01:53:59,696 - INFO - joeynmt.training - Epoch  18: total training loss 11503.63
2021-10-05 01:53:59,696 - INFO - joeynmt.training - EPOCH 19
2021-10-05 01:54:14,807 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:54:14,807 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:54:14,807 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:54:14,814 - INFO - joeynmt.training - Example #0
2021-10-05 01:54:14,815 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:54:14,815 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:54:14,815 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very good .
2021-10-05 01:54:14,815 - INFO - joeynmt.training - Example #1
2021-10-05 01:54:14,815 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:54:14,815 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:54:14,815 - INFO - joeynmt.training - 	Hypothesis: "When I was a woman , I was a woman , I was a woman , I was a woman in my school ."
2021-10-05 01:54:14,815 - INFO - joeynmt.training - Example #2
2021-10-05 01:54:14,816 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:54:14,816 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:54:14,816 - INFO - joeynmt.training - 	Hypothesis: "&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; The world is a good career ."
2021-10-05 01:54:14,816 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     2700: bleu:   6.14, loss: 76945.5938, ppl:  82.1029, duration: 3.5020s
2021-10-05 01:54:36,167 - INFO - joeynmt.training - Epoch  19: total training loss 11280.59
2021-10-05 01:54:36,167 - INFO - joeynmt.training - EPOCH 20
2021-10-05 01:54:41,609 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:54:41,609 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:54:41,609 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:54:41,617 - INFO - joeynmt.training - Example #0
2021-10-05 01:54:41,617 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:54:41,617 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:54:41,617 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very good .
2021-10-05 01:54:41,617 - INFO - joeynmt.training - Example #1
2021-10-05 01:54:41,618 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:54:41,618 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:54:41,618 - INFO - joeynmt.training - 	Hypothesis: "When I was married to my 14 years ago , I was a very few years old ."
2021-10-05 01:54:41,618 - INFO - joeynmt.training - Example #2
2021-10-05 01:54:41,618 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:54:41,618 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:54:41,618 - INFO - joeynmt.training - 	Hypothesis: "&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; of the world ."
2021-10-05 01:54:41,618 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     2800: bleu:   6.00, loss: 76700.8750, ppl:  80.9600, duration: 3.9383s
2021-10-05 01:55:09,010 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:55:09,010 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:55:09,010 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:55:09,018 - INFO - joeynmt.training - Example #0
2021-10-05 01:55:09,018 - INFO - joeynmt.training - 	Source:     ka
2021-10-05 01:55:09,018 - INFO - joeynmt.training - 	Reference:  en
2021-10-05 01:55:09,018 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very good idea .
2021-10-05 01:55:09,018 - INFO - joeynmt.training - Example #1
2021-10-05 01:55:09,019 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-05 01:55:09,019 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-05 01:55:09,019 - INFO - joeynmt.training - 	Hypothesis: "When I was a woman , I was a woman , I was a very girl in my finent , I was a hustiy ."
2021-10-05 01:55:09,019 - INFO - joeynmt.training - Example #2
2021-10-05 01:55:09,019 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-05 01:55:09,019 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-05 01:55:09,019 - INFO - joeynmt.training - 	Hypothesis: "&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; The man is not a concept of the world ."
2021-10-05 01:55:09,019 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     2900: bleu:   6.02, loss: 76697.2188, ppl:  80.9430, duration: 3.9407s
2021-10-05 01:55:19,037 - INFO - joeynmt.training - Epoch  20: total training loss 11032.78
2021-10-05 01:55:19,037 - INFO - joeynmt.training - Training ended after  20 epochs.
2021-10-05 01:55:19,037 - INFO - joeynmt.training - Best validation result (greedy) at step     2500:   6.40 eval_metric.
2021-10-05 01:55:19,064 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-05 01:55:19,064 - INFO - joeynmt.prediction - Loading model from models/LSTM_ka_r_b/2500.ckpt
2021-10-05 01:55:19,387 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-05 01:55:19,909 - INFO - joeynmt.model - Enc-dec model built.
2021-10-05 01:55:19,964 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/ka_r_b.en_b_ka/val.bpe.en_b_ka)...
2021-10-05 01:55:32,599 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:55:32,599 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:55:32,599 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:55:32,604 - INFO - joeynmt.prediction -  dev bleu[13a]:   6.52 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-05 01:55:32,606 - INFO - joeynmt.prediction - Translations saved to: models/LSTM_ka_r_b/00002500.hyps.dev
2021-10-05 01:55:32,606 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/ka_r_b.en_b_ka/test.bpe.en_b_ka)...
2021-10-05 01:55:49,255 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-05 01:55:49,256 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-05 01:55:49,256 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-05 01:55:49,263 - INFO - joeynmt.prediction - test bleu[13a]:   7.05 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-05 01:55:49,266 - INFO - joeynmt.prediction - Translations saved to: models/LSTM_ka_r_b/00002500.hyps.test
