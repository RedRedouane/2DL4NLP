2021-10-03 13:29:41,750 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-03 13:29:41,834 - INFO - joeynmt.data - Loading training data...
2021-10-03 13:29:41,933 - INFO - joeynmt.data - Building vocabulary...
2021-10-03 13:29:42,364 - INFO - joeynmt.data - Loading dev data...
2021-10-03 13:29:42,376 - INFO - joeynmt.data - Loading test data...
2021-10-03 13:29:42,385 - INFO - joeynmt.data - Data loaded.
2021-10-03 13:29:42,385 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:29:42,924 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:29:42,930 - INFO - joeynmt.training - Total params: 24673280
2021-10-03 13:29:47,101 - INFO - joeynmt.helpers - cfg.name                           : LSTM_el_r
2021-10-03 13:29:47,101 - INFO - joeynmt.helpers - cfg.data.src                       : el_r
2021-10-03 13:29:47,101 - INFO - joeynmt.helpers - cfg.data.trg                       : en_s
2021-10-03 13:29:47,101 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/el_r.en_s/train.bpe
2021-10-03 13:29:47,101 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/el_r.en_s/val.bpe
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/el_r.en_s/test.bpe
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 100000
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 100000
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/el_r.en_s/el_r.en_s.vocab.txt
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/el_r.en_s/el_r.en_s.vocab.txt
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-03 13:29:47,102 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 5e-07
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.clip_grad_norm        : 1.0
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.patience              : 10
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.5
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.epochs                : 20
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 100
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/LSTM_el_r
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-03 13:29:47,103 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.encoder.rnn_type         : lstm
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : False
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.encoder.bidirectional    : False
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.2
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 1
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.decoder.rnn_type         : lstm
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : False
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.decoder.emb_scale        : False
2021-10-03 13:29:47,104 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 1024
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.2
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_dropout   : 0.2
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 1
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - cfg.model.decoder.input_feeding    : True
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - cfg.model.decoder.init_hidden      : bridge
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - cfg.model.decoder.attention        : bahdanau
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - Data set sizes: 
	train 5145,
	valid 551,
	test 602
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - First training example:
	[SRC] el
	[TRG] en
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) to (8) the (9) na
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) to (8) the (9) na
2021-10-03 13:29:47,105 - INFO - joeynmt.helpers - Number of Src words (types): 4104
2021-10-03 13:29:47,106 - INFO - joeynmt.helpers - Number of Trg words (types): 4104
2021-10-03 13:29:47,106 - INFO - joeynmt.training - Model(
	encoder=RecurrentEncoder(LSTM(512, 512, batch_first=True)),
	decoder=RecurrentDecoder(rnn=LSTM(1536, 1024, batch_first=True), attention=BahdanauAttention),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4104),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4104))
2021-10-03 13:29:47,116 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-03 13:29:47,116 - INFO - joeynmt.training - EPOCH 1
2021-10-03 13:30:00,664 - INFO - joeynmt.training - Epoch   1: total training loss 8980.58
2021-10-03 13:30:00,665 - INFO - joeynmt.training - EPOCH 2
2021-10-03 13:30:10,237 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:30:10,237 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:30:10,238 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:30:10,242 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:30:11,162 - INFO - joeynmt.training - Example #0
2021-10-03 13:30:11,163 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:30:11,163 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:30:11,163 - INFO - joeynmt.training - 	Hypothesis: I the .
2021-10-03 13:30:11,163 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:30:11,166 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:30:11,167 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:30:11,167 - INFO - joeynmt.training - 	Hypothesis: "And the the the the the the the , , , , the the the the the the the
2021-10-03 13:30:11,167 - INFO - joeynmt.training - Example #2
2021-10-03 13:30:11,167 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:30:11,167 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:30:11,167 - INFO - joeynmt.training - 	Hypothesis: "And the the the the the the the the the the the the .
2021-10-03 13:30:11,167 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      100: bleu:   1.60, loss: 93748.7031, ppl: 491.8930, duration: 3.2320s
2021-10-03 13:30:18,739 - INFO - joeynmt.training - Epoch   2: total training loss 8398.12
2021-10-03 13:30:18,740 - INFO - joeynmt.training - EPOCH 3
2021-10-03 13:30:32,131 - INFO - joeynmt.training - Epoch   3: total training loss 8155.23
2021-10-03 13:30:32,132 - INFO - joeynmt.training - EPOCH 4
2021-10-03 13:30:36,697 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:30:36,697 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:30:36,697 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:30:36,704 - INFO - joeynmt.training - Example #0
2021-10-03 13:30:36,705 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:30:36,705 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:30:36,705 - INFO - joeynmt.training - 	Hypothesis: ( Applause
2021-10-03 13:30:36,705 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:30:36,706 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:30:36,707 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:30:36,707 - INFO - joeynmt.training - 	Hypothesis: "And I the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the ."
2021-10-03 13:30:36,707 - INFO - joeynmt.training - Example #2
2021-10-03 13:30:36,707 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:30:36,707 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:30:36,707 - INFO - joeynmt.training - 	Hypothesis: "And I the the the the the the the the the the the the the the the the the the the the the the ."
2021-10-03 13:30:36,707 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step      200: bleu:   0.83, loss: 89859.1875, ppl: 380.3544, duration: 3.5239s
2021-10-03 13:30:50,543 - INFO - joeynmt.training - Epoch   4: total training loss 7907.18
2021-10-03 13:30:50,543 - INFO - joeynmt.training - EPOCH 5
2021-10-03 13:31:02,329 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:31:02,329 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:31:02,329 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:31:02,335 - INFO - joeynmt.training - Example #0
2021-10-03 13:31:02,336 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:31:02,336 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:31:02,336 - INFO - joeynmt.training - 	Hypothesis: ( Applause )
2021-10-03 13:31:02,336 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:31:02,338 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:31:02,339 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:31:02,339 - INFO - joeynmt.training - 	Hypothesis: "And I we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we we have to the the the the the ."
2021-10-03 13:31:02,339 - INFO - joeynmt.training - Example #2
2021-10-03 13:31:02,339 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:31:02,339 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:31:02,339 - INFO - joeynmt.training - 	Hypothesis: "And I we we we we we we we we we we we we we we we we we we we we we we we have to the a a the the the ."
2021-10-03 13:31:02,339 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step      300: bleu:   0.40, loss: 87000.3750, ppl: 314.8483, duration: 3.4880s
2021-10-03 13:31:09,052 - INFO - joeynmt.training - Epoch   5: total training loss 7686.27
2021-10-03 13:31:09,052 - INFO - joeynmt.training - EPOCH 6
2021-10-03 13:31:22,472 - INFO - joeynmt.training - Epoch   6: total training loss 7390.38
2021-10-03 13:31:22,473 - INFO - joeynmt.training - EPOCH 7
2021-10-03 13:31:27,984 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:31:27,984 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:31:27,984 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:31:27,989 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:31:28,905 - INFO - joeynmt.helpers - delete models/LSTM_el_r/100.ckpt
2021-10-03 13:31:28,980 - INFO - joeynmt.helpers - delete /home/lcur0009/joeynmt/models/LSTM_el_r/100.ckpt
2021-10-03 13:31:28,980 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0009/joeynmt/models/LSTM_el_r/100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0009/joeynmt/models/LSTM_el_r/100.ckpt')
2021-10-03 13:31:28,982 - INFO - joeynmt.training - Example #0
2021-10-03 13:31:28,983 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:31:28,983 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:31:28,983 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a world .
2021-10-03 13:31:28,983 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:31:28,984 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:31:28,984 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:31:28,984 - INFO - joeynmt.training - 	Hypothesis: "And I think , I &apos;re a little sc--world , and I &apos;re a world , I &apos;re a world , the world ."
2021-10-03 13:31:28,984 - INFO - joeynmt.training - Example #2
2021-10-03 13:31:28,984 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:31:28,984 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:31:28,985 - INFO - joeynmt.training - 	Hypothesis: "And I think to be a world , I &apos;re a world , I &apos;re a world , I &apos;re a world ."
2021-10-03 13:31:28,985 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step      400: bleu:   2.42, loss: 81829.5703, ppl: 223.6807, duration: 4.4293s
2021-10-03 13:31:41,756 - INFO - joeynmt.training - Epoch   7: total training loss 7015.18
2021-10-03 13:31:41,757 - INFO - joeynmt.training - EPOCH 8
2021-10-03 13:31:54,724 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:31:54,724 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:31:54,724 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:31:54,729 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:31:55,618 - INFO - joeynmt.helpers - delete models/LSTM_el_r/400.ckpt
2021-10-03 13:31:55,691 - INFO - joeynmt.helpers - delete /home/lcur0009/joeynmt/models/LSTM_el_r/400.ckpt
2021-10-03 13:31:55,691 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0009/joeynmt/models/LSTM_el_r/400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0009/joeynmt/models/LSTM_el_r/400.ckpt')
2021-10-03 13:31:55,693 - INFO - joeynmt.training - Example #0
2021-10-03 13:31:55,693 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:31:55,693 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:31:55,693 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a world .
2021-10-03 13:31:55,693 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:31:55,694 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:31:55,694 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:31:55,694 - INFO - joeynmt.training - 	Hypothesis: "And I think , I can be a little way , I can be a way , and I can be a way ."
2021-10-03 13:31:55,694 - INFO - joeynmt.training - Example #2
2021-10-03 13:31:55,695 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:31:55,695 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:31:55,695 - INFO - joeynmt.training - 	Hypothesis: "And I think , I can be a lot of the world , and I can be a way ."
2021-10-03 13:31:55,695 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step      500: bleu:   2.99, loss: 78704.4219, ppl: 181.9256, duration: 4.6374s
2021-10-03 13:32:01,207 - INFO - joeynmt.training - Epoch   8: total training loss 6793.26
2021-10-03 13:32:01,208 - INFO - joeynmt.training - EPOCH 9
2021-10-03 13:32:14,725 - INFO - joeynmt.training - Epoch   9: total training loss 6589.49
2021-10-03 13:32:14,725 - INFO - joeynmt.training - EPOCH 10
2021-10-03 13:32:21,284 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:32:21,285 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:32:21,285 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:32:21,290 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:32:22,202 - INFO - joeynmt.helpers - delete models/LSTM_el_r/500.ckpt
2021-10-03 13:32:22,275 - INFO - joeynmt.helpers - delete /home/lcur0009/joeynmt/models/LSTM_el_r/500.ckpt
2021-10-03 13:32:22,275 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0009/joeynmt/models/LSTM_el_r/500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0009/joeynmt/models/LSTM_el_r/500.ckpt')
2021-10-03 13:32:22,278 - INFO - joeynmt.training - Example #0
2021-10-03 13:32:22,278 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:32:22,278 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:32:22,278 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot .
2021-10-03 13:32:22,278 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:32:22,280 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:32:22,281 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:32:22,281 - INFO - joeynmt.training - 	Hypothesis: "And I think , I was a lot of the universe , I was a lot of the universe , I have a lot of the world ."
2021-10-03 13:32:22,281 - INFO - joeynmt.training - Example #2
2021-10-03 13:32:22,281 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:32:22,281 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:32:22,281 - INFO - joeynmt.training - 	Hypothesis: "And I think , I was a lot of the world , I was a lot of the world ."
2021-10-03 13:32:22,281 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step      600: bleu:   4.33, loss: 77563.4219, ppl: 168.7064, duration: 4.4560s
2021-10-03 13:32:34,014 - INFO - joeynmt.training - Epoch  10: total training loss 6427.97
2021-10-03 13:32:34,014 - INFO - joeynmt.training - EPOCH 11
2021-10-03 13:32:47,911 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:32:47,911 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:32:47,911 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:32:47,916 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:32:48,824 - INFO - joeynmt.helpers - delete models/LSTM_el_r/600.ckpt
2021-10-03 13:32:48,892 - INFO - joeynmt.helpers - delete /home/lcur0009/joeynmt/models/LSTM_el_r/600.ckpt
2021-10-03 13:32:48,892 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0009/joeynmt/models/LSTM_el_r/600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0009/joeynmt/models/LSTM_el_r/600.ckpt')
2021-10-03 13:32:48,894 - INFO - joeynmt.training - Example #0
2021-10-03 13:32:48,894 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:32:48,894 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:32:48,894 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot .
2021-10-03 13:32:48,894 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:32:48,895 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:32:48,895 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:32:48,895 - INFO - joeynmt.training - 	Hypothesis: "And I was a little end , and I was a little bit of the world , and I was a lot of the world ."
2021-10-03 13:32:48,895 - INFO - joeynmt.training - Example #2
2021-10-03 13:32:48,895 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:32:48,895 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:32:48,896 - INFO - joeynmt.training - 	Hypothesis: "And I was a little bit , and I was a lot of the world , and I was a lot of the world ."
2021-10-03 13:32:48,896 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step      700: bleu:   4.55, loss: 76355.9531, ppl: 155.7617, duration: 4.4305s
2021-10-03 13:32:53,249 - INFO - joeynmt.training - Epoch  11: total training loss 6291.91
2021-10-03 13:32:53,249 - INFO - joeynmt.training - EPOCH 12
2021-10-03 13:33:06,650 - INFO - joeynmt.training - Epoch  12: total training loss 6141.19
2021-10-03 13:33:06,651 - INFO - joeynmt.training - EPOCH 13
2021-10-03 13:33:13,996 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:33:13,996 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:33:13,996 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:33:14,002 - INFO - joeynmt.training - Example #0
2021-10-03 13:33:14,003 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:33:14,003 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:33:14,003 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot .
2021-10-03 13:33:14,003 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:33:14,005 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:33:14,005 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:33:14,005 - INFO - joeynmt.training - 	Hypothesis: "And if you have a little bit of the world , and I was a little bit of the world , I was a bit ."
2021-10-03 13:33:14,005 - INFO - joeynmt.training - Example #2
2021-10-03 13:33:14,005 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:33:14,005 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:33:14,005 - INFO - joeynmt.training - 	Hypothesis: "I was a little bit , and I was a little bit of the world , I was a bit ."
2021-10-03 13:33:14,006 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step      800: bleu:   4.23, loss: 75539.2734, ppl: 147.5744, duration: 3.2194s
2021-10-03 13:33:24,678 - INFO - joeynmt.training - Epoch  13: total training loss 6005.54
2021-10-03 13:33:24,678 - INFO - joeynmt.training - EPOCH 14
2021-10-03 13:33:39,073 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:33:39,073 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:33:39,073 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:33:39,077 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:33:40,238 - INFO - joeynmt.helpers - delete models/LSTM_el_r/700.ckpt
2021-10-03 13:33:40,310 - INFO - joeynmt.helpers - delete /home/lcur0009/joeynmt/models/LSTM_el_r/700.ckpt
2021-10-03 13:33:40,311 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0009/joeynmt/models/LSTM_el_r/700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0009/joeynmt/models/LSTM_el_r/700.ckpt')
2021-10-03 13:33:40,312 - INFO - joeynmt.training - Example #0
2021-10-03 13:33:40,312 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:33:40,312 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:33:40,313 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot .
2021-10-03 13:33:40,313 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:33:40,315 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:33:40,315 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:33:40,315 - INFO - joeynmt.training - 	Hypothesis: "And I was a little bit of the world , I was going to be a lot of the world , I was a lot of the world ."
2021-10-03 13:33:40,315 - INFO - joeynmt.training - Example #2
2021-10-03 13:33:40,315 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:33:40,316 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:33:40,316 - INFO - joeynmt.training - 	Hypothesis: "I was a little bit of the first , and I was a lot of the same thing ."
2021-10-03 13:33:40,316 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step      900: bleu:   4.99, loss: 74844.5938, ppl: 140.9497, duration: 4.1632s
2021-10-03 13:33:43,642 - INFO - joeynmt.training - Epoch  14: total training loss 5862.45
2021-10-03 13:33:43,642 - INFO - joeynmt.training - EPOCH 15
2021-10-03 13:33:57,076 - INFO - joeynmt.training - Epoch  15: total training loss 5740.42
2021-10-03 13:33:57,076 - INFO - joeynmt.training - EPOCH 16
2021-10-03 13:34:02,341 - INFO - joeynmt.training - Epoch  16, Step:     1000, Batch Loss:    80.108177, Tokens per Sec:     8156, Lr: 0.000300
2021-10-03 13:34:05,457 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:34:05,457 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:34:05,457 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:34:05,464 - INFO - joeynmt.training - Example #0
2021-10-03 13:34:05,464 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:34:05,464 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:34:05,464 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot .
2021-10-03 13:34:05,464 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:34:05,465 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:34:05,466 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:34:05,466 - INFO - joeynmt.training - 	Hypothesis: "And I was a little bit of the hirst , I was a lot of the same thing , I was a lot of the world ."
2021-10-03 13:34:05,466 - INFO - joeynmt.training - Example #2
2021-10-03 13:34:05,466 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:34:05,466 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:34:05,466 - INFO - joeynmt.training - 	Hypothesis: "I was the first , I was a little bit of the first , I was a lot of the brain ."
2021-10-03 13:34:05,466 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     1000: bleu:   4.98, loss: 74551.5078, ppl: 138.2447, duration: 3.1248s
2021-10-03 13:34:14,941 - INFO - joeynmt.training - Epoch  16: total training loss 5611.64
2021-10-03 13:34:14,942 - INFO - joeynmt.training - EPOCH 17
2021-10-03 13:34:30,312 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:34:30,312 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:34:30,313 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:34:30,317 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:34:31,205 - INFO - joeynmt.helpers - delete models/LSTM_el_r/900.ckpt
2021-10-03 13:34:31,277 - INFO - joeynmt.helpers - delete /home/lcur0009/joeynmt/models/LSTM_el_r/900.ckpt
2021-10-03 13:34:31,278 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0009/joeynmt/models/LSTM_el_r/900.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0009/joeynmt/models/LSTM_el_r/900.ckpt')
2021-10-03 13:34:31,280 - INFO - joeynmt.training - Example #0
2021-10-03 13:34:31,280 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:34:31,280 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:34:31,280 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot .
2021-10-03 13:34:31,280 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:34:31,282 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:34:31,282 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:34:31,283 - INFO - joeynmt.training - 	Hypothesis: "And I was a child , I was a few in my village , I was a lot of a village ."
2021-10-03 13:34:31,283 - INFO - joeynmt.training - Example #2
2021-10-03 13:34:31,283 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:34:31,283 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:34:31,283 - INFO - joeynmt.training - 	Hypothesis: "He was a child , I was a school , I was a school , I was a school ."
2021-10-03 13:34:31,283 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step     1100: bleu:   5.18, loss: 74122.9141, ppl: 134.3823, duration: 3.9500s
2021-10-03 13:34:33,483 - INFO - joeynmt.training - Epoch  17: total training loss 5499.31
2021-10-03 13:34:33,483 - INFO - joeynmt.training - EPOCH 18
2021-10-03 13:34:46,939 - INFO - joeynmt.training - Epoch  18: total training loss 5399.64
2021-10-03 13:34:46,939 - INFO - joeynmt.training - EPOCH 19
2021-10-03 13:34:56,239 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:34:56,240 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:34:56,240 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:34:56,246 - INFO - joeynmt.training - Example #0
2021-10-03 13:34:56,246 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:34:56,246 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:34:56,247 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot .
2021-10-03 13:34:56,247 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:34:56,248 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:34:56,248 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:34:56,249 - INFO - joeynmt.training - 	Hypothesis: "And I was a child , I was a couple of a village , I was a lot of a village ."
2021-10-03 13:34:56,249 - INFO - joeynmt.training - Example #2
2021-10-03 13:34:56,249 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:34:56,249 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:34:56,249 - INFO - joeynmt.training - 	Hypothesis: "He was the first time , I was a school , &quot; &quot; Cwas a mother was a village ."
2021-10-03 13:34:56,250 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     1200: bleu:   5.08, loss: 74218.0469, ppl: 135.2301, duration: 3.0887s
2021-10-03 13:35:04,742 - INFO - joeynmt.training - Epoch  19: total training loss 5270.57
2021-10-03 13:35:04,743 - INFO - joeynmt.training - EPOCH 20
2021-10-03 13:35:21,008 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:35:21,008 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:35:21,008 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:35:21,013 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:35:21,901 - INFO - joeynmt.helpers - delete models/LSTM_el_r/1100.ckpt
2021-10-03 13:35:21,973 - INFO - joeynmt.helpers - delete /home/lcur0009/joeynmt/models/LSTM_el_r/1100.ckpt
2021-10-03 13:35:21,974 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0009/joeynmt/models/LSTM_el_r/1100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0009/joeynmt/models/LSTM_el_r/1100.ckpt')
2021-10-03 13:35:21,976 - INFO - joeynmt.training - Example #0
2021-10-03 13:35:21,976 - INFO - joeynmt.training - 	Source:     el
2021-10-03 13:35:21,976 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:35:21,976 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very case .
2021-10-03 13:35:21,976 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 73: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u038ctan', 'imoyn', '1@@', '1', ',', 'Th@@', 'u@@', 'ma@@', 'mai', 'pos', 'x@@', 'yp@@', 'n@@', 'isa', 'ena', 'pro@@', 'i', 'me', 'ich@@', 'oys', 'char@@', 'as', 'sto', 'spiti', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u038c' in position 66: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0009/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0009/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."',)
2021-10-03 13:35:21,977 - INFO - joeynmt.training - 	Source:     "\u038ctan imoyn 11 , Thumamai pos xypnisa ena proi me ichoys charas sto spiti ."
2021-10-03 13:35:21,977 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:35:21,977 - INFO - joeynmt.training - 	Hypothesis: "And in course , I was a little bit of the hols of the holart in the world ."
2021-10-03 13:35:21,977 - INFO - joeynmt.training - Example #2
2021-10-03 13:35:21,977 - INFO - joeynmt.training - 	Source:     O pateras moy akoyge ta Nea toy BBC sto mikro gkri radiofono toy .
2021-10-03 13:35:21,977 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:35:21,978 - INFO - joeynmt.training - 	Hypothesis: "He was a mother , I was a school , the first in the first in the Wold ."
2021-10-03 13:35:21,978 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     1300: bleu:   5.32, loss: 74321.8281, ppl: 136.1613, duration: 3.7187s
2021-10-03 13:35:23,236 - INFO - joeynmt.training - Epoch  20: total training loss 5149.15
2021-10-03 13:35:23,242 - INFO - joeynmt.training - Training ended after  20 epochs.
2021-10-03 13:35:23,242 - INFO - joeynmt.training - Best validation result (greedy) at step     1300:   5.32 eval_metric.
2021-10-03 13:35:23,269 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-03 13:35:23,269 - INFO - joeynmt.prediction - Loading model from models/LSTM_el_r/1300.ckpt
2021-10-03 13:35:23,580 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:35:24,098 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:35:24,153 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/el_r.en_s/val.bpe.en_s)...
2021-10-03 13:35:30,208 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:35:30,208 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:35:30,208 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:35:30,212 - INFO - joeynmt.prediction -  dev bleu[13a]:   5.97 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:35:30,214 - INFO - joeynmt.prediction - Translations saved to: models/LSTM_el_r/00001300.hyps.dev
2021-10-03 13:35:30,215 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/el_r.en_s/test.bpe.en_s)...
2021-10-03 13:35:37,344 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:35:37,344 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:35:37,344 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:35:37,348 - INFO - joeynmt.prediction - test bleu[13a]:   7.22 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:35:37,350 - INFO - joeynmt.prediction - Translations saved to: models/LSTM_el_r/00001300.hyps.test
