2021-10-03 14:17:42,641 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-03 14:17:42,701 - INFO - joeynmt.data - Loading training data...
2021-10-03 14:17:42,802 - INFO - joeynmt.data - Building vocabulary...
2021-10-03 14:17:43,233 - INFO - joeynmt.data - Loading dev data...
2021-10-03 14:17:43,246 - INFO - joeynmt.data - Loading test data...
2021-10-03 14:17:43,256 - INFO - joeynmt.data - Data loaded.
2021-10-03 14:17:43,256 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 14:17:44,100 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 14:17:44,111 - INFO - joeynmt.training - Total params: 33654784
2021-10-03 14:17:48,100 - INFO - joeynmt.helpers - cfg.name                           : TRANS_th_r
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.src                       : th_r
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.trg                       : en_s
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/th_r.en_s/train.bpe
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/th_r.en_s/val.bpe
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/th_r.en_s/test.bpe
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-03 14:17:48,101 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 10000
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 10000
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/th_r.en_s/th_r.en_s.vocab.txt
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/th_r.en_s/th_r.en_s.vocab.txt
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.patience              : 1
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.epochs                : 100
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-03 14:17:48,102 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/TRANS_th_r
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 60
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-10-03 14:17:48,103 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024
2021-10-03 14:17:48,104 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3
2021-10-03 14:17:48,105 - INFO - joeynmt.helpers - Data set sizes: 
	train 5168,
	valid 551,
	test 602
2021-10-03 14:17:48,105 - INFO - joeynmt.helpers - First training example:
	[SRC] th
	[TRG] en
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e47' in position 120: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 835, in train
    trg_vocab=trg_vocab)
  File "/home/lcur0008/joeynmt/joeynmt/helpers.py", line 164, in log_data_info
    " ".join('(%d) %s' % (i, t) for i, t in enumerate(src_vocab.itos[:10])))
Message: 'First 10 words (src): %s'
Arguments: ('(0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) \u0e47 (6) the (7) . (8) ." (9) of',)
2021-10-03 14:17:48,105 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) \u0e47 (6) the (7) . (8) ." (9) of
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e47' in position 120: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 835, in train
    trg_vocab=trg_vocab)
  File "/home/lcur0008/joeynmt/joeynmt/helpers.py", line 167, in log_data_info
    " ".join('(%d) %s' % (i, t) for i, t in enumerate(trg_vocab.itos[:10])))
Message: 'First 10 words (trg): %s'
Arguments: ('(0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) \u0e47 (6) the (7) . (8) ." (9) of',)
2021-10-03 14:17:48,107 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) \u0e47 (6) the (7) . (8) ." (9) of
2021-10-03 14:17:48,107 - INFO - joeynmt.helpers - Number of Src words (types): 4120
2021-10-03 14:17:48,108 - INFO - joeynmt.helpers - Number of Trg words (types): 4120
2021-10-03 14:17:48,108 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4120),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4120))
2021-10-03 14:17:48,117 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-03 14:17:48,117 - INFO - joeynmt.training - EPOCH 1
2021-10-03 14:18:00,413 - INFO - joeynmt.training - Epoch   1: total training loss 9081.34
2021-10-03 14:18:00,414 - INFO - joeynmt.training - EPOCH 2
2021-10-03 14:18:12,621 - INFO - joeynmt.training - Epoch   2: total training loss 8325.45
2021-10-03 14:18:12,622 - INFO - joeynmt.training - EPOCH 3
2021-10-03 14:18:24,785 - INFO - joeynmt.training - Epoch   3: total training loss 8016.28
2021-10-03 14:18:24,786 - INFO - joeynmt.training - EPOCH 4
2021-10-03 14:18:36,974 - INFO - joeynmt.training - Epoch   4: total training loss 7723.74
2021-10-03 14:18:36,975 - INFO - joeynmt.training - EPOCH 5
2021-10-03 14:18:49,165 - INFO - joeynmt.training - Epoch   5: total training loss 7347.41
2021-10-03 14:18:49,165 - INFO - joeynmt.training - EPOCH 6
2021-10-03 14:19:01,321 - INFO - joeynmt.training - Epoch   6: total training loss 7113.99
2021-10-03 14:19:01,322 - INFO - joeynmt.training - EPOCH 7
2021-10-03 14:19:13,496 - INFO - joeynmt.training - Epoch   7: total training loss 6936.98
2021-10-03 14:19:13,497 - INFO - joeynmt.training - EPOCH 8
2021-10-03 14:19:25,718 - INFO - joeynmt.training - Epoch   8: total training loss 6772.68
2021-10-03 14:19:25,719 - INFO - joeynmt.training - EPOCH 9
2021-10-03 14:19:37,984 - INFO - joeynmt.training - Epoch   9: total training loss 6607.01
2021-10-03 14:19:37,984 - INFO - joeynmt.training - EPOCH 10
2021-10-03 14:19:50,285 - INFO - joeynmt.training - Epoch  10: total training loss 6458.16
2021-10-03 14:19:50,286 - INFO - joeynmt.training - EPOCH 11
2021-10-03 14:20:02,623 - INFO - joeynmt.training - Epoch  11: total training loss 6313.17
2021-10-03 14:20:02,624 - INFO - joeynmt.training - EPOCH 12
2021-10-03 14:20:14,895 - INFO - joeynmt.training - Epoch  12: total training loss 6156.53
2021-10-03 14:20:14,896 - INFO - joeynmt.training - EPOCH 13
2021-10-03 14:20:27,043 - INFO - joeynmt.training - Epoch  13: total training loss 5989.30
2021-10-03 14:20:27,043 - INFO - joeynmt.training - EPOCH 14
2021-10-03 14:20:39,241 - INFO - joeynmt.training - Epoch  14: total training loss 5830.85
2021-10-03 14:20:39,242 - INFO - joeynmt.training - EPOCH 15
2021-10-03 14:20:51,342 - INFO - joeynmt.training - Epoch  15: total training loss 5676.84
2021-10-03 14:20:51,343 - INFO - joeynmt.training - EPOCH 16
2021-10-03 14:20:56,023 - INFO - joeynmt.training - Epoch  16, Step:     1000, Batch Loss:    74.929405, Tokens per Sec:     9180, Lr: 0.000300
2021-10-03 14:21:08,435 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 14:21:08,435 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 14:21:08,435 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 14:21:08,440 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 14:21:09,654 - INFO - joeynmt.training - Example #0
2021-10-03 14:21:09,654 - INFO - joeynmt.training - 	Source:     th
2021-10-03 14:21:09,655 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 14:21:09,655 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 14:21:09,655 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 93: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ton@@', 'chan@@', 'ai\u0e38', '1@@', '1', 'chan@@', 'cham@@', 'dai', 'w', '\u0e32@@', 'tue', 'n@@', 'kue', 'nma@@', 'che', '\u0e32w@@', 'an@@', 'nue', 'ng', 'lae@@', 'dai', 'yin@@', 'siang@@', 'r', 'ong@@', 'd', 'wokham@@', 'inti@@', 'nai@@', 'b', '\u0e32n'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n',)
2021-10-03 14:21:09,656 - INFO - joeynmt.training - 	Source:     tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n
2021-10-03 14:21:09,657 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 14:21:09,657 - INFO - joeynmt.training - 	Hypothesis: "I was born , I was a woman , I was a few years of the first , and I was a few years of the first , and I was a few years ."
2021-10-03 14:21:09,658 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 122: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ph', 'khongchan@@', 'kamnang@@', 'fang@@', 'kh', '\u0e32w@@', 'bi@@', 'bit@@', 'i', 'chak@@', 'wit@@', 'u@@', 'sit@@', 'ao@@', 'an@@', 'le', '\u0e47', 'k'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 91: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k',)
2021-10-03 14:21:09,658 - INFO - joeynmt.training - 	Source:     ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k
2021-10-03 14:21:09,659 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 14:21:09,659 - INFO - joeynmt.training - 	Hypothesis: And I was a few years of the first .
2021-10-03 14:21:09,659 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     1000: bleu:   5.00, loss: 71612.4609, ppl: 117.1026, duration: 13.6351s
2021-10-03 14:21:17,152 - INFO - joeynmt.training - Epoch  16: total training loss 5515.94
2021-10-03 14:21:17,152 - INFO - joeynmt.training - EPOCH 17
2021-10-03 14:21:29,352 - INFO - joeynmt.training - Epoch  17: total training loss 5360.70
2021-10-03 14:21:29,353 - INFO - joeynmt.training - EPOCH 18
2021-10-03 14:21:41,595 - INFO - joeynmt.training - Epoch  18: total training loss 5211.79
2021-10-03 14:21:41,596 - INFO - joeynmt.training - EPOCH 19
2021-10-03 14:21:53,860 - INFO - joeynmt.training - Epoch  19: total training loss 5074.47
2021-10-03 14:21:53,861 - INFO - joeynmt.training - EPOCH 20
2021-10-03 14:22:06,083 - INFO - joeynmt.training - Epoch  20: total training loss 4921.55
2021-10-03 14:22:06,084 - INFO - joeynmt.training - EPOCH 21
2021-10-03 14:22:18,414 - INFO - joeynmt.training - Epoch  21: total training loss 4773.07
2021-10-03 14:22:18,415 - INFO - joeynmt.training - EPOCH 22
2021-10-03 14:22:30,760 - INFO - joeynmt.training - Epoch  22: total training loss 4656.38
2021-10-03 14:22:30,761 - INFO - joeynmt.training - EPOCH 23
2021-10-03 14:22:43,076 - INFO - joeynmt.training - Epoch  23: total training loss 4517.94
2021-10-03 14:22:43,077 - INFO - joeynmt.training - EPOCH 24
2021-10-03 14:22:55,363 - INFO - joeynmt.training - Epoch  24: total training loss 4387.68
2021-10-03 14:22:55,364 - INFO - joeynmt.training - EPOCH 25
2021-10-03 14:23:07,621 - INFO - joeynmt.training - Epoch  25: total training loss 4260.46
2021-10-03 14:23:07,621 - INFO - joeynmt.training - EPOCH 26
2021-10-03 14:23:19,861 - INFO - joeynmt.training - Epoch  26: total training loss 4140.48
2021-10-03 14:23:19,862 - INFO - joeynmt.training - EPOCH 27
2021-10-03 14:23:32,170 - INFO - joeynmt.training - Epoch  27: total training loss 4028.99
2021-10-03 14:23:32,171 - INFO - joeynmt.training - EPOCH 28
2021-10-03 14:23:44,485 - INFO - joeynmt.training - Epoch  28: total training loss 3907.60
2021-10-03 14:23:44,486 - INFO - joeynmt.training - EPOCH 29
2021-10-03 14:23:56,716 - INFO - joeynmt.training - Epoch  29: total training loss 3789.83
2021-10-03 14:23:56,716 - INFO - joeynmt.training - EPOCH 30
2021-10-03 14:24:09,031 - INFO - joeynmt.training - Epoch  30: total training loss 3679.51
2021-10-03 14:24:09,032 - INFO - joeynmt.training - EPOCH 31
2021-10-03 14:24:18,562 - INFO - joeynmt.training - Epoch  31, Step:     2000, Batch Loss:    52.674648, Tokens per Sec:     8931, Lr: 0.000300
2021-10-03 14:24:30,089 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 14:24:30,090 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 14:24:30,090 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 14:24:30,095 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 14:24:31,329 - INFO - joeynmt.helpers - delete models/TRANS_th_r/1000.ckpt
2021-10-03 14:24:31,423 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/TRANS_th_r/1000.ckpt
2021-10-03 14:24:31,424 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/TRANS_th_r/1000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/TRANS_th_r/1000.ckpt')
2021-10-03 14:24:31,426 - INFO - joeynmt.training - Example #0
2021-10-03 14:24:31,426 - INFO - joeynmt.training - 	Source:     th
2021-10-03 14:24:31,426 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 14:24:31,426 - INFO - joeynmt.training - 	Hypothesis: You have .
2021-10-03 14:24:31,426 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 93: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ton@@', 'chan@@', 'ai\u0e38', '1@@', '1', 'chan@@', 'cham@@', 'dai', 'w', '\u0e32@@', 'tue', 'n@@', 'kue', 'nma@@', 'che', '\u0e32w@@', 'an@@', 'nue', 'ng', 'lae@@', 'dai', 'yin@@', 'siang@@', 'r', 'ong@@', 'd', 'wokham@@', 'inti@@', 'nai@@', 'b', '\u0e32n'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n',)
2021-10-03 14:24:31,427 - INFO - joeynmt.training - 	Source:     tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n
2021-10-03 14:24:31,428 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 14:24:31,428 - INFO - joeynmt.training - 	Hypothesis: "I was 27 years later , I was a few years ago , I was going to be able to read and came back back from the final ."
2021-10-03 14:24:31,428 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 122: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ph', 'khongchan@@', 'kamnang@@', 'fang@@', 'kh', '\u0e32w@@', 'bi@@', 'bit@@', 'i', 'chak@@', 'wit@@', 'u@@', 'sit@@', 'ao@@', 'an@@', 'le', '\u0e47', 'k'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 91: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k',)
2021-10-03 14:24:31,428 - INFO - joeynmt.training - 	Source:     ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k
2021-10-03 14:24:31,429 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 14:24:31,429 - INFO - joeynmt.training - 	Hypothesis: "I was born by the first girl girl girl , which is a fourth ."
2021-10-03 14:24:31,429 - INFO - joeynmt.training - Validation result (greedy) at epoch  31, step     2000: bleu:   8.09, loss: 72424.0234, ppl: 123.5973, duration: 12.8665s
2021-10-03 14:24:34,213 - INFO - joeynmt.training - Epoch  31: total training loss 3565.98
2021-10-03 14:24:34,214 - INFO - joeynmt.training - EPOCH 32
2021-10-03 14:24:46,444 - INFO - joeynmt.training - Epoch  32: total training loss 3454.09
2021-10-03 14:24:46,444 - INFO - joeynmt.training - EPOCH 33
2021-10-03 14:24:58,693 - INFO - joeynmt.training - Epoch  33: total training loss 3349.96
2021-10-03 14:24:58,694 - INFO - joeynmt.training - EPOCH 34
2021-10-03 14:25:10,995 - INFO - joeynmt.training - Epoch  34: total training loss 3248.28
2021-10-03 14:25:10,996 - INFO - joeynmt.training - EPOCH 35
2021-10-03 14:25:23,308 - INFO - joeynmt.training - Epoch  35: total training loss 3136.35
2021-10-03 14:25:23,309 - INFO - joeynmt.training - EPOCH 36
2021-10-03 14:25:35,641 - INFO - joeynmt.training - Epoch  36: total training loss 3042.04
2021-10-03 14:25:35,642 - INFO - joeynmt.training - EPOCH 37
2021-10-03 14:25:47,970 - INFO - joeynmt.training - Epoch  37: total training loss 2933.46
2021-10-03 14:25:47,970 - INFO - joeynmt.training - EPOCH 38
2021-10-03 14:26:00,281 - INFO - joeynmt.training - Epoch  38: total training loss 2837.56
2021-10-03 14:26:00,282 - INFO - joeynmt.training - EPOCH 39
2021-10-03 14:26:12,514 - INFO - joeynmt.training - Epoch  39: total training loss 2740.27
2021-10-03 14:26:12,514 - INFO - joeynmt.training - EPOCH 40
2021-10-03 14:26:24,768 - INFO - joeynmt.training - Epoch  40: total training loss 2650.73
2021-10-03 14:26:24,769 - INFO - joeynmt.training - EPOCH 41
2021-10-03 14:26:37,070 - INFO - joeynmt.training - Epoch  41: total training loss 2559.94
2021-10-03 14:26:37,071 - INFO - joeynmt.training - EPOCH 42
2021-10-03 14:26:49,309 - INFO - joeynmt.training - Epoch  42: total training loss 2473.76
2021-10-03 14:26:49,310 - INFO - joeynmt.training - EPOCH 43
2021-10-03 14:27:01,611 - INFO - joeynmt.training - Epoch  43: total training loss 2385.68
2021-10-03 14:27:01,611 - INFO - joeynmt.training - EPOCH 44
2021-10-03 14:27:13,815 - INFO - joeynmt.training - Epoch  44: total training loss 2295.57
2021-10-03 14:27:13,816 - INFO - joeynmt.training - EPOCH 45
2021-10-03 14:27:26,106 - INFO - joeynmt.training - Epoch  45: total training loss 2215.25
2021-10-03 14:27:26,107 - INFO - joeynmt.training - EPOCH 46
2021-10-03 14:27:38,429 - INFO - joeynmt.training - Epoch  46: total training loss 2128.83
2021-10-03 14:27:38,430 - INFO - joeynmt.training - EPOCH 47
2021-10-03 14:27:40,358 - INFO - joeynmt.training - Epoch  47, Step:     3000, Batch Loss:    28.158106, Tokens per Sec:     8735, Lr: 0.000300
2021-10-03 14:27:51,363 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 14:27:51,364 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 14:27:51,364 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 14:27:51,369 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 14:27:52,630 - INFO - joeynmt.helpers - delete models/TRANS_th_r/2000.ckpt
2021-10-03 14:27:52,718 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/TRANS_th_r/2000.ckpt
2021-10-03 14:27:52,718 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/TRANS_th_r/2000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/TRANS_th_r/2000.ckpt')
2021-10-03 14:27:52,721 - INFO - joeynmt.training - Example #0
2021-10-03 14:27:52,721 - INFO - joeynmt.training - 	Source:     th
2021-10-03 14:27:52,721 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 14:27:52,721 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 14:27:52,721 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 93: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ton@@', 'chan@@', 'ai\u0e38', '1@@', '1', 'chan@@', 'cham@@', 'dai', 'w', '\u0e32@@', 'tue', 'n@@', 'kue', 'nma@@', 'che', '\u0e32w@@', 'an@@', 'nue', 'ng', 'lae@@', 'dai', 'yin@@', 'siang@@', 'r', 'ong@@', 'd', 'wokham@@', 'inti@@', 'nai@@', 'b', '\u0e32n'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n',)
2021-10-03 14:27:52,723 - INFO - joeynmt.training - 	Source:     tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n
2021-10-03 14:27:52,723 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 14:27:52,723 - INFO - joeynmt.training - 	Hypothesis: "And I was 18 , I was recently far up from the work-of-of-of-of-grow an herous trade ."
2021-10-03 14:27:52,723 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 122: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ph', 'khongchan@@', 'kamnang@@', 'fang@@', 'kh', '\u0e32w@@', 'bi@@', 'bit@@', 'i', 'chak@@', 'wit@@', 'u@@', 'sit@@', 'ao@@', 'an@@', 'le', '\u0e47', 'k'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 91: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k',)
2021-10-03 14:27:52,724 - INFO - joeynmt.training - 	Source:     ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k
2021-10-03 14:27:52,724 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 14:27:52,724 - INFO - joeynmt.training - 	Hypothesis: "My secret is that School , tried for reading ."
2021-10-03 14:27:52,724 - INFO - joeynmt.training - Validation result (greedy) at epoch  47, step     3000: bleu:   8.61, loss: 80745.5625, ppl: 214.9717, duration: 12.3660s
2021-10-03 14:28:03,066 - INFO - joeynmt.training - Epoch  47: total training loss 2051.96
2021-10-03 14:28:03,067 - INFO - joeynmt.training - EPOCH 48
2021-10-03 14:28:15,400 - INFO - joeynmt.training - Epoch  48: total training loss 1980.13
2021-10-03 14:28:15,401 - INFO - joeynmt.training - EPOCH 49
2021-10-03 14:28:27,679 - INFO - joeynmt.training - Epoch  49: total training loss 1907.84
2021-10-03 14:28:27,679 - INFO - joeynmt.training - EPOCH 50
2021-10-03 14:28:39,887 - INFO - joeynmt.training - Epoch  50: total training loss 1837.48
2021-10-03 14:28:39,888 - INFO - joeynmt.training - EPOCH 51
2021-10-03 14:28:52,138 - INFO - joeynmt.training - Epoch  51: total training loss 1770.06
2021-10-03 14:28:52,139 - INFO - joeynmt.training - EPOCH 52
2021-10-03 14:29:04,392 - INFO - joeynmt.training - Epoch  52: total training loss 1699.07
2021-10-03 14:29:04,392 - INFO - joeynmt.training - EPOCH 53
2021-10-03 14:29:16,541 - INFO - joeynmt.training - Epoch  53: total training loss 1644.05
2021-10-03 14:29:16,542 - INFO - joeynmt.training - EPOCH 54
2021-10-03 14:29:28,796 - INFO - joeynmt.training - Epoch  54: total training loss 1575.76
2021-10-03 14:29:28,796 - INFO - joeynmt.training - EPOCH 55
2021-10-03 14:29:41,121 - INFO - joeynmt.training - Epoch  55: total training loss 1524.38
2021-10-03 14:29:41,122 - INFO - joeynmt.training - EPOCH 56
2021-10-03 14:29:53,340 - INFO - joeynmt.training - Epoch  56: total training loss 1458.29
2021-10-03 14:29:53,341 - INFO - joeynmt.training - EPOCH 57
2021-10-03 14:30:05,567 - INFO - joeynmt.training - Epoch  57: total training loss 1399.40
2021-10-03 14:30:05,567 - INFO - joeynmt.training - EPOCH 58
2021-10-03 14:30:17,784 - INFO - joeynmt.training - Epoch  58: total training loss 1349.33
2021-10-03 14:30:17,785 - INFO - joeynmt.training - EPOCH 59
2021-10-03 14:30:29,928 - INFO - joeynmt.training - Epoch  59: total training loss 1300.24
2021-10-03 14:30:29,928 - INFO - joeynmt.training - EPOCH 60
2021-10-03 14:30:42,090 - INFO - joeynmt.training - Epoch  60: total training loss 1253.55
2021-10-03 14:30:42,090 - INFO - joeynmt.training - EPOCH 61
2021-10-03 14:30:54,336 - INFO - joeynmt.training - Epoch  61: total training loss 1203.40
2021-10-03 14:30:54,337 - INFO - joeynmt.training - EPOCH 62
2021-10-03 14:31:00,965 - INFO - joeynmt.training - Epoch  62, Step:     4000, Batch Loss:    17.522259, Tokens per Sec:     9049, Lr: 0.000300
2021-10-03 14:31:11,518 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 14:31:11,519 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 14:31:11,519 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 14:31:11,525 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 14:31:12,773 - INFO - joeynmt.helpers - delete models/TRANS_th_r/3000.ckpt
2021-10-03 14:31:12,868 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/TRANS_th_r/3000.ckpt
2021-10-03 14:31:12,868 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/TRANS_th_r/3000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/TRANS_th_r/3000.ckpt')
2021-10-03 14:31:12,870 - INFO - joeynmt.training - Example #0
2021-10-03 14:31:12,871 - INFO - joeynmt.training - 	Source:     th
2021-10-03 14:31:12,871 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 14:31:12,871 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-03 14:31:12,871 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 93: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ton@@', 'chan@@', 'ai\u0e38', '1@@', '1', 'chan@@', 'cham@@', 'dai', 'w', '\u0e32@@', 'tue', 'n@@', 'kue', 'nma@@', 'che', '\u0e32w@@', 'an@@', 'nue', 'ng', 'lae@@', 'dai', 'yin@@', 'siang@@', 'r', 'ong@@', 'd', 'wokham@@', 'inti@@', 'nai@@', 'b', '\u0e32n'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n',)
2021-10-03 14:31:12,872 - INFO - joeynmt.training - 	Source:     tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n
2021-10-03 14:31:12,872 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 14:31:12,872 - INFO - joeynmt.training - 	Hypothesis: "I was 27 , I was now aftered myself , and I was getting healing in my own home , and I was up reak ."
2021-10-03 14:31:12,873 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 122: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ph', 'khongchan@@', 'kamnang@@', 'fang@@', 'kh', '\u0e32w@@', 'bi@@', 'bit@@', 'i', 'chak@@', 'wit@@', 'u@@', 'sit@@', 'ao@@', 'an@@', 'le', '\u0e47', 'k'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 91: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k',)
2021-10-03 14:31:12,873 - INFO - joeynmt.training - 	Source:     ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k
2021-10-03 14:31:12,873 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 14:31:12,873 - INFO - joeynmt.training - 	Hypothesis: "My secret was that Casks , two cats were full ."
2021-10-03 14:31:12,873 - INFO - joeynmt.training - Validation result (greedy) at epoch  62, step     4000: bleu:   8.87, loss: 91631.5391, ppl: 443.4339, duration: 11.9075s
2021-10-03 14:31:18,501 - INFO - joeynmt.training - Epoch  62: total training loss 1169.00
2021-10-03 14:31:18,502 - INFO - joeynmt.training - EPOCH 63
2021-10-03 14:31:30,762 - INFO - joeynmt.training - Epoch  63: total training loss 1123.86
2021-10-03 14:31:30,763 - INFO - joeynmt.training - EPOCH 64
2021-10-03 14:31:43,195 - INFO - joeynmt.training - Epoch  64: total training loss 1081.89
2021-10-03 14:31:43,196 - INFO - joeynmt.training - EPOCH 65
2021-10-03 14:31:55,795 - INFO - joeynmt.training - Epoch  65: total training loss 1054.52
2021-10-03 14:31:55,796 - INFO - joeynmt.training - EPOCH 66
2021-10-03 14:32:07,986 - INFO - joeynmt.training - Epoch  66: total training loss 1007.25
2021-10-03 14:32:07,986 - INFO - joeynmt.training - EPOCH 67
2021-10-03 14:32:20,254 - INFO - joeynmt.training - Epoch  67: total training loss 974.15
2021-10-03 14:32:20,254 - INFO - joeynmt.training - EPOCH 68
2021-10-03 14:32:32,372 - INFO - joeynmt.training - Epoch  68: total training loss 949.20
2021-10-03 14:32:32,372 - INFO - joeynmt.training - EPOCH 69
2021-10-03 14:32:44,501 - INFO - joeynmt.training - Epoch  69: total training loss 918.37
2021-10-03 14:32:44,501 - INFO - joeynmt.training - EPOCH 70
2021-10-03 14:32:56,816 - INFO - joeynmt.training - Epoch  70: total training loss 882.83
2021-10-03 14:32:56,816 - INFO - joeynmt.training - EPOCH 71
2021-10-03 14:33:08,978 - INFO - joeynmt.training - Epoch  71: total training loss 851.95
2021-10-03 14:33:08,979 - INFO - joeynmt.training - EPOCH 72
2021-10-03 14:33:21,156 - INFO - joeynmt.training - Epoch  72: total training loss 828.00
2021-10-03 14:33:21,156 - INFO - joeynmt.training - EPOCH 73
2021-10-03 14:33:33,388 - INFO - joeynmt.training - Epoch  73: total training loss 802.92
2021-10-03 14:33:33,389 - INFO - joeynmt.training - EPOCH 74
2021-10-03 14:33:45,617 - INFO - joeynmt.training - Epoch  74: total training loss 771.41
2021-10-03 14:33:45,618 - INFO - joeynmt.training - EPOCH 75
2021-10-03 14:33:57,895 - INFO - joeynmt.training - Epoch  75: total training loss 759.11
2021-10-03 14:33:57,896 - INFO - joeynmt.training - EPOCH 76
2021-10-03 14:34:10,065 - INFO - joeynmt.training - Epoch  76: total training loss 726.43
2021-10-03 14:34:10,066 - INFO - joeynmt.training - EPOCH 77
2021-10-03 14:34:21,386 - INFO - joeynmt.training - Epoch  77, Step:     5000, Batch Loss:    11.037459, Tokens per Sec:     9032, Lr: 0.000300
2021-10-03 14:34:30,723 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 14:34:30,723 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 14:34:30,723 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 14:34:30,731 - INFO - joeynmt.training - Example #0
2021-10-03 14:34:30,732 - INFO - joeynmt.training - 	Source:     th
2021-10-03 14:34:30,732 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 14:34:30,732 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-03 14:34:30,732 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 93: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ton@@', 'chan@@', 'ai\u0e38', '1@@', '1', 'chan@@', 'cham@@', 'dai', 'w', '\u0e32@@', 'tue', 'n@@', 'kue', 'nma@@', 'che', '\u0e32w@@', 'an@@', 'nue', 'ng', 'lae@@', 'dai', 'yin@@', 'siang@@', 'r', 'ong@@', 'd', 'wokham@@', 'inti@@', 'nai@@', 'b', '\u0e32n'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n',)
2021-10-03 14:34:30,735 - INFO - joeynmt.training - 	Source:     tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n
2021-10-03 14:34:30,735 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 14:34:30,735 - INFO - joeynmt.training - 	Hypothesis: "I was 21 now , as I was second in the deep , and I was getting big through my felt ."
2021-10-03 14:34:30,736 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 122: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ph', 'khongchan@@', 'kamnang@@', 'fang@@', 'kh', '\u0e32w@@', 'bi@@', 'bit@@', 'i', 'chak@@', 'wit@@', 'u@@', 'sit@@', 'ao@@', 'an@@', 'le', '\u0e47', 'k'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u2014' in position 165: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 735, in _log_examples
    logger.debug("\tRaw hypothesis: %s", hypotheses_raw[p])
Message: '\tRaw hypothesis: %s'
Arguments: (['M@@', 'y', 'father', 'worked', 'for', 'read@@', 'ing', 'C@@', 'our@@', '-@@', 'Mar@@', 'k', '\u2014', 'year', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 91: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k',)
2021-10-03 14:34:30,736 - INFO - joeynmt.training - 	Source:     ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k
2021-10-03 14:34:30,737 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u2014' in position 104: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 739, in _log_examples
    logger.info("\tHypothesis: %s", hypotheses[p])
Message: '\tHypothesis: %s'
Arguments: ('My father worked for reading Cour-Mark \u2014 year .',)
2021-10-03 14:34:30,737 - INFO - joeynmt.training - 	Hypothesis: My father worked for reading Cour-Mark \u2014 year .
2021-10-03 14:34:30,737 - INFO - joeynmt.training - Validation result (greedy) at epoch  77, step     5000: bleu:   8.18, loss: 101546.3906, ppl: 857.4816, duration: 9.3503s
2021-10-03 14:34:31,637 - INFO - joeynmt.training - Epoch  77: total training loss 706.14
2021-10-03 14:34:31,637 - INFO - joeynmt.training - EPOCH 78
2021-10-03 14:34:43,849 - INFO - joeynmt.training - Epoch  78: total training loss 686.96
2021-10-03 14:34:43,850 - INFO - joeynmt.training - EPOCH 79
2021-10-03 14:34:56,013 - INFO - joeynmt.training - Epoch  79: total training loss 673.21
2021-10-03 14:34:56,014 - INFO - joeynmt.training - EPOCH 80
2021-10-03 14:35:08,193 - INFO - joeynmt.training - Epoch  80: total training loss 653.44
2021-10-03 14:35:08,193 - INFO - joeynmt.training - EPOCH 81
2021-10-03 14:35:20,436 - INFO - joeynmt.training - Epoch  81: total training loss 636.20
2021-10-03 14:35:20,436 - INFO - joeynmt.training - EPOCH 82
2021-10-03 14:35:32,678 - INFO - joeynmt.training - Epoch  82: total training loss 615.46
2021-10-03 14:35:32,678 - INFO - joeynmt.training - EPOCH 83
2021-10-03 14:35:44,944 - INFO - joeynmt.training - Epoch  83: total training loss 603.90
2021-10-03 14:35:44,945 - INFO - joeynmt.training - EPOCH 84
2021-10-03 14:35:57,146 - INFO - joeynmt.training - Epoch  84: total training loss 581.02
2021-10-03 14:35:57,147 - INFO - joeynmt.training - EPOCH 85
2021-10-03 14:36:09,419 - INFO - joeynmt.training - Epoch  85: total training loss 576.35
2021-10-03 14:36:09,420 - INFO - joeynmt.training - EPOCH 86
2021-10-03 14:36:21,626 - INFO - joeynmt.training - Epoch  86: total training loss 555.12
2021-10-03 14:36:21,627 - INFO - joeynmt.training - EPOCH 87
2021-10-03 14:36:33,836 - INFO - joeynmt.training - Epoch  87: total training loss 544.18
2021-10-03 14:36:33,836 - INFO - joeynmt.training - EPOCH 88
2021-10-03 14:36:46,061 - INFO - joeynmt.training - Epoch  88: total training loss 540.00
2021-10-03 14:36:46,061 - INFO - joeynmt.training - EPOCH 89
2021-10-03 14:36:58,265 - INFO - joeynmt.training - Epoch  89: total training loss 524.62
2021-10-03 14:36:58,265 - INFO - joeynmt.training - EPOCH 90
2021-10-03 14:37:10,483 - INFO - joeynmt.training - Epoch  90: total training loss 510.74
2021-10-03 14:37:10,484 - INFO - joeynmt.training - EPOCH 91
2021-10-03 14:37:22,772 - INFO - joeynmt.training - Epoch  91: total training loss 502.85
2021-10-03 14:37:22,773 - INFO - joeynmt.training - EPOCH 92
2021-10-03 14:37:35,040 - INFO - joeynmt.training - Epoch  92: total training loss 486.75
2021-10-03 14:37:35,040 - INFO - joeynmt.training - EPOCH 93
2021-10-03 14:37:38,842 - INFO - joeynmt.training - Epoch  93, Step:     6000, Batch Loss:     6.848999, Tokens per Sec:     8948, Lr: 0.000300
2021-10-03 14:37:48,150 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 14:37:48,150 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 14:37:48,150 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 14:37:48,157 - INFO - joeynmt.training - Example #0
2021-10-03 14:37:48,157 - INFO - joeynmt.training - 	Source:     th
2021-10-03 14:37:48,157 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 14:37:48,157 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-03 14:37:48,157 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 93: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ton@@', 'chan@@', 'ai\u0e38', '1@@', '1', 'chan@@', 'cham@@', 'dai', 'w', '\u0e32@@', 'tue', 'n@@', 'kue', 'nma@@', 'che', '\u0e32w@@', 'an@@', 'nue', 'ng', 'lae@@', 'dai', 'yin@@', 'siang@@', 'r', 'ong@@', 'd', 'wokham@@', 'inti@@', 'nai@@', 'b', '\u0e32n'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e38' in position 74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n',)
2021-10-03 14:37:48,159 - INFO - joeynmt.training - 	Source:     tonchanai\u0e38 11 chanchamdai w \u0e32tue nkue nmache \u0e32wannue ng laedai yinsiangr ongd wokhamintinaib \u0e32n
2021-10-03 14:37:48,159 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 14:37:48,159 - INFO - joeynmt.training - 	Hypothesis: "I was 17 , I was 16 years ago , as I was getting to work at my own biology and came up with my own neuron ."
2021-10-03 14:37:48,159 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 122: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['ph', 'khongchan@@', 'kamnang@@', 'fang@@', 'kh', '\u0e32w@@', 'bi@@', 'bit@@', 'i', 'chak@@', 'wit@@', 'u@@', 'sit@@', 'ao@@', 'an@@', 'le', '\u0e47', 'k'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e32' in position 91: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k',)
2021-10-03 14:37:48,160 - INFO - joeynmt.training - 	Source:     ph khongchankamnangfangkh \u0e32wbibiti chakwitusitaoanle \u0e47 k
2021-10-03 14:37:48,160 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 14:37:48,160 - INFO - joeynmt.training - 	Hypothesis: My father came to Holy Grail .
2021-10-03 14:37:48,160 - INFO - joeynmt.training - Validation result (greedy) at epoch  93, step     6000: bleu:   8.48, loss: 109640.1797, ppl: 1468.9891, duration: 9.3177s
2021-10-03 14:37:56,614 - INFO - joeynmt.training - Epoch  93: total training loss 446.60
2021-10-03 14:37:56,614 - INFO - joeynmt.training - EPOCH 94
2021-10-03 14:38:08,910 - INFO - joeynmt.training - Epoch  94: total training loss 405.45
2021-10-03 14:38:08,910 - INFO - joeynmt.training - EPOCH 95
2021-10-03 14:38:21,161 - INFO - joeynmt.training - Epoch  95: total training loss 385.01
2021-10-03 14:38:21,162 - INFO - joeynmt.training - EPOCH 96
2021-10-03 14:38:33,424 - INFO - joeynmt.training - Epoch  96: total training loss 370.61
2021-10-03 14:38:33,424 - INFO - joeynmt.training - EPOCH 97
2021-10-03 14:38:45,633 - INFO - joeynmt.training - Epoch  97: total training loss 358.71
2021-10-03 14:38:45,634 - INFO - joeynmt.training - EPOCH 98
2021-10-03 14:38:57,852 - INFO - joeynmt.training - Epoch  98: total training loss 349.40
2021-10-03 14:38:57,852 - INFO - joeynmt.training - EPOCH 99
2021-10-03 14:39:10,152 - INFO - joeynmt.training - Epoch  99: total training loss 337.66
2021-10-03 14:39:10,153 - INFO - joeynmt.training - EPOCH 100
2021-10-03 14:39:22,433 - INFO - joeynmt.training - Epoch 100: total training loss 335.95
2021-10-03 14:39:22,434 - INFO - joeynmt.training - Training ended after 100 epochs.
2021-10-03 14:39:22,434 - INFO - joeynmt.training - Best validation result (greedy) at step     4000:   8.87 eval_metric.
2021-10-03 14:39:22,465 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-03 14:39:22,465 - INFO - joeynmt.prediction - Loading model from models/TRANS_th_r/4000.ckpt
2021-10-03 14:39:23,397 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 14:39:24,219 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 14:39:24,406 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/th_r.en_s/val.bpe.en_s)...
2021-10-03 14:39:45,991 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 14:39:45,991 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 14:39:45,991 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 14:39:45,994 - INFO - joeynmt.prediction -  dev bleu[13a]:   9.09 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 14:39:45,997 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_th_r/00004000.hyps.dev
2021-10-03 14:39:45,998 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/th_r.en_s/test.bpe.en_s)...
2021-10-03 14:40:10,851 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 14:40:10,852 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 14:40:10,852 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 14:40:10,856 - INFO - joeynmt.prediction - test bleu[13a]:   9.39 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 14:40:10,859 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_th_r/00004000.hyps.test
