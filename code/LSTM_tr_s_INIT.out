2021-10-04 12:29:49,217 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-04 12:29:49,284 - INFO - joeynmt.data - Loading training data...
2021-10-04 12:29:49,388 - INFO - joeynmt.data - Building vocabulary...
2021-10-04 12:29:49,837 - INFO - joeynmt.data - Loading dev data...
2021-10-04 12:29:49,851 - INFO - joeynmt.data - Loading test data...
2021-10-04 12:29:49,862 - INFO - joeynmt.data - Data loaded.
2021-10-04 12:29:49,862 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-04 12:29:50,401 - INFO - joeynmt.model - Enc-dec model built.
2021-10-04 12:29:50,407 - INFO - joeynmt.training - Total params: 24738816
2021-10-04 12:29:55,260 - INFO - joeynmt.helpers - cfg.name                           : LSTM_tr_s_INIT
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.src                       : tr_s
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.trg                       : en_s
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/tr_s.en_s/train.bpe
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/tr_s.en_s/val.bpe
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/tr_s.en_s/test.bpe
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-04 12:29:55,261 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 100000
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 100000
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/tr_s.en_s/tr_s.en_s.vocab.txt
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/tr_s.en_s/tr_s.en_s.vocab.txt
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 5e-07
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.clip_grad_norm        : 1.0
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.patience              : 10
2021-10-04 12:29:55,262 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.5
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.epochs                : 2
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 100
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/LSTM_tr_s_INIT
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.model.encoder.rnn_type         : lstm
2021-10-04 12:29:55,263 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : False
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.encoder.bidirectional    : False
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.2
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 1
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.rnn_type         : lstm
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : False
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.emb_scale        : False
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 1024
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.2
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_dropout   : 0.2
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 1
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.input_feeding    : True
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.init_hidden      : bridge
2021-10-04 12:29:55,264 - INFO - joeynmt.helpers - cfg.model.decoder.attention        : bahdanau
2021-10-04 12:29:55,265 - INFO - joeynmt.helpers - Data set sizes: 
	train 5177,
	valid 551,
	test 602
2021-10-04 12:29:55,265 - INFO - joeynmt.helpers - First training example:
	[SRC] tr
	[TRG] en
2021-10-04 12:29:55,265 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) a (9) &quot;
2021-10-04 12:29:55,265 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) a (9) &quot;
2021-10-04 12:29:55,265 - INFO - joeynmt.helpers - Number of Src words (types): 4136
2021-10-04 12:29:55,265 - INFO - joeynmt.helpers - Number of Trg words (types): 4136
2021-10-04 12:29:55,266 - INFO - joeynmt.training - Model(
	encoder=RecurrentEncoder(LSTM(512, 512, batch_first=True)),
	decoder=RecurrentDecoder(rnn=LSTM(1536, 1024, batch_first=True), attention=BahdanauAttention),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4136),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4136))
2021-10-04 12:29:55,275 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-04 12:29:55,275 - INFO - joeynmt.training - EPOCH 1
2021-10-04 12:30:08,904 - INFO - joeynmt.training - Epoch   1: total training loss 9091.29
2021-10-04 12:30:08,905 - INFO - joeynmt.training - EPOCH 2
2021-10-04 12:30:19,105 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-04 12:30:19,105 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-04 12:30:19,105 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-04 12:30:19,111 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-04 12:30:20,015 - INFO - joeynmt.training - Example #0
2021-10-04 12:30:20,016 - INFO - joeynmt.training - 	Source:     tr
2021-10-04 12:30:20,016 - INFO - joeynmt.training - 	Reference:  en
2021-10-04 12:30:20,016 - INFO - joeynmt.training - 	Hypothesis: I &apos;s .
2021-10-04 12:30:20,016 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u015f' in position 86: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['1@@', '1', 'ya\u015f@@', '\u0131nday@@', 'ken', 'bir', 'sa@@', 'ba@@', 'h', 'ev@@', 'im@@', 'deki', 'sev@@', 'in@@', 'ç', 'ç@@', '\u0131\u011f@@', 'l\u0131@@', 'klar@@', '\u0131yla', 'u@@', 'yan@@', 'd\u0131\u011f@@', '\u0131m\u0131', 'hat\u0131r@@', 'l\u0131@@', 'yorum', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 70-71: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .',)
2021-10-04 12:30:20,020 - INFO - joeynmt.training - 	Source:     11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .
2021-10-04 12:30:20,021 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-04 12:30:20,021 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s , , , , , , , , , , , , , , , , , , , , , , , , , , , , ."
2021-10-04 12:30:20,021 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 151: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"B@@', 'ab@@', 'am', 'B@@', 'B@@', 'C', 'H@@', 'ab@@', 'er', 'kan@@', 'al@@', '\u0131n\u0131', 'din@@', 'li@@', 'yordu', ';', 'o', 'uf@@', 'ak', ',', 'gr@@', 'i', 'rad@@', 'y@@', 'os@@', 'undan', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 87: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."',)
2021-10-04 12:30:20,022 - INFO - joeynmt.training - 	Source:     "Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."
2021-10-04 12:30:20,023 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-04 12:30:20,023 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s , , , , , , , , , , , , , , , , , , , , , , ."
2021-10-04 12:30:20,023 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      100: bleu:   2.43, loss: 93242.4531, ppl: 464.4415, duration: 3.6831s
2021-10-04 12:30:27,804 - INFO - joeynmt.training - Epoch   2: total training loss 8483.01
2021-10-04 12:30:27,805 - INFO - joeynmt.training - Training ended after   2 epochs.
2021-10-04 12:30:27,805 - INFO - joeynmt.training - Best validation result (greedy) at step      100:   2.43 eval_metric.
2021-10-04 12:30:27,831 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-04 12:30:27,831 - INFO - joeynmt.prediction - Loading model from models/LSTM_tr_s_INIT/100.ckpt
2021-10-04 12:30:28,146 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-04 12:30:28,669 - INFO - joeynmt.model - Enc-dec model built.
2021-10-04 12:30:28,723 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/tr_s.en_s/val.bpe.en_s)...
2021-10-04 12:30:33,743 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-04 12:30:33,743 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-04 12:30:33,743 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-04 12:30:33,746 - INFO - joeynmt.prediction -  dev bleu[13a]:   2.34 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-04 12:30:33,749 - INFO - joeynmt.prediction - Translations saved to: models/LSTM_tr_s_INIT/00000100.hyps.dev
2021-10-04 12:30:33,749 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/tr_s.en_s/test.bpe.en_s)...
2021-10-04 12:30:39,823 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-04 12:30:39,823 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-04 12:30:39,823 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-04 12:30:39,827 - INFO - joeynmt.prediction - test bleu[13a]:   3.03 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-04 12:30:39,830 - INFO - joeynmt.prediction - Translations saved to: models/LSTM_tr_s_INIT/00000100.hyps.test
