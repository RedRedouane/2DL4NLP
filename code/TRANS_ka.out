2021-10-03 13:11:09,160 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-03 13:11:09,238 - INFO - joeynmt.data - Loading training data...
2021-10-03 13:11:09,355 - INFO - joeynmt.data - Building vocabulary...
2021-10-03 13:11:09,815 - INFO - joeynmt.data - Loading dev data...
2021-10-03 13:11:09,829 - INFO - joeynmt.data - Loading test data...
2021-10-03 13:11:09,841 - INFO - joeynmt.data - Data loaded.
2021-10-03 13:11:09,841 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:11:10,685 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:11:10,695 - INFO - joeynmt.training - Total params: 33678848
2021-10-03 13:11:14,992 - INFO - joeynmt.helpers - cfg.name                           : TRANS_ka
2021-10-03 13:11:14,993 - INFO - joeynmt.helpers - cfg.data.src                       : ka
2021-10-03 13:11:14,993 - INFO - joeynmt.helpers - cfg.data.trg                       : en_s
2021-10-03 13:11:14,993 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/ka.en_s/train.bpe
2021-10-03 13:11:14,993 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/ka.en_s/val.bpe
2021-10-03 13:11:14,993 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/ka.en_s/test.bpe
2021-10-03 13:11:14,993 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 10000
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 10000
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/ka.en_s/ka.en_s.vocab.txt
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/ka.en_s/ka.en_s.vocab.txt
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-03 13:11:14,994 - INFO - joeynmt.helpers - cfg.training.patience              : 1
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.epochs                : 100
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/TRANS_ka
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 60
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-10-03 13:11:14,995 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4
2021-10-03 13:11:14,996 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - Data set sizes: 
	train 5114,
	valid 551,
	test 602
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - First training example:
	[SRC] ka
	[TRG] en
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) &quot; (9) of
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) &quot; (9) of
2021-10-03 13:11:14,997 - INFO - joeynmt.helpers - Number of Src words (types): 4167
2021-10-03 13:11:14,998 - INFO - joeynmt.helpers - Number of Trg words (types): 4167
2021-10-03 13:11:14,998 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4167),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4167))
2021-10-03 13:11:15,009 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-03 13:11:15,009 - INFO - joeynmt.training - EPOCH 1
2021-10-03 13:11:26,997 - INFO - joeynmt.training - Epoch   1: total training loss 9020.95
2021-10-03 13:11:26,997 - INFO - joeynmt.training - EPOCH 2
2021-10-03 13:11:38,912 - INFO - joeynmt.training - Epoch   2: total training loss 8283.80
2021-10-03 13:11:38,912 - INFO - joeynmt.training - EPOCH 3
2021-10-03 13:11:50,931 - INFO - joeynmt.training - Epoch   3: total training loss 7989.74
2021-10-03 13:11:50,932 - INFO - joeynmt.training - EPOCH 4
2021-10-03 13:12:03,068 - INFO - joeynmt.training - Epoch   4: total training loss 7634.71
2021-10-03 13:12:03,069 - INFO - joeynmt.training - EPOCH 5
2021-10-03 13:12:15,275 - INFO - joeynmt.training - Epoch   5: total training loss 7330.76
2021-10-03 13:12:15,275 - INFO - joeynmt.training - EPOCH 6
2021-10-03 13:12:27,467 - INFO - joeynmt.training - Epoch   6: total training loss 7110.42
2021-10-03 13:12:27,467 - INFO - joeynmt.training - EPOCH 7
2021-10-03 13:12:39,613 - INFO - joeynmt.training - Epoch   7: total training loss 6938.25
2021-10-03 13:12:39,614 - INFO - joeynmt.training - EPOCH 8
2021-10-03 13:12:51,912 - INFO - joeynmt.training - Epoch   8: total training loss 6760.08
2021-10-03 13:12:51,913 - INFO - joeynmt.training - EPOCH 9
2021-10-03 13:13:04,337 - INFO - joeynmt.training - Epoch   9: total training loss 6597.87
2021-10-03 13:13:04,337 - INFO - joeynmt.training - EPOCH 10
2021-10-03 13:13:16,479 - INFO - joeynmt.training - Epoch  10: total training loss 6447.25
2021-10-03 13:13:16,480 - INFO - joeynmt.training - EPOCH 11
2021-10-03 13:13:29,236 - INFO - joeynmt.training - Epoch  11: total training loss 6272.95
2021-10-03 13:13:29,236 - INFO - joeynmt.training - EPOCH 12
2021-10-03 13:13:42,510 - INFO - joeynmt.training - Epoch  12: total training loss 6109.98
2021-10-03 13:13:42,510 - INFO - joeynmt.training - EPOCH 13
2021-10-03 13:13:55,752 - INFO - joeynmt.training - Epoch  13: total training loss 5955.11
2021-10-03 13:13:55,752 - INFO - joeynmt.training - EPOCH 14
2021-10-03 13:14:09,040 - INFO - joeynmt.training - Epoch  14: total training loss 5786.95
2021-10-03 13:14:09,041 - INFO - joeynmt.training - EPOCH 15
2021-10-03 13:14:22,326 - INFO - joeynmt.training - Epoch  15: total training loss 5622.76
2021-10-03 13:14:22,326 - INFO - joeynmt.training - EPOCH 16
2021-10-03 13:14:30,022 - INFO - joeynmt.training - Epoch  16, Step:     1000, Batch Loss:    91.542908, Tokens per Sec:     9033, Lr: 0.000300
2021-10-03 13:14:41,423 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:14:41,424 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:14:41,424 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:14:41,428 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:14:42,735 - INFO - joeynmt.training - Example #0
2021-10-03 13:14:42,735 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:14:42,735 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:14:42,736 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:14:42,736 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 73-79: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea', '1@@', '1', '\u10ec\u10da\u10d8\u10e1', '\u10d5\u10d8\u10e7\u10d0\u10d5\u10d8', ',', '\u10db\u10d4', '\u10db\u10d0@@', '\u10ee\u10e1\u10dd@@', '\u10d5\u10e1', '\u10e1\u10d8@@', '\u10ee\u10d0\u10e0@@', '\u10e3\u10da\u10d8\u10e1', '\u10dc\u10dd@@', '\u10e2\u10d0@@', '\u10d6\u10d4', '\u10d2\u10d0@@', '\u10e6@@', '\u10d5\u10d8@@', '\u10eb@@', '\u10d4\u10d1\u10d0', '\u10d4\u10e0\u10d7', '\u10d3\u10d8@@', '\u10da\u10d0\u10e1', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 66-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."',)
2021-10-03 13:14:42,740 - INFO - joeynmt.training - 	Source:     "\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."
2021-10-03 13:14:42,741 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:14:42,741 - INFO - joeynmt.training - 	Hypothesis: "I was a few years , I was a lot of the first of the same ."
2021-10-03 13:14:42,741 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-75: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u10db\u10d0\u10db\u10d0@@', '\u10e9\u10d4\u10db\u10d8', '\u10e3@@', '\u10e1\u10db\u10d4@@', '\u10dc\u10d3\u10d0', '&quot;', '&quot;', '\u10d1\u10d8@@', '\u10d1@@', '\u10d8\u10e1@@', '\u10d8\u10e1', '&quot;', '&quot;', '\u10e1\u10d8@@', '\u10d0\u10ee@@', '\u10da\u10d4@@', '\u10d4\u10d1\u10e1', '\u10d7\u10d0\u10d5\u10d8\u10e1', '\u10de\u10d0\u10e2\u10d0\u10e0\u10d0', '\u10dc\u10d0@@', '\u10ea@@', '\u10e0\u10d8\u10e1@@', '\u10e4\u10d4@@', '\u10e0', '\u10e0\u10d0@@', '\u10d3\u10d8@@', '\u10dd\u10e8\u10d8', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .',)
2021-10-03 13:14:42,742 - INFO - joeynmt.training - 	Source:     \u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .
2021-10-03 13:14:42,742 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:14:42,742 - INFO - joeynmt.training - 	Hypothesis: I was my first . &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; The first bombsight .
2021-10-03 13:14:42,743 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     1000: bleu:   6.46, loss: 72711.0000, ppl: 108.7351, duration: 12.7206s
2021-10-03 13:14:47,264 - INFO - joeynmt.training - Epoch  16: total training loss 5462.43
2021-10-03 13:14:47,264 - INFO - joeynmt.training - EPOCH 17
2021-10-03 13:14:59,479 - INFO - joeynmt.training - Epoch  17: total training loss 5295.78
2021-10-03 13:14:59,479 - INFO - joeynmt.training - EPOCH 18
2021-10-03 13:15:12,721 - INFO - joeynmt.training - Epoch  18: total training loss 5138.91
2021-10-03 13:15:12,721 - INFO - joeynmt.training - EPOCH 19
2021-10-03 13:15:25,950 - INFO - joeynmt.training - Epoch  19: total training loss 4992.75
2021-10-03 13:15:25,950 - INFO - joeynmt.training - EPOCH 20
2021-10-03 13:15:38,183 - INFO - joeynmt.training - Epoch  20: total training loss 4842.98
2021-10-03 13:15:38,184 - INFO - joeynmt.training - EPOCH 21
2021-10-03 13:15:50,378 - INFO - joeynmt.training - Epoch  21: total training loss 4705.06
2021-10-03 13:15:50,378 - INFO - joeynmt.training - EPOCH 22
2021-10-03 13:16:02,584 - INFO - joeynmt.training - Epoch  22: total training loss 4578.31
2021-10-03 13:16:02,584 - INFO - joeynmt.training - EPOCH 23
2021-10-03 13:16:14,744 - INFO - joeynmt.training - Epoch  23: total training loss 4446.49
2021-10-03 13:16:14,744 - INFO - joeynmt.training - EPOCH 24
2021-10-03 13:16:26,870 - INFO - joeynmt.training - Epoch  24: total training loss 4320.78
2021-10-03 13:16:26,871 - INFO - joeynmt.training - EPOCH 25
2021-10-03 13:16:39,041 - INFO - joeynmt.training - Epoch  25: total training loss 4203.40
2021-10-03 13:16:39,042 - INFO - joeynmt.training - EPOCH 26
2021-10-03 13:16:51,203 - INFO - joeynmt.training - Epoch  26: total training loss 4078.95
2021-10-03 13:16:51,203 - INFO - joeynmt.training - EPOCH 27
2021-10-03 13:17:03,351 - INFO - joeynmt.training - Epoch  27: total training loss 3967.82
2021-10-03 13:17:03,351 - INFO - joeynmt.training - EPOCH 28
2021-10-03 13:17:15,519 - INFO - joeynmt.training - Epoch  28: total training loss 3859.71
2021-10-03 13:17:15,520 - INFO - joeynmt.training - EPOCH 29
2021-10-03 13:17:27,720 - INFO - joeynmt.training - Epoch  29: total training loss 3751.28
2021-10-03 13:17:27,720 - INFO - joeynmt.training - EPOCH 30
2021-10-03 13:17:39,913 - INFO - joeynmt.training - Epoch  30: total training loss 3646.35
2021-10-03 13:17:39,914 - INFO - joeynmt.training - EPOCH 31
2021-10-03 13:17:52,092 - INFO - joeynmt.training - Epoch  31: total training loss 3545.83
2021-10-03 13:17:52,092 - INFO - joeynmt.training - EPOCH 32
2021-10-03 13:17:55,137 - INFO - joeynmt.training - Epoch  32, Step:     2000, Batch Loss:    53.050854, Tokens per Sec:     8982, Lr: 0.000300
2021-10-03 13:18:05,527 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:18:05,527 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:18:05,527 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:18:05,532 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:18:06,785 - INFO - joeynmt.helpers - delete models/TRANS_ka/1000.ckpt
2021-10-03 13:18:06,890 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/TRANS_ka/1000.ckpt
2021-10-03 13:18:06,890 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/TRANS_ka/1000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/TRANS_ka/1000.ckpt')
2021-10-03 13:18:06,892 - INFO - joeynmt.training - Example #0
2021-10-03 13:18:06,892 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:18:06,892 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:18:06,892 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:18:06,893 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 73-79: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea', '1@@', '1', '\u10ec\u10da\u10d8\u10e1', '\u10d5\u10d8\u10e7\u10d0\u10d5\u10d8', ',', '\u10db\u10d4', '\u10db\u10d0@@', '\u10ee\u10e1\u10dd@@', '\u10d5\u10e1', '\u10e1\u10d8@@', '\u10ee\u10d0\u10e0@@', '\u10e3\u10da\u10d8\u10e1', '\u10dc\u10dd@@', '\u10e2\u10d0@@', '\u10d6\u10d4', '\u10d2\u10d0@@', '\u10e6@@', '\u10d5\u10d8@@', '\u10eb@@', '\u10d4\u10d1\u10d0', '\u10d4\u10e0\u10d7', '\u10d3\u10d8@@', '\u10da\u10d0\u10e1', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 66-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."',)
2021-10-03 13:18:06,894 - INFO - joeynmt.training - 	Source:     "\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."
2021-10-03 13:18:06,895 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:18:06,895 - INFO - joeynmt.training - 	Hypothesis: "When I was born to 20s , I was a few feeding about the other ."
2021-10-03 13:18:06,895 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-75: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u10db\u10d0\u10db\u10d0@@', '\u10e9\u10d4\u10db\u10d8', '\u10e3@@', '\u10e1\u10db\u10d4@@', '\u10dc\u10d3\u10d0', '&quot;', '&quot;', '\u10d1\u10d8@@', '\u10d1@@', '\u10d8\u10e1@@', '\u10d8\u10e1', '&quot;', '&quot;', '\u10e1\u10d8@@', '\u10d0\u10ee@@', '\u10da\u10d4@@', '\u10d4\u10d1\u10e1', '\u10d7\u10d0\u10d5\u10d8\u10e1', '\u10de\u10d0\u10e2\u10d0\u10e0\u10d0', '\u10dc\u10d0@@', '\u10ea@@', '\u10e0\u10d8\u10e1@@', '\u10e4\u10d4@@', '\u10e0', '\u10e0\u10d0@@', '\u10d3\u10d8@@', '\u10dd\u10e8\u10d8', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .',)
2021-10-03 13:18:06,895 - INFO - joeynmt.training - 	Source:     \u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .
2021-10-03 13:18:06,896 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:18:06,896 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to my grandmothers . &quot; &quot; The first Brived to the first Brip Brichest Bricha .
2021-10-03 13:18:06,896 - INFO - joeynmt.training - Validation result (greedy) at epoch  32, step     2000: bleu:   8.07, loss: 72840.0703, ppl: 109.6440, duration: 11.7584s
2021-10-03 13:18:16,280 - INFO - joeynmt.training - Epoch  32: total training loss 3439.27
2021-10-03 13:18:16,280 - INFO - joeynmt.training - EPOCH 33
2021-10-03 13:18:29,269 - INFO - joeynmt.training - Epoch  33: total training loss 3327.35
2021-10-03 13:18:29,270 - INFO - joeynmt.training - EPOCH 34
2021-10-03 13:18:41,378 - INFO - joeynmt.training - Epoch  34: total training loss 3223.31
2021-10-03 13:18:41,378 - INFO - joeynmt.training - EPOCH 35
2021-10-03 13:18:53,513 - INFO - joeynmt.training - Epoch  35: total training loss 3130.14
2021-10-03 13:18:53,513 - INFO - joeynmt.training - EPOCH 36
2021-10-03 13:19:05,579 - INFO - joeynmt.training - Epoch  36: total training loss 3033.35
2021-10-03 13:19:05,579 - INFO - joeynmt.training - EPOCH 37
2021-10-03 13:19:17,650 - INFO - joeynmt.training - Epoch  37: total training loss 2935.61
2021-10-03 13:19:17,650 - INFO - joeynmt.training - EPOCH 38
2021-10-03 13:19:29,765 - INFO - joeynmt.training - Epoch  38: total training loss 2835.10
2021-10-03 13:19:29,766 - INFO - joeynmt.training - EPOCH 39
2021-10-03 13:19:41,941 - INFO - joeynmt.training - Epoch  39: total training loss 2751.26
2021-10-03 13:19:41,942 - INFO - joeynmt.training - EPOCH 40
2021-10-03 13:19:54,124 - INFO - joeynmt.training - Epoch  40: total training loss 2658.78
2021-10-03 13:19:54,124 - INFO - joeynmt.training - EPOCH 41
2021-10-03 13:20:06,327 - INFO - joeynmt.training - Epoch  41: total training loss 2572.02
2021-10-03 13:20:06,327 - INFO - joeynmt.training - EPOCH 42
2021-10-03 13:20:19,575 - INFO - joeynmt.training - Epoch  42: total training loss 2483.13
2021-10-03 13:20:19,575 - INFO - joeynmt.training - EPOCH 43
2021-10-03 13:20:32,639 - INFO - joeynmt.training - Epoch  43: total training loss 2408.66
2021-10-03 13:20:32,639 - INFO - joeynmt.training - EPOCH 44
2021-10-03 13:20:44,639 - INFO - joeynmt.training - Epoch  44: total training loss 2322.94
2021-10-03 13:20:44,639 - INFO - joeynmt.training - EPOCH 45
2021-10-03 13:20:57,346 - INFO - joeynmt.training - Epoch  45: total training loss 2246.38
2021-10-03 13:20:57,347 - INFO - joeynmt.training - EPOCH 46
2021-10-03 13:21:09,710 - INFO - joeynmt.training - Epoch  46: total training loss 2158.12
2021-10-03 13:21:09,710 - INFO - joeynmt.training - EPOCH 47
2021-10-03 13:21:20,385 - INFO - joeynmt.training - Epoch  47, Step:     3000, Batch Loss:    32.483059, Tokens per Sec:     9119, Lr: 0.000300
2021-10-03 13:21:30,959 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:21:30,959 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:21:30,959 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:21:30,964 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:21:32,244 - INFO - joeynmt.helpers - delete models/TRANS_ka/2000.ckpt
2021-10-03 13:21:32,347 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/TRANS_ka/2000.ckpt
2021-10-03 13:21:32,348 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/TRANS_ka/2000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/TRANS_ka/2000.ckpt')
2021-10-03 13:21:32,350 - INFO - joeynmt.training - Example #0
2021-10-03 13:21:32,350 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:21:32,351 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:21:32,351 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:21:32,351 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 73-79: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea', '1@@', '1', '\u10ec\u10da\u10d8\u10e1', '\u10d5\u10d8\u10e7\u10d0\u10d5\u10d8', ',', '\u10db\u10d4', '\u10db\u10d0@@', '\u10ee\u10e1\u10dd@@', '\u10d5\u10e1', '\u10e1\u10d8@@', '\u10ee\u10d0\u10e0@@', '\u10e3\u10da\u10d8\u10e1', '\u10dc\u10dd@@', '\u10e2\u10d0@@', '\u10d6\u10d4', '\u10d2\u10d0@@', '\u10e6@@', '\u10d5\u10d8@@', '\u10eb@@', '\u10d4\u10d1\u10d0', '\u10d4\u10e0\u10d7', '\u10d3\u10d8@@', '\u10da\u10d0\u10e1', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 66-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."',)
2021-10-03 13:21:32,353 - INFO - joeynmt.training - 	Source:     "\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."
2021-10-03 13:21:32,353 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:21:32,353 - INFO - joeynmt.training - 	Hypothesis: "When I was 18 , I was a day on the trade of these guys ."
2021-10-03 13:21:32,353 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-75: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u10db\u10d0\u10db\u10d0@@', '\u10e9\u10d4\u10db\u10d8', '\u10e3@@', '\u10e1\u10db\u10d4@@', '\u10dc\u10d3\u10d0', '&quot;', '&quot;', '\u10d1\u10d8@@', '\u10d1@@', '\u10d8\u10e1@@', '\u10d8\u10e1', '&quot;', '&quot;', '\u10e1\u10d8@@', '\u10d0\u10ee@@', '\u10da\u10d4@@', '\u10d4\u10d1\u10e1', '\u10d7\u10d0\u10d5\u10d8\u10e1', '\u10de\u10d0\u10e2\u10d0\u10e0\u10d0', '\u10dc\u10d0@@', '\u10ea@@', '\u10e0\u10d8\u10e1@@', '\u10e4\u10d4@@', '\u10e0', '\u10e0\u10d0@@', '\u10d3\u10d8@@', '\u10dd\u10e8\u10d8', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .',)
2021-10-03 13:21:32,354 - INFO - joeynmt.training - 	Source:     \u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .
2021-10-03 13:21:32,354 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:21:32,354 - INFO - joeynmt.training - 	Hypothesis: My Moup &apos;s my chool in my wish Egone . &quot; &quot; So my first poot Cloott project .
2021-10-03 13:21:32,354 - INFO - joeynmt.training - Validation result (greedy) at epoch  47, step     3000: bleu:   8.58, loss: 82313.5859, ppl: 201.9776, duration: 11.9688s
2021-10-03 13:21:33,877 - INFO - joeynmt.training - Epoch  47: total training loss 2089.19
2021-10-03 13:21:33,877 - INFO - joeynmt.training - EPOCH 48
2021-10-03 13:21:45,965 - INFO - joeynmt.training - Epoch  48: total training loss 2017.14
2021-10-03 13:21:45,965 - INFO - joeynmt.training - EPOCH 49
2021-10-03 13:21:58,035 - INFO - joeynmt.training - Epoch  49: total training loss 1948.36
2021-10-03 13:21:58,035 - INFO - joeynmt.training - EPOCH 50
2021-10-03 13:22:11,035 - INFO - joeynmt.training - Epoch  50: total training loss 1867.31
2021-10-03 13:22:11,035 - INFO - joeynmt.training - EPOCH 51
2021-10-03 13:22:24,232 - INFO - joeynmt.training - Epoch  51: total training loss 1809.36
2021-10-03 13:22:24,233 - INFO - joeynmt.training - EPOCH 52
2021-10-03 13:22:36,443 - INFO - joeynmt.training - Epoch  52: total training loss 1745.09
2021-10-03 13:22:36,443 - INFO - joeynmt.training - EPOCH 53
2021-10-03 13:22:48,764 - INFO - joeynmt.training - Epoch  53: total training loss 1679.56
2021-10-03 13:22:48,764 - INFO - joeynmt.training - EPOCH 54
2021-10-03 13:23:01,827 - INFO - joeynmt.training - Epoch  54: total training loss 1620.69
2021-10-03 13:23:01,827 - INFO - joeynmt.training - EPOCH 55
2021-10-03 13:23:14,015 - INFO - joeynmt.training - Epoch  55: total training loss 1568.49
2021-10-03 13:23:14,015 - INFO - joeynmt.training - EPOCH 56
2021-10-03 13:23:26,170 - INFO - joeynmt.training - Epoch  56: total training loss 1513.97
2021-10-03 13:23:26,170 - INFO - joeynmt.training - EPOCH 57
2021-10-03 13:23:38,338 - INFO - joeynmt.training - Epoch  57: total training loss 1458.48
2021-10-03 13:23:38,339 - INFO - joeynmt.training - EPOCH 58
2021-10-03 13:23:50,551 - INFO - joeynmt.training - Epoch  58: total training loss 1396.00
2021-10-03 13:23:50,552 - INFO - joeynmt.training - EPOCH 59
2021-10-03 13:24:03,192 - INFO - joeynmt.training - Epoch  59: total training loss 1351.16
2021-10-03 13:24:03,192 - INFO - joeynmt.training - EPOCH 60
2021-10-03 13:24:16,502 - INFO - joeynmt.training - Epoch  60: total training loss 1303.12
2021-10-03 13:24:16,502 - INFO - joeynmt.training - EPOCH 61
2021-10-03 13:24:29,131 - INFO - joeynmt.training - Epoch  61: total training loss 1257.60
2021-10-03 13:24:29,131 - INFO - joeynmt.training - EPOCH 62
2021-10-03 13:24:41,248 - INFO - joeynmt.training - Epoch  62: total training loss 1223.42
2021-10-03 13:24:41,249 - INFO - joeynmt.training - EPOCH 63
2021-10-03 13:24:47,324 - INFO - joeynmt.training - Epoch  63, Step:     4000, Batch Loss:    18.150051, Tokens per Sec:     9103, Lr: 0.000300
2021-10-03 13:24:58,088 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:24:58,088 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:24:58,088 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:24:58,095 - INFO - joeynmt.training - Example #0
2021-10-03 13:24:58,095 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:24:58,095 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:24:58,095 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:24:58,095 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 73-79: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea', '1@@', '1', '\u10ec\u10da\u10d8\u10e1', '\u10d5\u10d8\u10e7\u10d0\u10d5\u10d8', ',', '\u10db\u10d4', '\u10db\u10d0@@', '\u10ee\u10e1\u10dd@@', '\u10d5\u10e1', '\u10e1\u10d8@@', '\u10ee\u10d0\u10e0@@', '\u10e3\u10da\u10d8\u10e1', '\u10dc\u10dd@@', '\u10e2\u10d0@@', '\u10d6\u10d4', '\u10d2\u10d0@@', '\u10e6@@', '\u10d5\u10d8@@', '\u10eb@@', '\u10d4\u10d1\u10d0', '\u10d4\u10e0\u10d7', '\u10d3\u10d8@@', '\u10da\u10d0\u10e1', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 66-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."',)
2021-10-03 13:24:58,097 - INFO - joeynmt.training - 	Source:     "\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."
2021-10-03 13:24:58,097 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:24:58,097 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I was on the elf of a day at the table ."
2021-10-03 13:24:58,097 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-75: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u10db\u10d0\u10db\u10d0@@', '\u10e9\u10d4\u10db\u10d8', '\u10e3@@', '\u10e1\u10db\u10d4@@', '\u10dc\u10d3\u10d0', '&quot;', '&quot;', '\u10d1\u10d8@@', '\u10d1@@', '\u10d8\u10e1@@', '\u10d8\u10e1', '&quot;', '&quot;', '\u10e1\u10d8@@', '\u10d0\u10ee@@', '\u10da\u10d4@@', '\u10d4\u10d1\u10e1', '\u10d7\u10d0\u10d5\u10d8\u10e1', '\u10de\u10d0\u10e2\u10d0\u10e0\u10d0', '\u10dc\u10d0@@', '\u10ea@@', '\u10e0\u10d8\u10e1@@', '\u10e4\u10d4@@', '\u10e0', '\u10e0\u10d0@@', '\u10d3\u10d8@@', '\u10dd\u10e8\u10d8', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .',)
2021-10-03 13:24:58,098 - INFO - joeynmt.training - 	Source:     \u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .
2021-10-03 13:24:58,098 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:24:58,098 - INFO - joeynmt.training - 	Hypothesis: My Many Zen was in my first Bell over the first Bluma substment Sierpen &apos;s Egotd .
2021-10-03 13:24:58,098 - INFO - joeynmt.training - Validation result (greedy) at epoch  63, step     4000: bleu:   8.15, loss: 93660.4688, ppl: 419.8441, duration: 10.7739s
2021-10-03 13:25:04,163 - INFO - joeynmt.training - Epoch  63: total training loss 1171.54
2021-10-03 13:25:04,164 - INFO - joeynmt.training - EPOCH 64
2021-10-03 13:25:16,311 - INFO - joeynmt.training - Epoch  64: total training loss 1128.01
2021-10-03 13:25:16,312 - INFO - joeynmt.training - EPOCH 65
2021-10-03 13:25:28,742 - INFO - joeynmt.training - Epoch  65: total training loss 1098.70
2021-10-03 13:25:28,743 - INFO - joeynmt.training - EPOCH 66
2021-10-03 13:25:41,959 - INFO - joeynmt.training - Epoch  66: total training loss 1055.06
2021-10-03 13:25:41,959 - INFO - joeynmt.training - EPOCH 67
2021-10-03 13:25:55,240 - INFO - joeynmt.training - Epoch  67: total training loss 1027.40
2021-10-03 13:25:55,241 - INFO - joeynmt.training - EPOCH 68
2021-10-03 13:26:08,501 - INFO - joeynmt.training - Epoch  68: total training loss 987.61
2021-10-03 13:26:08,501 - INFO - joeynmt.training - EPOCH 69
2021-10-03 13:26:21,290 - INFO - joeynmt.training - Epoch  69: total training loss 955.60
2021-10-03 13:26:21,290 - INFO - joeynmt.training - EPOCH 70
2021-10-03 13:26:34,551 - INFO - joeynmt.training - Epoch  70: total training loss 927.95
2021-10-03 13:26:34,552 - INFO - joeynmt.training - EPOCH 71
2021-10-03 13:26:46,930 - INFO - joeynmt.training - Epoch  71: total training loss 895.50
2021-10-03 13:26:46,930 - INFO - joeynmt.training - EPOCH 72
2021-10-03 13:26:59,114 - INFO - joeynmt.training - Epoch  72: total training loss 875.50
2021-10-03 13:26:59,114 - INFO - joeynmt.training - EPOCH 73
2021-10-03 13:27:12,190 - INFO - joeynmt.training - Epoch  73: total training loss 854.55
2021-10-03 13:27:12,190 - INFO - joeynmt.training - EPOCH 74
2021-10-03 13:27:25,426 - INFO - joeynmt.training - Epoch  74: total training loss 819.49
2021-10-03 13:27:25,426 - INFO - joeynmt.training - EPOCH 75
2021-10-03 13:27:38,635 - INFO - joeynmt.training - Epoch  75: total training loss 803.87
2021-10-03 13:27:38,635 - INFO - joeynmt.training - EPOCH 76
2021-10-03 13:27:51,408 - INFO - joeynmt.training - Epoch  76: total training loss 771.93
2021-10-03 13:27:51,408 - INFO - joeynmt.training - EPOCH 77
2021-10-03 13:28:04,005 - INFO - joeynmt.training - Epoch  77: total training loss 755.54
2021-10-03 13:28:04,005 - INFO - joeynmt.training - EPOCH 78
2021-10-03 13:28:17,321 - INFO - joeynmt.training - Epoch  78: total training loss 733.96
2021-10-03 13:28:17,322 - INFO - joeynmt.training - EPOCH 79
2021-10-03 13:28:18,987 - INFO - joeynmt.training - Epoch  79, Step:     5000, Batch Loss:    11.230964, Tokens per Sec:     8291, Lr: 0.000300
2021-10-03 13:28:28,236 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:28:28,236 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:28:28,236 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:28:28,242 - INFO - joeynmt.training - Example #0
2021-10-03 13:28:28,243 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:28:28,243 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:28:28,243 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:28:28,243 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 73-79: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea', '1@@', '1', '\u10ec\u10da\u10d8\u10e1', '\u10d5\u10d8\u10e7\u10d0\u10d5\u10d8', ',', '\u10db\u10d4', '\u10db\u10d0@@', '\u10ee\u10e1\u10dd@@', '\u10d5\u10e1', '\u10e1\u10d8@@', '\u10ee\u10d0\u10e0@@', '\u10e3\u10da\u10d8\u10e1', '\u10dc\u10dd@@', '\u10e2\u10d0@@', '\u10d6\u10d4', '\u10d2\u10d0@@', '\u10e6@@', '\u10d5\u10d8@@', '\u10eb@@', '\u10d4\u10d1\u10d0', '\u10d4\u10e0\u10d7', '\u10d3\u10d8@@', '\u10da\u10d0\u10e1', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 66-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."',)
2021-10-03 13:28:28,244 - INFO - joeynmt.training - 	Source:     "\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."
2021-10-03 13:28:28,245 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:28:28,245 - INFO - joeynmt.training - 	Hypothesis: "When I was 18 , I found the afternoon of these guys were a single tric ."
2021-10-03 13:28:28,245 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-75: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u10db\u10d0\u10db\u10d0@@', '\u10e9\u10d4\u10db\u10d8', '\u10e3@@', '\u10e1\u10db\u10d4@@', '\u10dc\u10d3\u10d0', '&quot;', '&quot;', '\u10d1\u10d8@@', '\u10d1@@', '\u10d8\u10e1@@', '\u10d8\u10e1', '&quot;', '&quot;', '\u10e1\u10d8@@', '\u10d0\u10ee@@', '\u10da\u10d4@@', '\u10d4\u10d1\u10e1', '\u10d7\u10d0\u10d5\u10d8\u10e1', '\u10de\u10d0\u10e2\u10d0\u10e0\u10d0', '\u10dc\u10d0@@', '\u10ea@@', '\u10e0\u10d8\u10e1@@', '\u10e4\u10d4@@', '\u10e0', '\u10e0\u10d0@@', '\u10d3\u10d8@@', '\u10dd\u10e8\u10d8', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .',)
2021-10-03 13:28:28,245 - INFO - joeynmt.training - 	Source:     \u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .
2021-10-03 13:28:28,246 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:28:28,246 - INFO - joeynmt.training - 	Hypothesis: My Malk Zen did my first Chrisk Courk submistic man .
2021-10-03 13:28:28,246 - INFO - joeynmt.training - Validation result (greedy) at epoch  79, step     5000: bleu:   8.47, loss: 104950.0156, ppl: 869.4948, duration: 9.2586s
2021-10-03 13:28:39,072 - INFO - joeynmt.training - Epoch  79: total training loss 653.22
2021-10-03 13:28:39,072 - INFO - joeynmt.training - EPOCH 80
2021-10-03 13:28:51,531 - INFO - joeynmt.training - Epoch  80: total training loss 603.20
2021-10-03 13:28:51,531 - INFO - joeynmt.training - EPOCH 81
2021-10-03 13:29:04,818 - INFO - joeynmt.training - Epoch  81: total training loss 588.38
2021-10-03 13:29:04,819 - INFO - joeynmt.training - EPOCH 82
2021-10-03 13:29:17,560 - INFO - joeynmt.training - Epoch  82: total training loss 567.15
2021-10-03 13:29:17,560 - INFO - joeynmt.training - EPOCH 83
2021-10-03 13:29:29,777 - INFO - joeynmt.training - Epoch  83: total training loss 560.08
2021-10-03 13:29:29,778 - INFO - joeynmt.training - EPOCH 84
2021-10-03 13:29:41,948 - INFO - joeynmt.training - Epoch  84: total training loss 544.91
2021-10-03 13:29:41,948 - INFO - joeynmt.training - EPOCH 85
2021-10-03 13:29:54,053 - INFO - joeynmt.training - Epoch  85: total training loss 538.43
2021-10-03 13:29:54,053 - INFO - joeynmt.training - EPOCH 86
2021-10-03 13:30:06,159 - INFO - joeynmt.training - Epoch  86: total training loss 526.10
2021-10-03 13:30:06,160 - INFO - joeynmt.training - EPOCH 87
2021-10-03 13:30:18,254 - INFO - joeynmt.training - Epoch  87: total training loss 522.70
2021-10-03 13:30:18,254 - INFO - joeynmt.training - EPOCH 88
2021-10-03 13:30:30,433 - INFO - joeynmt.training - Epoch  88: total training loss 512.51
2021-10-03 13:30:30,434 - INFO - joeynmt.training - EPOCH 89
2021-10-03 13:30:43,304 - INFO - joeynmt.training - Epoch  89: total training loss 507.90
2021-10-03 13:30:43,305 - INFO - joeynmt.training - EPOCH 90
2021-10-03 13:30:55,429 - INFO - joeynmt.training - Epoch  90: total training loss 504.09
2021-10-03 13:30:55,430 - INFO - joeynmt.training - EPOCH 91
2021-10-03 13:31:07,618 - INFO - joeynmt.training - Epoch  91: total training loss 494.61
2021-10-03 13:31:07,619 - INFO - joeynmt.training - EPOCH 92
2021-10-03 13:31:19,796 - INFO - joeynmt.training - Epoch  92: total training loss 496.04
2021-10-03 13:31:19,797 - INFO - joeynmt.training - EPOCH 93
2021-10-03 13:31:31,927 - INFO - joeynmt.training - Epoch  93: total training loss 491.09
2021-10-03 13:31:31,927 - INFO - joeynmt.training - EPOCH 94
2021-10-03 13:31:41,109 - INFO - joeynmt.training - Epoch  94, Step:     6000, Batch Loss:     7.799337, Tokens per Sec:     9087, Lr: 0.000030
2021-10-03 13:31:51,343 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:31:51,344 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:31:51,344 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:31:51,350 - INFO - joeynmt.training - Example #0
2021-10-03 13:31:51,351 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:31:51,351 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:31:51,351 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-03 13:31:51,351 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 73-79: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea', '1@@', '1', '\u10ec\u10da\u10d8\u10e1', '\u10d5\u10d8\u10e7\u10d0\u10d5\u10d8', ',', '\u10db\u10d4', '\u10db\u10d0@@', '\u10ee\u10e1\u10dd@@', '\u10d5\u10e1', '\u10e1\u10d8@@', '\u10ee\u10d0\u10e0@@', '\u10e3\u10da\u10d8\u10e1', '\u10dc\u10dd@@', '\u10e2\u10d0@@', '\u10d6\u10d4', '\u10d2\u10d0@@', '\u10e6@@', '\u10d5\u10d8@@', '\u10eb@@', '\u10d4\u10d1\u10d0', '\u10d4\u10e0\u10d7', '\u10d3\u10d8@@', '\u10da\u10d0\u10e1', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 66-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."',)
2021-10-03 13:31:51,354 - INFO - joeynmt.training - 	Source:     "\u10e0\u10dd\u10d3\u10d4\u10e1\u10d0\u10ea 11 \u10ec\u10da\u10d8\u10e1 \u10d5\u10d8\u10e7\u10d0\u10d5\u10d8 , \u10db\u10d4 \u10db\u10d0\u10ee\u10e1\u10dd\u10d5\u10e1 \u10e1\u10d8\u10ee\u10d0\u10e0\u10e3\u10da\u10d8\u10e1 \u10dc\u10dd\u10e2\u10d0\u10d6\u10d4 \u10d2\u10d0\u10e6\u10d5\u10d8\u10eb\u10d4\u10d1\u10d0 \u10d4\u10e0\u10d7 \u10d3\u10d8\u10da\u10d0\u10e1 ."
2021-10-03 13:31:51,355 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:31:51,355 - INFO - joeynmt.training - 	Hypothesis: "When I was 17 years old , I found out of a terrible trained for a second ."
2021-10-03 13:31:51,355 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-75: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u10db\u10d0\u10db\u10d0@@', '\u10e9\u10d4\u10db\u10d8', '\u10e3@@', '\u10e1\u10db\u10d4@@', '\u10dc\u10d3\u10d0', '&quot;', '&quot;', '\u10d1\u10d8@@', '\u10d1@@', '\u10d8\u10e1@@', '\u10d8\u10e1', '&quot;', '&quot;', '\u10e1\u10d8@@', '\u10d0\u10ee@@', '\u10da\u10d4@@', '\u10d4\u10d1\u10e1', '\u10d7\u10d0\u10d5\u10d8\u10e1', '\u10de\u10d0\u10e2\u10d0\u10e0\u10d0', '\u10dc\u10d0@@', '\u10ea@@', '\u10e0\u10d8\u10e1@@', '\u10e4\u10d4@@', '\u10e0', '\u10e0\u10d0@@', '\u10d3\u10d8@@', '\u10dd\u10e8\u10d8', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .',)
2021-10-03 13:31:51,356 - INFO - joeynmt.training - 	Source:     \u10db\u10d0\u10db\u10d0\u10e9\u10d4\u10db\u10d8 \u10e3\u10e1\u10db\u10d4\u10dc\u10d3\u10d0 &quot; &quot; \u10d1\u10d8\u10d1\u10d8\u10e1\u10d8\u10e1 &quot; &quot; \u10e1\u10d8\u10d0\u10ee\u10da\u10d4\u10d4\u10d1\u10e1 \u10d7\u10d0\u10d5\u10d8\u10e1 \u10de\u10d0\u10e2\u10d0\u10e0\u10d0 \u10dc\u10d0\u10ea\u10e0\u10d8\u10e1\u10e4\u10d4\u10e0 \u10e0\u10d0\u10d3\u10d8\u10dd\u10e8\u10d8 .
2021-10-03 13:31:51,357 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:31:51,357 - INFO - joeynmt.training - 	Hypothesis: My Malk also in my first Chrisk Eginian Issad World War .
2021-10-03 13:31:51,357 - INFO - joeynmt.training - Validation result (greedy) at epoch  94, step     6000: bleu:   8.58, loss: 105505.5234, ppl: 901.2077, duration: 10.2475s
2021-10-03 13:31:54,392 - INFO - joeynmt.training - Epoch  94: total training loss 478.71
2021-10-03 13:31:54,393 - INFO - joeynmt.training - EPOCH 95
2021-10-03 13:32:06,579 - INFO - joeynmt.training - Epoch  95: total training loss 480.46
2021-10-03 13:32:06,579 - INFO - joeynmt.training - EPOCH 96
2021-10-03 13:32:18,857 - INFO - joeynmt.training - Epoch  96: total training loss 471.92
2021-10-03 13:32:18,857 - INFO - joeynmt.training - EPOCH 97
2021-10-03 13:32:31,529 - INFO - joeynmt.training - Epoch  97: total training loss 474.88
2021-10-03 13:32:31,529 - INFO - joeynmt.training - EPOCH 98
2021-10-03 13:32:44,170 - INFO - joeynmt.training - Epoch  98: total training loss 464.58
2021-10-03 13:32:44,170 - INFO - joeynmt.training - EPOCH 99
2021-10-03 13:32:57,414 - INFO - joeynmt.training - Epoch  99: total training loss 463.71
2021-10-03 13:32:57,415 - INFO - joeynmt.training - EPOCH 100
2021-10-03 13:33:10,612 - INFO - joeynmt.training - Epoch 100: total training loss 456.77
2021-10-03 13:33:10,612 - INFO - joeynmt.training - Training ended after 100 epochs.
2021-10-03 13:33:10,613 - INFO - joeynmt.training - Best validation result (greedy) at step     3000:   8.58 eval_metric.
2021-10-03 13:33:10,647 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-03 13:33:10,648 - INFO - joeynmt.prediction - Loading model from models/TRANS_ka/3000.ckpt
2021-10-03 13:33:11,397 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:33:12,208 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:33:12,364 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/ka.en_s/val.bpe.en_s)...
2021-10-03 13:33:34,918 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:33:34,918 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:33:34,918 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:33:34,922 - INFO - joeynmt.prediction -  dev bleu[13a]:   9.19 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:33:34,925 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_ka/00003000.hyps.dev
2021-10-03 13:33:34,925 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/ka.en_s/test.bpe.en_s)...
2021-10-03 13:34:03,166 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:34:03,167 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:34:03,167 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:34:03,171 - INFO - joeynmt.prediction - test bleu[13a]:   9.91 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:34:03,174 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_ka/00003000.hyps.test
