2021-10-11 19:24:20,305 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-11 19:24:20,370 - INFO - joeynmt.data - Loading training data...
2021-10-11 19:24:23,009 - INFO - joeynmt.data - Building vocabulary...
2021-10-11 19:24:23,461 - INFO - joeynmt.data - Loading dev data...
2021-10-11 19:24:23,548 - INFO - joeynmt.data - Loading test data...
2021-10-11 19:24:23,636 - INFO - joeynmt.data - Data loaded.
2021-10-11 19:24:23,636 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-11 19:24:24,217 - INFO - joeynmt.model - Enc-dec model built.
2021-10-11 19:24:24,223 - INFO - joeynmt.training - Total params: 24914944
2021-10-11 19:24:28,385 - INFO - joeynmt.helpers - cfg.name                           : FINAL_ru_r
2021-10-11 19:24:28,385 - INFO - joeynmt.helpers - cfg.data.src                       : ru_r
2021-10-11 19:24:28,385 - INFO - joeynmt.helpers - cfg.data.trg                       : en
2021-10-11 19:24:28,385 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/ru_r.en/train.bpe
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/ru_r.en/val.bpe
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/ru_r.en/test.bpe
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 100000
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 100000
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/ru_r.en/ru_r.en.vocab.txt
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/ru_r.en/ru_r.en.vocab.txt
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-11 19:24:28,386 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 5e-07
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.clip_grad_norm        : 1.0
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.patience              : 10
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.5
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.epochs                : 20
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 100
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/FINAL_ru_r
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-11 19:24:28,387 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.encoder.rnn_type         : lstm
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : False
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.encoder.bidirectional    : False
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.2
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 1
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.decoder.rnn_type         : lstm
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : False
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.decoder.emb_scale        : False
2021-10-11 19:24:28,388 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 1024
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.2
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_dropout   : 0.2
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 1
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - cfg.model.decoder.input_feeding    : True
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - cfg.model.decoder.init_hidden      : bridge
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - cfg.model.decoder.attention        : bahdanau
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - Data set sizes: 
	train 82612,
	valid 4815,
	test 5484
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - First training example:
	[SRC] ru_r
	[TRG] en
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) ." (6) . (7) the (8) a (9) to
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) ." (6) . (7) the (8) a (9) to
2021-10-11 19:24:28,389 - INFO - joeynmt.helpers - Number of Src words (types): 4222
2021-10-11 19:24:28,390 - INFO - joeynmt.helpers - Number of Trg words (types): 4222
2021-10-11 19:24:28,390 - INFO - joeynmt.training - Model(
	encoder=RecurrentEncoder(LSTM(512, 512, batch_first=True)),
	decoder=RecurrentDecoder(rnn=LSTM(1536, 1024, batch_first=True), attention=BahdanauAttention),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4222),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4222))
2021-10-11 19:24:28,400 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-11 19:24:28,401 - INFO - joeynmt.training - EPOCH 1
2021-10-11 19:25:19,067 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:25:19,067 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:25:19,067 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:25:19,100 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:25:20,033 - INFO - joeynmt.training - Example #0
2021-10-11 19:25:20,033 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:25:20,034 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:25:20,034 - INFO - joeynmt.training - 	Hypothesis: I &apos;s .
2021-10-11 19:25:20,034 - INFO - joeynmt.training - Example #1
2021-10-11 19:25:20,034 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:25:20,034 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:25:20,035 - INFO - joeynmt.training - 	Hypothesis: "And I , , , , , , , , , , , , , , , , , , , , , , , ."
2021-10-11 19:25:20,035 - INFO - joeynmt.training - Example #2
2021-10-11 19:25:20,035 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:25:20,035 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:25:20,035 - INFO - joeynmt.training - 	Hypothesis: "And I , , , , , , , , , , , , , , , , , ,
2021-10-11 19:25:20,036 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      100: bleu:   0.23, loss: 877329.0625, ppl: 528.6644, duration: 30.1968s
2021-10-11 19:26:16,059 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:26:16,060 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:26:16,060 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:26:16,094 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:26:17,008 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/100.ckpt
2021-10-11 19:26:17,081 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/100.ckpt
2021-10-11 19:26:17,082 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/100.ckpt')
2021-10-11 19:26:17,087 - INFO - joeynmt.training - Example #0
2021-10-11 19:26:17,087 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:26:17,087 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:26:17,087 - INFO - joeynmt.training - 	Hypothesis: ( Applause
2021-10-11 19:26:17,087 - INFO - joeynmt.training - Example #1
2021-10-11 19:26:17,088 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:26:17,088 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:26:17,088 - INFO - joeynmt.training - 	Hypothesis: "And , the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2021-10-11 19:26:17,088 - INFO - joeynmt.training - Example #2
2021-10-11 19:26:17,088 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:26:17,088 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:26:17,088 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2021-10-11 19:26:17,088 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      200: bleu:   0.57, loss: 842034.9375, ppl: 410.7986, duration: 34.4204s
2021-10-11 19:27:16,152 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:27:16,153 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:27:16,153 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:27:16,187 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:27:17,103 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/200.ckpt
2021-10-11 19:27:17,175 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/200.ckpt
2021-10-11 19:27:17,175 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/200.ckpt')
2021-10-11 19:27:17,180 - INFO - joeynmt.training - Example #0
2021-10-11 19:27:17,180 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:27:17,180 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:27:17,180 - INFO - joeynmt.training - 	Hypothesis: Thank you a .
2021-10-11 19:27:17,180 - INFO - joeynmt.training - Example #1
2021-10-11 19:27:17,180 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:27:17,180 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:27:17,180 - INFO - joeynmt.training - 	Hypothesis: "And I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I you the the the the the the the the the the the the the the the the the ."
2021-10-11 19:27:17,180 - INFO - joeynmt.training - Example #2
2021-10-11 19:27:17,181 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:27:17,181 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:27:17,181 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s a the the the the the the the the the the the the the the the the the the the the the ."
2021-10-11 19:27:17,181 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      300: bleu:   0.92, loss: 808078.6250, ppl: 322.2780, duration: 33.9211s
2021-10-11 19:28:08,913 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:28:08,913 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:28:08,913 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:28:08,948 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:28:09,852 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/300.ckpt
2021-10-11 19:28:09,923 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/300.ckpt
2021-10-11 19:28:09,923 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/300.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/300.ckpt')
2021-10-11 19:28:09,927 - INFO - joeynmt.training - Example #0
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Hypothesis: Thank you a a lot
2021-10-11 19:28:09,928 - INFO - joeynmt.training - Example #1
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Hypothesis: "And I have a lot of the world , I have a lot of the world , we have a lot of the world , we &apos;re a lot of the world ."
2021-10-11 19:28:09,928 - INFO - joeynmt.training - Example #2
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:28:09,928 - INFO - joeynmt.training - 	Hypothesis: "And the first a little a world , we have a lot of the world , we have a lot of the world ."
2021-10-11 19:28:09,929 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      400: bleu:   2.29, loss: 752441.6875, ppl: 216.5392, duration: 28.8568s
2021-10-11 19:28:57,909 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:28:57,909 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:28:57,909 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:28:57,944 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:28:58,829 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/400.ckpt
2021-10-11 19:28:58,900 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/400.ckpt
2021-10-11 19:28:58,900 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/400.ckpt')
2021-10-11 19:28:58,905 - INFO - joeynmt.training - Example #0
2021-10-11 19:28:58,905 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:28:58,905 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:28:58,905 - INFO - joeynmt.training - 	Hypothesis: Thank you a very very very very very .
2021-10-11 19:28:58,905 - INFO - joeynmt.training - Example #1
2021-10-11 19:28:58,906 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:28:58,906 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:28:58,906 - INFO - joeynmt.training - 	Hypothesis: "And I &apos;m going to be a way , I &apos;m going to be a way , I &apos;m going to be a way , I &apos;m going to be a world ."
2021-10-11 19:28:58,906 - INFO - joeynmt.training - Example #2
2021-10-11 19:28:58,906 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:28:58,906 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:28:58,906 - INFO - joeynmt.training - 	Hypothesis: "And I have a little way , I have a little way , I &apos;re going to be a world ."
2021-10-11 19:28:58,906 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      500: bleu:   3.26, loss: 729846.3750, ppl: 184.2477, duration: 26.0016s
2021-10-11 19:29:45,997 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:29:45,997 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:29:45,997 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:29:46,032 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:29:46,946 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/500.ckpt
2021-10-11 19:29:47,016 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/500.ckpt
2021-10-11 19:29:47,017 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/500.ckpt')
2021-10-11 19:29:47,021 - INFO - joeynmt.training - Example #0
2021-10-11 19:29:47,022 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:29:47,022 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:29:47,022 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-11 19:29:47,022 - INFO - joeynmt.training - Example #1
2021-10-11 19:29:47,022 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:29:47,022 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:29:47,022 - INFO - joeynmt.training - 	Hypothesis: "And I &apos;m going to be a little world , and I can see the world , and I can do it , and I can do it ."
2021-10-11 19:29:47,022 - INFO - joeynmt.training - Example #2
2021-10-11 19:29:47,022 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:29:47,022 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:29:47,023 - INFO - joeynmt.training - 	Hypothesis: "And I can be a lot of the world , and I can be a lot of the world ."
2021-10-11 19:29:47,023 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      600: bleu:   3.74, loss: 713969.3125, ppl: 164.4828, duration: 25.5033s
2021-10-11 19:30:33,412 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:30:33,412 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:30:33,412 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:30:33,448 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:30:34,349 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/600.ckpt
2021-10-11 19:30:34,421 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/600.ckpt
2021-10-11 19:30:34,421 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/600.ckpt')
2021-10-11 19:30:34,426 - INFO - joeynmt.training - Example #0
2021-10-11 19:30:34,426 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:30:34,426 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:30:34,426 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very bit .
2021-10-11 19:30:34,426 - INFO - joeynmt.training - Example #1
2021-10-11 19:30:34,427 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:30:34,427 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:30:34,427 - INFO - joeynmt.training - 	Hypothesis: "And I &apos;m going to be a lot of the world , and I &apos;m going to be a lot of the world ."
2021-10-11 19:30:34,427 - INFO - joeynmt.training - Example #2
2021-10-11 19:30:34,427 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:30:34,427 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:30:34,427 - INFO - joeynmt.training - 	Hypothesis: "( Laughter ) And I was a lot of the world , and I was a lot of the world , and I was a lot of the world ."
2021-10-11 19:30:34,427 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      700: bleu:   4.29, loss: 701057.4375, ppl: 149.9833, duration: 24.6486s
2021-10-11 19:31:27,697 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:31:27,698 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:31:27,698 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:31:27,738 - INFO - joeynmt.training - Example #0
2021-10-11 19:31:27,739 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:31:27,739 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:31:27,739 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a lot s .
2021-10-11 19:31:27,739 - INFO - joeynmt.training - Example #1
2021-10-11 19:31:27,739 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:31:27,739 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:31:27,739 - INFO - joeynmt.training - 	Hypothesis: "And I &apos;m going to say , I &apos;m going to be a lot of the world , and I &apos;m going to be a lot of the world ."
2021-10-11 19:31:27,739 - INFO - joeynmt.training - Example #2
2021-10-11 19:31:27,740 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:31:27,740 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:31:27,740 - INFO - joeynmt.training - 	Hypothesis: ( Laughter ) And the first thing , and I &apos;ve been a lot of the world , and I &apos;ve been a lot of the world ."
2021-10-11 19:31:27,740 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      800: bleu:   4.18, loss: 687728.6875, ppl: 136.3550, duration: 30.2102s
2021-10-11 19:32:24,094 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:32:24,095 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:32:24,095 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:32:24,147 - INFO - joeynmt.training - Example #0
2021-10-11 19:32:24,148 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:32:24,148 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:32:24,148 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very bit .
2021-10-11 19:32:24,148 - INFO - joeynmt.training - Example #1
2021-10-11 19:32:24,148 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:32:24,148 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:32:24,148 - INFO - joeynmt.training - 	Hypothesis: "I &apos;m going to show you a lot of the world , and I &apos;m going to be a lot of the world ."
2021-10-11 19:32:24,148 - INFO - joeynmt.training - Example #2
2021-10-11 19:32:24,148 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:32:24,149 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:32:24,149 - INFO - joeynmt.training - 	Hypothesis: ( Laughter ) And I was a lot of the world , and the same time , and the same thing ."
2021-10-11 19:32:24,149 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step      900: bleu:   3.88, loss: 679291.6875, ppl: 128.3758, duration: 33.0888s
2021-10-11 19:32:46,846 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:   102.827713, Tokens per Sec:     7862, Lr: 0.000300
2021-10-11 19:33:20,006 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:33:20,006 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:33:20,006 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:33:20,045 - INFO - joeynmt.training - Example #0
2021-10-11 19:33:20,046 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:33:20,046 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:33:20,046 - INFO - joeynmt.training - 	Hypothesis: It &apos;s a very .
2021-10-11 19:33:20,046 - INFO - joeynmt.training - Example #1
2021-10-11 19:33:20,046 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:33:20,046 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:33:20,046 - INFO - joeynmt.training - 	Hypothesis: "And I was going to be a little bit , I was to be a little bit of the first time , and I was to be a little bit ."
2021-10-11 19:33:20,046 - INFO - joeynmt.training - Example #2
2021-10-11 19:33:20,046 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:33:20,046 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:33:20,047 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to be a little bit of the first , and I was a little bit of the same ."
2021-10-11 19:33:20,047 - INFO - joeynmt.training - Validation result (greedy) at epoch   1, step     1000: bleu:   3.25, loss: 664608.4375, ppl: 115.5865, duration: 33.1996s
2021-10-11 19:33:28,560 - INFO - joeynmt.training - Epoch   1: total training loss 124617.50
2021-10-11 19:33:28,560 - INFO - joeynmt.training - EPOCH 2
2021-10-11 19:34:15,372 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:34:15,372 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:34:15,372 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:34:15,407 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:34:16,360 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/700.ckpt
2021-10-11 19:34:16,427 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/700.ckpt
2021-10-11 19:34:16,427 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/700.ckpt')
2021-10-11 19:34:16,432 - INFO - joeynmt.training - Example #0
2021-10-11 19:34:16,432 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:34:16,432 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:34:16,432 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:34:16,432 - INFO - joeynmt.training - Example #1
2021-10-11 19:34:16,433 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:34:16,433 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:34:16,433 - INFO - joeynmt.training - 	Hypothesis: "I was going to me , I was going to be a lot of the world , I was going to be a lot of the world ."
2021-10-11 19:34:16,433 - INFO - joeynmt.training - Example #2
2021-10-11 19:34:16,433 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:34:16,433 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:34:16,433 - INFO - joeynmt.training - 	Hypothesis: "I &apos;m going to my first , I was a little bit of the Saban , and I was a very bit of the Saban ."
2021-10-11 19:34:16,433 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1100: bleu:   4.97, loss: 653798.8750, ppl: 106.9929, duration: 33.5950s
2021-10-11 19:35:10,822 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:35:10,822 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:35:10,822 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:35:10,863 - INFO - joeynmt.training - Example #0
2021-10-11 19:35:10,863 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:35:10,863 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:35:10,863 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:35:10,863 - INFO - joeynmt.training - Example #1
2021-10-11 19:35:10,863 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:35:10,864 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:35:10,864 - INFO - joeynmt.training - 	Hypothesis: "And I was going to , I was a little bit of the first time , and I was a lot of the world ."
2021-10-11 19:35:10,864 - INFO - joeynmt.training - Example #2
2021-10-11 19:35:10,864 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:35:10,864 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:35:10,864 - INFO - joeynmt.training - 	Hypothesis: I was a little bit of my Soran Sidan , and the first is a little bit of the world ."
2021-10-11 19:35:10,864 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1200: bleu:   4.86, loss: 647450.6250, ppl: 102.2469, duration: 31.5963s
2021-10-11 19:36:06,379 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:36:06,379 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:36:06,379 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:36:06,419 - INFO - joeynmt.training - Example #0
2021-10-11 19:36:06,420 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:36:06,420 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:36:06,420 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:36:06,420 - INFO - joeynmt.training - Example #1
2021-10-11 19:36:06,420 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:36:06,420 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:36:06,420 - INFO - joeynmt.training - 	Hypothesis: "When I was a few years ago , I was a little bit of the first time , and I was a little bit of the first time ."
2021-10-11 19:36:06,421 - INFO - joeynmt.training - Example #2
2021-10-11 19:36:06,421 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:36:06,421 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:36:06,421 - INFO - joeynmt.training - 	Hypothesis: I &apos;m a little bit of my first time . &quot; &quot; And the first is a little bit of the first time .
2021-10-11 19:36:06,421 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1300: bleu:   4.86, loss: 638623.3750, ppl:  95.9955, duration: 32.5966s
2021-10-11 19:37:02,341 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:37:02,341 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:37:02,341 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:37:02,378 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:37:03,328 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/1100.ckpt
2021-10-11 19:37:03,402 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/1100.ckpt
2021-10-11 19:37:03,402 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/1100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/1100.ckpt')
2021-10-11 19:37:03,408 - INFO - joeynmt.training - Example #0
2021-10-11 19:37:03,408 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:37:03,408 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:37:03,408 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:37:03,408 - INFO - joeynmt.training - Example #1
2021-10-11 19:37:03,408 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:37:03,408 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:37:03,408 - INFO - joeynmt.training - 	Hypothesis: "And I was a few years ago , I was a little bit of the first time , I was a little bit of the first time ."
2021-10-11 19:37:03,408 - INFO - joeynmt.training - Example #2
2021-10-11 19:37:03,408 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:37:03,409 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:37:03,409 - INFO - joeynmt.training - 	Hypothesis: I &apos;m a little girl . &quot; &quot; The Sar is a very bit of the first time .
2021-10-11 19:37:03,409 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1400: bleu:   4.99, loss: 631635.6250, ppl:  91.3191, duration: 34.1822s
2021-10-11 19:37:59,120 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:37:59,120 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:37:59,121 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:37:59,164 - INFO - joeynmt.training - Example #0
2021-10-11 19:37:59,165 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:37:59,165 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:37:59,165 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:37:59,165 - INFO - joeynmt.training - Example #1
2021-10-11 19:37:59,165 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:37:59,165 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:37:59,165 - INFO - joeynmt.training - 	Hypothesis: "When I was my first time , I was a little bit of my time , I was a little bit of the last time ."
2021-10-11 19:37:59,165 - INFO - joeynmt.training - Example #2
2021-10-11 19:37:59,166 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:37:59,166 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:37:59,166 - INFO - joeynmt.training - 	Hypothesis: "I &apos;m going to say , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-11 19:37:59,166 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1500: bleu:   4.60, loss: 624133.4375, ppl:  86.5516, duration: 33.2556s
2021-10-11 19:38:57,528 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:38:57,529 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:38:57,529 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:38:57,571 - INFO - joeynmt.training - Example #0
2021-10-11 19:38:57,571 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:38:57,572 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:38:57,572 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:38:57,572 - INFO - joeynmt.training - Example #1
2021-10-11 19:38:57,572 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:38:57,572 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:38:57,573 - INFO - joeynmt.training - 	Hypothesis: "And I was my first time , I was a little bit of the first time that I was a little bit of the time ."
2021-10-11 19:38:57,573 - INFO - joeynmt.training - Example #2
2021-10-11 19:38:57,573 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:38:57,573 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:38:57,573 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to show you a little bit of the Wian Barban .
2021-10-11 19:38:57,574 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1600: bleu:   4.77, loss: 616376.0000, ppl:  81.8835, duration: 34.0339s
2021-10-11 19:39:53,876 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:39:53,877 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:39:53,877 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:39:53,914 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:39:54,844 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/1400.ckpt
2021-10-11 19:39:54,917 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/1400.ckpt
2021-10-11 19:39:54,918 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/1400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/1400.ckpt')
2021-10-11 19:39:54,923 - INFO - joeynmt.training - Example #0
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:39:54,924 - INFO - joeynmt.training - Example #1
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Hypothesis: "And I was a few years ago , I was going to be a little bit of the first of the end of the end of the end of the end of the end ."
2021-10-11 19:39:54,924 - INFO - joeynmt.training - Example #2
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:39:54,924 - INFO - joeynmt.training - 	Hypothesis: My first I was a very bit of the SESSES .
2021-10-11 19:39:54,925 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1700: bleu:   5.32, loss: 612760.7500, ppl:  79.7948, duration: 34.5326s
2021-10-11 19:40:51,085 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:40:51,085 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:40:51,085 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:40:51,129 - INFO - joeynmt.training - Example #0
2021-10-11 19:40:51,130 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:40:51,130 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:40:51,131 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:40:51,131 - INFO - joeynmt.training - Example #1
2021-10-11 19:40:51,131 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:40:51,131 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:40:51,132 - INFO - joeynmt.training - 	Hypothesis: "And I was a few years ago , I was going to be a little bit of a couple of my life ."
2021-10-11 19:40:51,132 - INFO - joeynmt.training - Example #2
2021-10-11 19:40:51,132 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:40:51,132 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:40:51,133 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to show you a little bit of the Miaa Reaa .
2021-10-11 19:40:51,133 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1800: bleu:   4.44, loss: 604362.7500, ppl:  75.1463, duration: 33.5466s
2021-10-11 19:41:45,608 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:41:45,608 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:41:45,608 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:41:45,651 - INFO - joeynmt.training - Example #0
2021-10-11 19:41:45,651 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:41:45,651 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:41:45,651 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:41:45,651 - INFO - joeynmt.training - Example #1
2021-10-11 19:41:45,651 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:41:45,652 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:41:45,652 - INFO - joeynmt.training - 	Hypothesis: "When I was in my day , I was going to be a little bit of the first time in the end of the end of the front of the front of the time ."
2021-10-11 19:41:45,652 - INFO - joeynmt.training - Example #2
2021-10-11 19:41:45,652 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:41:45,652 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:41:45,652 - INFO - joeynmt.training - 	Hypothesis: ( Laughter ) And I &apos;m going to show you a little girl .
2021-10-11 19:41:45,652 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     1900: bleu:   5.13, loss: 599273.8125, ppl:  72.4623, duration: 31.9400s
2021-10-11 19:42:08,376 - INFO - joeynmt.training - Epoch   2, Step:     2000, Batch Loss:    93.385025, Tokens per Sec:     7981, Lr: 0.000300
2021-10-11 19:42:40,771 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:42:40,771 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:42:40,771 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:42:40,809 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:42:41,729 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/1700.ckpt
2021-10-11 19:42:41,802 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/1700.ckpt
2021-10-11 19:42:41,802 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/1700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/1700.ckpt')
2021-10-11 19:42:41,807 - INFO - joeynmt.training - Example #0
2021-10-11 19:42:41,807 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:42:41,807 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:42:41,807 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:42:41,807 - INFO - joeynmt.training - Example #1
2021-10-11 19:42:41,807 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:42:41,807 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:42:41,807 - INFO - joeynmt.training - 	Hypothesis: "And I was 10,000 years ago , I &apos;m going to be a little bit of the end of the end of the end of the end of the end of the end ."
2021-10-11 19:42:41,807 - INFO - joeynmt.training - Example #2
2021-10-11 19:42:41,808 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:42:41,808 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:42:41,808 - INFO - joeynmt.training - 	Hypothesis: My name is my Britical Budan in the Morban Paxin .
2021-10-11 19:42:41,808 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step     2000: bleu:   5.96, loss: 594884.1250, ppl:  70.2242, duration: 33.4310s
2021-10-11 19:42:57,137 - INFO - joeynmt.training - Epoch   2: total training loss 101682.89
2021-10-11 19:42:57,138 - INFO - joeynmt.training - EPOCH 3
2021-10-11 19:43:36,762 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:43:36,762 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:43:36,763 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:43:36,801 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:43:37,701 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/2000.ckpt
2021-10-11 19:43:37,775 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/2000.ckpt
2021-10-11 19:43:37,776 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/2000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/2000.ckpt')
2021-10-11 19:43:37,780 - INFO - joeynmt.training - Example #0
2021-10-11 19:43:37,780 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:43:37,780 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:43:37,780 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:43:37,780 - INFO - joeynmt.training - Example #1
2021-10-11 19:43:37,780 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:43:37,781 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:43:37,781 - INFO - joeynmt.training - 	Hypothesis: "And when I was in the first time , I &apos;m going to be a couple of my own time in the next time ."
2021-10-11 19:43:37,781 - INFO - joeynmt.training - Example #2
2021-10-11 19:43:37,781 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:43:37,781 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:43:37,781 - INFO - joeynmt.training - 	Hypothesis: "My father was a little bit of Beli , the Bewi &apos;s a bum of the South ."
2021-10-11 19:43:37,781 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2100: bleu:   6.00, loss: 589286.6875, ppl:  67.4703, duration: 33.2511s
2021-10-11 19:44:32,132 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:44:32,133 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:44:32,133 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:44:32,170 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:44:33,085 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/2100.ckpt
2021-10-11 19:44:33,157 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/2100.ckpt
2021-10-11 19:44:33,157 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/2100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/2100.ckpt')
2021-10-11 19:44:33,162 - INFO - joeynmt.training - Example #0
2021-10-11 19:44:33,162 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:44:33,162 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:44:33,162 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:44:33,162 - INFO - joeynmt.training - Example #1
2021-10-11 19:44:33,163 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:44:33,163 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:44:33,163 - INFO - joeynmt.training - 	Hypothesis: "And when I was 2,000 years ago , I &apos;m going to be a little bit of my own time ."
2021-10-11 19:44:33,163 - INFO - joeynmt.training - Example #2
2021-10-11 19:44:33,163 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:44:33,163 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:44:33,163 - INFO - joeynmt.training - 	Hypothesis: "My father was a very simple , Bank , I &apos;m a very simple critical ."
2021-10-11 19:44:33,163 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2200: bleu:   6.16, loss: 582614.1875, ppl:  64.3282, duration: 32.8023s
2021-10-11 19:45:29,164 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:45:29,164 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:45:29,164 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:45:29,202 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:45:30,126 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/2200.ckpt
2021-10-11 19:45:30,199 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/2200.ckpt
2021-10-11 19:45:30,199 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/2200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/2200.ckpt')
2021-10-11 19:45:30,204 - INFO - joeynmt.training - Example #0
2021-10-11 19:45:30,204 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:45:30,204 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:45:30,204 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:45:30,204 - INFO - joeynmt.training - Example #1
2021-10-11 19:45:30,204 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:45:30,204 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:45:30,204 - INFO - joeynmt.training - 	Hypothesis: "And I was a year , I said , I think I &apos;m going to go to the end of the end of the end of my life ."
2021-10-11 19:45:30,204 - INFO - joeynmt.training - Example #2
2021-10-11 19:45:30,205 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:45:30,205 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:45:30,205 - INFO - joeynmt.training - 	Hypothesis: My mother was a little bit of the Banch Cib in the South Sib .
2021-10-11 19:45:30,205 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2300: bleu:   6.67, loss: 579349.1250, ppl:  62.8445, duration: 34.1780s
2021-10-11 19:46:23,458 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:46:23,458 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:46:23,458 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:46:23,498 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:46:24,412 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/2300.ckpt
2021-10-11 19:46:24,482 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/2300.ckpt
2021-10-11 19:46:24,483 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/2300.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/2300.ckpt')
2021-10-11 19:46:24,487 - INFO - joeynmt.training - Example #0
2021-10-11 19:46:24,487 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:46:24,487 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:46:24,487 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:46:24,488 - INFO - joeynmt.training - Example #1
2021-10-11 19:46:24,488 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:46:24,488 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:46:24,488 - INFO - joeynmt.training - 	Hypothesis: "And I was 16 years , I &apos;m going to go to the end of the billage of the end of the table ."
2021-10-11 19:46:24,488 - INFO - joeynmt.training - Example #2
2021-10-11 19:46:24,488 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:46:24,488 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:46:24,488 - INFO - joeynmt.training - 	Hypothesis: My name is a very simple . The Loo Sert in the South Warch .
2021-10-11 19:46:24,488 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2400: bleu:   6.76, loss: 574921.1250, ppl:  60.8867, duration: 31.4819s
2021-10-11 19:47:21,338 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:47:21,338 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:47:21,338 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:47:21,413 - INFO - joeynmt.training - Example #0
2021-10-11 19:47:21,413 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:47:21,414 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:47:21,414 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:47:21,414 - INFO - joeynmt.training - Example #1
2021-10-11 19:47:21,414 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:47:21,415 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:47:21,415 - INFO - joeynmt.training - 	Hypothesis: "When I was a year , I &apos;m going to be a woman in the last time of the end of the end of the time ."
2021-10-11 19:47:21,415 - INFO - joeynmt.training - Example #2
2021-10-11 19:47:21,416 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:47:21,416 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:47:21,416 - INFO - joeynmt.training - 	Hypothesis: My name is my Britical Bake in the right in the right .
2021-10-11 19:47:21,416 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2500: bleu:   6.07, loss: 567263.9375, ppl:  57.6442, duration: 33.9103s
2021-10-11 19:48:16,739 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:48:16,739 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:48:16,739 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:48:16,775 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:48:17,827 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/2400.ckpt
2021-10-11 19:48:17,899 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/2400.ckpt
2021-10-11 19:48:17,900 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/2400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/2400.ckpt')
2021-10-11 19:48:17,904 - INFO - joeynmt.training - Example #0
2021-10-11 19:48:17,904 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:48:17,905 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:48:17,905 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:48:17,905 - INFO - joeynmt.training - Example #1
2021-10-11 19:48:17,905 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:48:17,905 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:48:17,905 - INFO - joeynmt.training - 	Hypothesis: "When I was 14 years ago , I think that I &apos;m going to go back to my house ."
2021-10-11 19:48:17,905 - INFO - joeynmt.training - Example #2
2021-10-11 19:48:17,905 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:48:17,905 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:48:17,905 - INFO - joeynmt.training - 	Hypothesis: My name is my Bakid Bake in the Bake Saxer .
2021-10-11 19:48:17,905 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2600: bleu:   7.36, loss: 562627.1250, ppl:  55.7652, duration: 33.9267s
2021-10-11 19:49:13,840 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:49:13,841 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:49:13,841 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:49:13,885 - INFO - joeynmt.training - Example #0
2021-10-11 19:49:13,885 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:49:13,886 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:49:13,886 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 19:49:13,886 - INFO - joeynmt.training - Example #1
2021-10-11 19:49:13,886 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:49:13,886 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:49:13,886 - INFO - joeynmt.training - 	Hypothesis: "And I was a young years ago , I &apos;m going to go back to the end of the bourth of the end of my own day ."
2021-10-11 19:49:13,886 - INFO - joeynmt.training - Example #2
2021-10-11 19:49:13,886 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:49:13,886 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:49:13,886 - INFO - joeynmt.training - 	Hypothesis: "My name was a little bit of a very small , John Dake , a little bit of the croyn ."
2021-10-11 19:49:13,887 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2700: bleu:   6.87, loss: 557861.1250, ppl:  53.8976, duration: 32.4487s
2021-10-11 19:50:09,645 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:50:09,646 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:50:09,646 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:50:09,683 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:50:10,601 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/2600.ckpt
2021-10-11 19:50:10,675 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/2600.ckpt
2021-10-11 19:50:10,676 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/2600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/2600.ckpt')
2021-10-11 19:50:10,680 - INFO - joeynmt.training - Example #0
2021-10-11 19:50:10,681 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:50:10,681 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:50:10,681 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:50:10,681 - INFO - joeynmt.training - Example #1
2021-10-11 19:50:10,681 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:50:10,681 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:50:10,681 - INFO - joeynmt.training - 	Hypothesis: "When I was 14 years , I &apos;m going to go to the end of my own day ."
2021-10-11 19:50:10,681 - INFO - joeynmt.training - Example #2
2021-10-11 19:50:10,681 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:50:10,682 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:50:10,682 - INFO - joeynmt.training - 	Hypothesis: My mother is a little bit of BIT .
2021-10-11 19:50:10,682 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2800: bleu:   7.38, loss: 553492.9375, ppl:  52.2409, duration: 33.7153s
/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/site-packages/matplotlib/image.py:863: UserWarning: Attempting to set identical left==right results
in singular transformations; automatically expanding.
left=-0.5, right=-0.5
  self.axes.set_xlim((xmin, xmax), auto=None)
2021-10-11 19:51:01,508 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:51:01,509 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:51:01,509 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:51:01,546 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:51:02,459 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/2800.ckpt
2021-10-11 19:51:02,527 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/2800.ckpt
2021-10-11 19:51:02,527 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/2800.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/2800.ckpt')
2021-10-11 19:51:02,532 - INFO - joeynmt.training - Example #0
2021-10-11 19:51:02,532 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:51:02,532 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:51:02,532 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:51:02,532 - INFO - joeynmt.training - Example #1
2021-10-11 19:51:02,532 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:51:02,532 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:51:02,532 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years ago , I &apos;m going to take a little bit of the walls of my own time ."
2021-10-11 19:51:02,532 - INFO - joeynmt.training - Example #2
2021-10-11 19:51:02,533 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:51:02,533 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:51:02,533 - INFO - joeynmt.training - 	Hypothesis: My father is a little bit of a very good . Somei .
2021-10-11 19:51:02,533 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     2900: bleu:   8.33, loss: 550385.3750, ppl:  51.0934, duration: 29.6178s
2021-10-11 19:51:25,018 - INFO - joeynmt.training - Epoch   3, Step:     3000, Batch Loss:    90.560555, Tokens per Sec:     7993, Lr: 0.000300
2021-10-11 19:51:55,954 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:51:55,954 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:51:55,954 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:51:55,995 - INFO - joeynmt.training - Example #0
2021-10-11 19:51:55,996 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:51:55,996 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:51:55,996 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:51:55,996 - INFO - joeynmt.training - Example #1
2021-10-11 19:51:55,996 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:51:55,996 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:51:55,996 - INFO - joeynmt.training - 	Hypothesis: "When I was 14 years ago , I think , I &apos;m going to go to the site of the air ."
2021-10-11 19:51:55,997 - INFO - joeynmt.training - Example #2
2021-10-11 19:51:55,997 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:51:55,997 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:51:55,997 - INFO - joeynmt.training - 	Hypothesis: My father was a little bit of a very good example of Bake .
2021-10-11 19:51:55,997 - INFO - joeynmt.training - Validation result (greedy) at epoch   3, step     3000: bleu:   8.22, loss: 545814.0625, ppl:  49.4511, duration: 30.9787s
2021-10-11 19:52:18,346 - INFO - joeynmt.training - Epoch   3: total training loss 91417.62
2021-10-11 19:52:18,346 - INFO - joeynmt.training - EPOCH 4
2021-10-11 19:52:50,231 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:52:50,232 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:52:50,232 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:52:50,278 - INFO - joeynmt.training - Example #0
2021-10-11 19:52:50,278 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:52:50,278 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:52:50,278 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:52:50,278 - INFO - joeynmt.training - Example #1
2021-10-11 19:52:50,278 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:52:50,279 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:52:50,279 - INFO - joeynmt.training - 	Hypothesis: "And when I was 15 years ago , I remember , I &apos;m going to be a soundous of the walls of the street ."
2021-10-11 19:52:50,279 - INFO - joeynmt.training - Example #2
2021-10-11 19:52:50,279 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:52:50,279 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:52:50,279 - INFO - joeynmt.training - 	Hypothesis: My father was a little bit of the BDP in a very small phone .
2021-10-11 19:52:50,279 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3100: bleu:   8.04, loss: 539971.3750, ppl:  47.4286, duration: 31.5655s
2021-10-11 19:53:43,463 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:53:43,463 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:53:43,463 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:53:43,501 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:53:44,411 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/2900.ckpt
2021-10-11 19:53:44,495 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/2900.ckpt
2021-10-11 19:53:44,496 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/2900.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/2900.ckpt')
2021-10-11 19:53:44,500 - INFO - joeynmt.training - Example #0
2021-10-11 19:53:44,501 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:53:44,501 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:53:44,501 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:53:44,501 - INFO - joeynmt.training - Example #1
2021-10-11 19:53:44,501 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:53:44,501 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:53:44,501 - INFO - joeynmt.training - 	Hypothesis: "When I was born , I remember , I think , I &apos;m going to go to the top of the code of the house ."
2021-10-11 19:53:44,501 - INFO - joeynmt.training - Example #2
2021-10-11 19:53:44,501 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:53:44,502 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:53:44,502 - INFO - joeynmt.training - 	Hypothesis: "My name is my word . Beir Bake , a little bit of the surface of the British ."
2021-10-11 19:53:44,502 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3200: bleu:   8.36, loss: 537331.7500, ppl:  46.5423, duration: 31.5887s
2021-10-11 19:54:39,023 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:54:39,023 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:54:39,023 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:54:39,065 - INFO - joeynmt.training - Example #0
2021-10-11 19:54:39,066 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:54:39,066 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:54:39,066 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:54:39,066 - INFO - joeynmt.training - Example #1
2021-10-11 19:54:39,066 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:54:39,066 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:54:39,066 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years , I remember the soundation of the sounds of the wall of my own day ."
2021-10-11 19:54:39,066 - INFO - joeynmt.training - Example #2
2021-10-11 19:54:39,067 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:54:39,067 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:54:39,067 - INFO - joeynmt.training - 	Hypothesis: My father was a good news . Beir Bake &apos;s a little bit of the surface of the British .
2021-10-11 19:54:39,067 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3300: bleu:   7.81, loss: 532383.1250, ppl:  44.9249, duration: 31.7318s
2021-10-11 19:55:30,186 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:55:30,186 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:55:30,186 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:55:30,225 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:55:31,179 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/3200.ckpt
2021-10-11 19:55:31,257 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/3200.ckpt
2021-10-11 19:55:31,257 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/3200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/3200.ckpt')
2021-10-11 19:55:31,262 - INFO - joeynmt.training - Example #0
2021-10-11 19:55:31,262 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:55:31,262 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:55:31,262 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:55:31,262 - INFO - joeynmt.training - Example #1
2021-10-11 19:55:31,262 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:55:31,262 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:55:31,262 - INFO - joeynmt.training - 	Hypothesis: "When I was in 1950 , I remember that the site of the left of the south of the left ."
2021-10-11 19:55:31,262 - INFO - joeynmt.training - Example #2
2021-10-11 19:55:31,263 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:55:31,263 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:55:31,263 - INFO - joeynmt.training - 	Hypothesis: "My father was a little bit of BDP , a little bit of the surface of the surface ."
2021-10-11 19:55:31,263 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3400: bleu:   8.79, loss: 530284.3125, ppl:  44.2561, duration: 29.4451s
2021-10-11 19:56:24,495 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:56:24,495 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:56:24,495 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:56:24,540 - INFO - joeynmt.training - Example #0
2021-10-11 19:56:24,540 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:56:24,540 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:56:24,540 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:56:24,541 - INFO - joeynmt.training - Example #1
2021-10-11 19:56:24,541 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:56:24,541 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:56:24,541 - INFO - joeynmt.training - 	Hypothesis: "When I was 14 years old , I remember to the end of the street of the beginning of my own day ."
2021-10-11 19:56:24,541 - INFO - joeynmt.training - Example #2
2021-10-11 19:56:24,541 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:56:24,541 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:56:24,541 - INFO - joeynmt.training - 	Hypothesis: My father is a good Bakian Bake in a little bit of the mathematics .
2021-10-11 19:56:24,542 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3500: bleu:   8.41, loss: 528280.9375, ppl:  43.6269, duration: 29.9805s
2021-10-11 19:57:20,139 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:57:20,139 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:57:20,139 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:57:20,182 - INFO - joeynmt.training - Example #0
2021-10-11 19:57:20,183 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:57:20,183 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:57:20,183 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:57:20,183 - INFO - joeynmt.training - Example #1
2021-10-11 19:57:20,183 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:57:20,183 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:57:20,183 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years , I remember that the rest of the rest of the crus of the walls ."
2021-10-11 19:57:20,183 - INFO - joeynmt.training - Example #2
2021-10-11 19:57:20,184 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:57:20,184 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:57:20,184 - INFO - joeynmt.training - 	Hypothesis: My father was a good news . Bwo Bake .
2021-10-11 19:57:20,184 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3600: bleu:   8.22, loss: 521942.4062, ppl:  41.6946, duration: 32.9062s
2021-10-11 19:58:13,443 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:58:13,444 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:58:13,444 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:58:13,482 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 19:58:14,392 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/3400.ckpt
2021-10-11 19:58:14,464 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/3400.ckpt
2021-10-11 19:58:14,464 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/3400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/3400.ckpt')
2021-10-11 19:58:14,469 - INFO - joeynmt.training - Example #0
2021-10-11 19:58:14,469 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:58:14,469 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:58:14,469 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:58:14,469 - INFO - joeynmt.training - Example #1
2021-10-11 19:58:14,470 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:58:14,470 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:58:14,470 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years , I remember , as a clip of a day , I &apos;m going to go back to my mother ."
2021-10-11 19:58:14,470 - INFO - joeynmt.training - Example #2
2021-10-11 19:58:14,470 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:58:14,470 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:58:14,470 - INFO - joeynmt.training - 	Hypothesis: "My father was a little bit of BBP , a little bit of a small Boldge ."
2021-10-11 19:58:14,470 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3700: bleu:   8.82, loss: 520373.0938, ppl:  41.2296, duration: 31.4938s
2021-10-11 19:59:08,326 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 19:59:08,327 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 19:59:08,327 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 19:59:08,369 - INFO - joeynmt.training - Example #0
2021-10-11 19:59:08,369 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 19:59:08,369 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 19:59:08,369 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 19:59:08,370 - INFO - joeynmt.training - Example #1
2021-10-11 19:59:08,370 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 19:59:08,370 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 19:59:08,370 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years , I remember , as I &apos;m going to go back to the movie in my house ."
2021-10-11 19:59:08,370 - INFO - joeynmt.training - Example #2
2021-10-11 19:59:08,370 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 19:59:08,370 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 19:59:08,370 - INFO - joeynmt.training - 	Hypothesis: "My father was , my name is a good Bub Bup , a little bit of the Bewish ."
2021-10-11 19:59:08,370 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3800: bleu:   8.56, loss: 517741.7188, ppl:  40.4614, duration: 31.2358s
2021-10-11 20:00:00,183 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:00:00,183 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:00:00,183 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:00:00,221 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:00:01,128 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/3700.ckpt
2021-10-11 20:00:01,195 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/3700.ckpt
2021-10-11 20:00:01,195 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/3700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/3700.ckpt')
2021-10-11 20:00:01,200 - INFO - joeynmt.training - Example #0
2021-10-11 20:00:01,200 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:00:01,200 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:00:01,201 - INFO - joeynmt.training - 	Hypothesis: s .
2021-10-11 20:00:01,201 - INFO - joeynmt.training - Example #1
2021-10-11 20:00:01,201 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:00:01,201 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:00:01,201 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the rest of the south of the table ."
2021-10-11 20:00:01,201 - INFO - joeynmt.training - Example #2
2021-10-11 20:00:01,201 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:00:01,201 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:00:01,201 - INFO - joeynmt.training - 	Hypothesis: "My mother was heard of the news BF , a little bit of the dark of the British ."
2021-10-11 20:00:01,201 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     3900: bleu:   9.45, loss: 512851.0000, ppl:  39.0715, duration: 30.0951s
2021-10-11 20:00:24,083 - INFO - joeynmt.training - Epoch   4, Step:     4000, Batch Loss:    84.796196, Tokens per Sec:     7966, Lr: 0.000300
2021-10-11 20:00:56,076 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:00:56,076 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:00:56,076 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:00:56,121 - INFO - joeynmt.training - Example #0
2021-10-11 20:00:56,121 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:00:56,121 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:00:56,121 - INFO - joeynmt.training - 	Hypothesis: s .
2021-10-11 20:00:56,121 - INFO - joeynmt.training - Example #1
2021-10-11 20:00:56,122 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:00:56,122 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:00:56,122 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years , I remember , like the crumest of the street of the time ."
2021-10-11 20:00:56,122 - INFO - joeynmt.training - Example #2
2021-10-11 20:00:56,122 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:00:56,122 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:00:56,122 - INFO - joeynmt.training - 	Hypothesis: "My grandmother asked me a good news , BBP to get a small box ."
2021-10-11 20:00:56,122 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     4000: bleu:   9.05, loss: 509018.5938, ppl:  38.0159, duration: 32.0384s
2021-10-11 20:01:51,016 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:01:51,016 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:01:51,016 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:01:51,059 - INFO - joeynmt.training - Example #0
2021-10-11 20:01:51,059 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:01:51,059 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:01:51,059 - INFO - joeynmt.training - 	Hypothesis: an .
2021-10-11 20:01:51,059 - INFO - joeynmt.training - Example #1
2021-10-11 20:01:51,060 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:01:51,060 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:01:51,060 - INFO - joeynmt.training - 	Hypothesis: "When I was in 1970s , I remember the site of the street of the house ."
2021-10-11 20:01:51,060 - INFO - joeynmt.training - Example #2
2021-10-11 20:01:51,060 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:01:51,060 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:01:51,060 - INFO - joeynmt.training - 	Hypothesis: "My father &apos;s mother heard , BBP and his Bod &apos;s a little bit of the horrible ."
2021-10-11 20:01:51,060 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step     4100: bleu:   9.31, loss: 504328.0938, ppl:  36.7626, duration: 32.1125s
2021-10-11 20:01:59,255 - INFO - joeynmt.training - Epoch   4: total training loss 83597.31
2021-10-11 20:01:59,256 - INFO - joeynmt.training - EPOCH 5
2021-10-11 20:02:43,247 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:02:43,247 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:02:43,247 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:02:43,285 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:02:44,225 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/3900.ckpt
2021-10-11 20:02:44,296 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/3900.ckpt
2021-10-11 20:02:44,297 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/3900.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/3900.ckpt')
2021-10-11 20:02:44,303 - INFO - joeynmt.training - Example #0
2021-10-11 20:02:44,303 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:02:44,303 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:02:44,303 - INFO - joeynmt.training - 	Hypothesis: s .
2021-10-11 20:02:44,304 - INFO - joeynmt.training - Example #1
2021-10-11 20:02:44,304 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:02:44,304 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:02:44,304 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember the site of the site of the left ."
2021-10-11 20:02:44,304 - INFO - joeynmt.training - Example #2
2021-10-11 20:02:44,305 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:02:44,305 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:02:44,305 - INFO - joeynmt.training - 	Hypothesis: "My father was listening to the news BBP , and I was a little bit of the red of the British ."
2021-10-11 20:02:44,305 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     4200: bleu:   9.66, loss: 501783.8125, ppl:  36.1001, duration: 30.5587s
2021-10-11 20:03:39,003 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:03:39,004 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:03:39,004 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:03:39,040 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:03:39,989 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/4200.ckpt
2021-10-11 20:03:40,066 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/4200.ckpt
2021-10-11 20:03:40,066 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/4200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/4200.ckpt')
2021-10-11 20:03:40,071 - INFO - joeynmt.training - Example #0
2021-10-11 20:03:40,071 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:03:40,071 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:03:40,071 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 20:03:40,071 - INFO - joeynmt.training - Example #1
2021-10-11 20:03:40,071 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:03:40,071 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:03:40,071 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years , I remember , like a month of a month of my house ."
2021-10-11 20:03:40,071 - INFO - joeynmt.training - Example #2
2021-10-11 20:03:40,072 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:03:40,072 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:03:40,072 - INFO - joeynmt.training - 	Hypothesis: My father was listening to the news BBP for a little bit of the map of the microphone .
2021-10-11 20:03:40,072 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     4300: bleu:   9.83, loss: 499584.0312, ppl:  35.5370, duration: 32.7043s
2021-10-11 20:04:35,765 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:04:35,765 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:04:35,765 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:04:35,803 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:04:36,716 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/4300.ckpt
2021-10-11 20:04:36,791 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/4300.ckpt
2021-10-11 20:04:36,791 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/4300.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/4300.ckpt')
2021-10-11 20:04:36,796 - INFO - joeynmt.training - Example #0
2021-10-11 20:04:36,796 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:04:36,796 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:04:36,796 - INFO - joeynmt.training - 	Hypothesis: an .
2021-10-11 20:04:36,797 - INFO - joeynmt.training - Example #1
2021-10-11 20:04:36,797 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:04:36,797 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:04:36,797 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years ago , I remember , as I was a kid of a house ."
2021-10-11 20:04:36,797 - INFO - joeynmt.training - Example #2
2021-10-11 20:04:36,797 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:04:36,797 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:04:36,797 - INFO - joeynmt.training - 	Hypothesis: My father was listening to the news BBBT in a little bit of a small British .
2021-10-11 20:04:36,797 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     4400: bleu:   9.88, loss: 497594.7812, ppl:  35.0353, duration: 33.3247s
2021-10-11 20:05:32,214 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:05:32,214 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:05:32,214 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:05:32,251 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:05:33,609 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/4400.ckpt
2021-10-11 20:05:33,683 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/4400.ckpt
2021-10-11 20:05:33,683 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/4400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/4400.ckpt')
2021-10-11 20:05:33,688 - INFO - joeynmt.training - Example #0
2021-10-11 20:05:33,688 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:05:33,688 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:05:33,688 - INFO - joeynmt.training - 	Hypothesis: an .
2021-10-11 20:05:33,688 - INFO - joeynmt.training - Example #1
2021-10-11 20:05:33,688 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:05:33,688 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:05:33,688 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years ago , I remember the month of the movie of my home ."
2021-10-11 20:05:33,689 - INFO - joeynmt.training - Example #2
2021-10-11 20:05:33,689 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:05:33,689 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:05:33,689 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BF : Look at the most beautiful market .
2021-10-11 20:05:33,689 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     4500: bleu:   9.98, loss: 494484.4062, ppl:  34.2651, duration: 34.0887s
2021-10-11 20:06:27,071 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:06:27,071 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:06:27,071 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:06:27,108 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:06:28,166 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/4500.ckpt
2021-10-11 20:06:28,238 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/4500.ckpt
2021-10-11 20:06:28,238 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/4500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/4500.ckpt')
2021-10-11 20:06:28,242 - INFO - joeynmt.training - Example #0
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Hypothesis: an .
2021-10-11 20:06:28,243 - INFO - joeynmt.training - Example #1
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember , how to the end of the room of the room of my eyes ."
2021-10-11 20:06:28,243 - INFO - joeynmt.training - Example #2
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:06:28,243 - INFO - joeynmt.training - 	Hypothesis: My father read the news BF Bain to my own market market .
2021-10-11 20:06:28,244 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     4600: bleu:  10.21, loss: 493235.8125, ppl:  33.9607, duration: 31.7018s
2021-10-11 20:07:20,120 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:07:20,120 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:07:20,121 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:07:20,161 - INFO - joeynmt.training - Example #0
2021-10-11 20:07:20,162 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:07:20,162 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:07:20,162 - INFO - joeynmt.training - 	Hypothesis: an .
2021-10-11 20:07:20,162 - INFO - joeynmt.training - Example #1
2021-10-11 20:07:20,162 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:07:20,162 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:07:20,162 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember how to put the back of the time with the house of the time with my house ."
2021-10-11 20:07:20,162 - INFO - joeynmt.training - Example #2
2021-10-11 20:07:20,162 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:07:20,162 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:07:20,163 - INFO - joeynmt.training - 	Hypothesis: "My father heard , I &apos;m listening to the news B.B. , and I &apos;m going to give you a little bit of a small psychological ."
2021-10-11 20:07:20,163 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     4700: bleu:   9.96, loss: 492429.7500, ppl:  33.7656, duration: 29.6275s
2021-10-11 20:08:12,562 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:08:12,562 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:08:12,562 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:08:12,598 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:08:13,521 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/4600.ckpt
2021-10-11 20:08:13,597 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/4600.ckpt
2021-10-11 20:08:13,598 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/4600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/4600.ckpt')
2021-10-11 20:08:13,602 - INFO - joeynmt.training - Example #0
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Hypothesis: a .
2021-10-11 20:08:13,603 - INFO - joeynmt.training - Example #1
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years ago , I remember the same , and I remember the fast of the street of my home ."
2021-10-11 20:08:13,603 - INFO - joeynmt.training - Example #2
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:08:13,603 - INFO - joeynmt.training - 	Hypothesis: My mother heard of the news B.B. City for me a little bit of a little bit of a magical crisis .
2021-10-11 20:08:13,604 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     4800: bleu:  10.67, loss: 487674.3125, ppl:  32.6373, duration: 30.6667s
2021-10-11 20:09:09,176 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:09:09,177 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:09:09,177 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:09:09,220 - INFO - joeynmt.training - Example #0
2021-10-11 20:09:09,220 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:09:09,220 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:09:09,220 - INFO - joeynmt.training - 	Hypothesis: s .
2021-10-11 20:09:09,220 - INFO - joeynmt.training - Example #1
2021-10-11 20:09:09,220 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:09:09,220 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:09:09,221 - INFO - joeynmt.training - 	Hypothesis: "When I was born , I remember the last year , I remember the day of the room of the room of the day ."
2021-10-11 20:09:09,221 - INFO - joeynmt.training - Example #2
2021-10-11 20:09:09,221 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:09:09,221 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:09:09,221 - INFO - joeynmt.training - 	Hypothesis: My father heard of the news BF Bank you have a small British of a magnetic psychology .
2021-10-11 20:09:09,221 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     4900: bleu:   9.97, loss: 484450.6562, ppl:  31.8939, duration: 32.1435s
2021-10-11 20:09:31,846 - INFO - joeynmt.training - Epoch   5, Step:     5000, Batch Loss:    79.906204, Tokens per Sec:     7991, Lr: 0.000300
2021-10-11 20:10:00,934 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:10:00,934 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:10:00,934 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:10:00,971 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:10:02,207 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/4800.ckpt
2021-10-11 20:10:02,279 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/4800.ckpt
2021-10-11 20:10:02,280 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/4800.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/4800.ckpt')
2021-10-11 20:10:02,284 - INFO - joeynmt.training - Example #0
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Hypothesis: s .
2021-10-11 20:10:02,285 - INFO - joeynmt.training - Example #1
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember the female of the time of the time of the night ."
2021-10-11 20:10:02,285 - INFO - joeynmt.training - Example #2
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:10:02,285 - INFO - joeynmt.training - 	Hypothesis: "My father heard , I heard the news Bayn Bather in my own map ."
2021-10-11 20:10:02,286 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     5000: bleu:  10.86, loss: 482782.2812, ppl:  31.5158, duration: 30.4386s
2021-10-11 20:10:53,604 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:10:53,604 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:10:53,604 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:10:53,641 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:10:54,661 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/5000.ckpt
2021-10-11 20:10:54,735 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/5000.ckpt
2021-10-11 20:10:54,736 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/5000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/5000.ckpt')
2021-10-11 20:10:54,741 - INFO - joeynmt.training - Example #0
2021-10-11 20:10:54,741 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:10:54,741 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:10:54,741 - INFO - joeynmt.training - 	Hypothesis: .
2021-10-11 20:10:54,741 - INFO - joeynmt.training - Example #1
2021-10-11 20:10:54,741 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:10:54,741 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:10:54,741 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember the way of the end of the walls of the clips in my house ."
2021-10-11 20:10:54,742 - INFO - joeynmt.training - Example #2
2021-10-11 20:10:54,742 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:10:54,742 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:10:54,742 - INFO - joeynmt.training - 	Hypothesis: My father heard the news BBP to my very large British to my map of a mobile phone .
2021-10-11 20:10:54,742 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step     5100: bleu:  11.18, loss: 480241.7500, ppl:  30.9488, duration: 30.0672s
2021-10-11 20:11:09,952 - INFO - joeynmt.training - Epoch   5: total training loss 77586.72
2021-10-11 20:11:09,952 - INFO - joeynmt.training - EPOCH 6
2021-10-11 20:11:46,749 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:11:46,750 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:11:46,750 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:11:46,791 - INFO - joeynmt.training - Example #0
2021-10-11 20:11:46,792 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:11:46,792 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:11:46,792 - INFO - joeynmt.training - 	Hypothesis: s
2021-10-11 20:11:46,792 - INFO - joeynmt.training - Example #1
2021-10-11 20:11:46,792 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:11:46,792 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:11:46,792 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the movie of the movie of the modern of my house ."
2021-10-11 20:11:46,792 - INFO - joeynmt.training - Example #2
2021-10-11 20:11:46,793 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:11:46,793 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:11:46,793 - INFO - joeynmt.training - 	Hypothesis: "My father listening to the news BBP , and I &apos;m going to take a little bit of a small psychologist ."
2021-10-11 20:11:46,793 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     5200: bleu:  11.14, loss: 479138.4375, ppl:  30.7057, duration: 29.2736s
2021-10-11 20:12:38,749 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:12:38,749 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:12:38,750 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:12:38,792 - INFO - joeynmt.training - Example #0
2021-10-11 20:12:38,792 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:12:38,792 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:12:38,792 - INFO - joeynmt.training - 	Hypothesis: es
2021-10-11 20:12:38,792 - INFO - joeynmt.training - Example #1
2021-10-11 20:12:38,793 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:12:38,793 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:12:38,793 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember , as I remember the day of the day of the day of my son ."
2021-10-11 20:12:38,793 - INFO - joeynmt.training - Example #2
2021-10-11 20:12:38,793 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:12:38,793 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:12:38,793 - INFO - joeynmt.training - 	Hypothesis: "My father listening to the news BBP , and I was a little bit of a small psychologist ."
2021-10-11 20:12:38,793 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     5300: bleu:  11.01, loss: 476615.4375, ppl:  30.1570, duration: 29.4176s
2021-10-11 20:13:33,372 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:13:33,373 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:13:33,373 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:13:33,415 - INFO - joeynmt.training - Example #0
2021-10-11 20:13:33,416 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:13:33,416 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:13:33,416 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:13:33,416 - INFO - joeynmt.training - Example #1
2021-10-11 20:13:33,416 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:13:33,416 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:13:33,417 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember , as I remember , the month of the house is the month of my house ."
2021-10-11 20:13:33,417 - INFO - joeynmt.training - Example #2
2021-10-11 20:13:33,417 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:13:33,417 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:13:33,417 - INFO - joeynmt.training - 	Hypothesis: "My father heard , I was listening to the news BBP in my own radical magnetic phenomenon ."
2021-10-11 20:13:33,417 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     5400: bleu:  10.67, loss: 473353.5312, ppl:  29.4620, duration: 32.0574s
2021-10-11 20:14:26,250 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:14:26,251 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:14:26,251 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:14:26,294 - INFO - joeynmt.training - Example #0
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:14:26,295 - INFO - joeynmt.training - Example #1
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember the end of the end of the day of the house of the house ."
2021-10-11 20:14:26,295 - INFO - joeynmt.training - Example #2
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:14:26,295 - INFO - joeynmt.training - 	Hypothesis: "My father heard , BBP in my own red radical psychologist ."
2021-10-11 20:14:26,296 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     5500: bleu:  11.11, loss: 472194.5625, ppl:  29.2190, duration: 29.9073s
2021-10-11 20:15:17,872 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:15:17,873 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:15:17,873 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:15:17,910 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:15:18,975 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/5100.ckpt
2021-10-11 20:15:19,050 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/5100.ckpt
2021-10-11 20:15:19,051 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/5100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/5100.ckpt')
2021-10-11 20:15:19,055 - INFO - joeynmt.training - Example #0
2021-10-11 20:15:19,055 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:15:19,055 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:15:19,055 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:15:19,055 - INFO - joeynmt.training - Example #1
2021-10-11 20:15:19,056 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:15:19,056 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:15:19,056 - INFO - joeynmt.training - 	Hypothesis: "When I was 15 years old , I remember the site of the morning of the door of the modern of the night ."
2021-10-11 20:15:19,056 - INFO - joeynmt.training - Example #2
2021-10-11 20:15:19,056 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:15:19,056 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:15:19,056 - INFO - joeynmt.training - 	Hypothesis: My father listening ) BBP in my own radio map .
2021-10-11 20:15:19,056 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     5600: bleu:  11.78, loss: 470018.9062, ppl:  28.7682, duration: 30.1599s
2021-10-11 20:16:09,235 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:16:09,236 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:16:09,236 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:16:09,273 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:16:10,193 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/5600.ckpt
2021-10-11 20:16:10,267 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/5600.ckpt
2021-10-11 20:16:10,268 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/5600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/5600.ckpt')
2021-10-11 20:16:10,272 - INFO - joeynmt.training - Example #0
2021-10-11 20:16:10,272 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:16:10,272 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:16:10,272 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:16:10,272 - INFO - joeynmt.training - Example #1
2021-10-11 20:16:10,272 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:16:10,272 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:16:10,272 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the day of the day of the day of the movie of the movie of my home ."
2021-10-11 20:16:10,273 - INFO - joeynmt.training - Example #2
2021-10-11 20:16:10,273 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:16:10,273 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:16:10,273 - INFO - joeynmt.training - 	Hypothesis: My father listen ) BF : Bwo in your own radio .
2021-10-11 20:16:10,273 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     5700: bleu:  12.16, loss: 470293.7500, ppl:  28.8247, duration: 28.5540s
2021-10-11 20:17:03,960 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:17:03,961 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:17:03,961 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:17:04,002 - INFO - joeynmt.training - Example #0
2021-10-11 20:17:04,003 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:17:04,003 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:17:04,003 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:17:04,003 - INFO - joeynmt.training - Example #1
2021-10-11 20:17:04,003 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:17:04,003 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:17:04,003 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember a law of the day of the house ."
2021-10-11 20:17:04,003 - INFO - joeynmt.training - Example #2
2021-10-11 20:17:04,004 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:17:04,004 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:17:04,004 - INFO - joeynmt.training - 	Hypothesis: My father listening ) BF in my own radio .
2021-10-11 20:17:04,004 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     5800: bleu:  12.02, loss: 469594.5625, ppl:  28.6811, duration: 31.0552s
2021-10-11 20:17:56,850 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:17:56,850 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:17:56,850 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:17:56,892 - INFO - joeynmt.training - Example #0
2021-10-11 20:17:56,893 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:17:56,893 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:17:56,893 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:17:56,893 - INFO - joeynmt.training - Example #1
2021-10-11 20:17:56,893 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:17:56,894 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:17:56,894 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the sounds of the day of the night of my father ."
2021-10-11 20:17:56,894 - INFO - joeynmt.training - Example #2
2021-10-11 20:17:56,894 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:17:56,894 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:17:56,894 - INFO - joeynmt.training - 	Hypothesis: "My father listen ) BF , BBP in my very large ."
2021-10-11 20:17:56,894 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     5900: bleu:  11.83, loss: 464722.2188, ppl:  27.6995, duration: 30.5010s
2021-10-11 20:18:20,577 - INFO - joeynmt.training - Epoch   6, Step:     6000, Batch Loss:    71.476746, Tokens per Sec:     8017, Lr: 0.000300
2021-10-11 20:18:51,667 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:18:51,667 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:18:51,667 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:18:51,710 - INFO - joeynmt.training - Example #0
2021-10-11 20:18:51,710 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:18:51,711 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:18:51,711 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:18:51,711 - INFO - joeynmt.training - Example #1
2021-10-11 20:18:51,711 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:18:51,711 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:18:51,711 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the site of the end of the site of the first time ."
2021-10-11 20:18:51,711 - INFO - joeynmt.training - Example #2
2021-10-11 20:18:51,711 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:18:51,711 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:18:51,711 - INFO - joeynmt.training - 	Hypothesis: My father listen ) BG : I &apos;m going to hear the news of the magic of the radio .
2021-10-11 20:18:51,711 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     6000: bleu:  11.61, loss: 461372.9375, ppl:  27.0443, duration: 31.1334s
2021-10-11 20:19:44,514 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:19:44,515 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:19:44,515 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:19:44,553 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:19:45,459 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/5700.ckpt
2021-10-11 20:19:45,532 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/5700.ckpt
2021-10-11 20:19:45,533 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/5700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/5700.ckpt')
2021-10-11 20:19:45,537 - INFO - joeynmt.training - Example #0
2021-10-11 20:19:45,537 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:19:45,537 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:19:45,537 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:19:45,537 - INFO - joeynmt.training - Example #1
2021-10-11 20:19:45,537 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:19:45,538 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:19:45,538 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the fourth of the door of the door of the office of my home ."
2021-10-11 20:19:45,538 - INFO - joeynmt.training - Example #2
2021-10-11 20:19:45,538 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:19:45,538 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:19:45,538 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news B.L. City in my own massive pung .
2021-10-11 20:19:45,538 - INFO - joeynmt.training - Validation result (greedy) at epoch   6, step     6100: bleu:  12.29, loss: 460617.7812, ppl:  26.8987, duration: 31.2789s
2021-10-11 20:20:07,895 - INFO - joeynmt.training - Epoch   6: total training loss 72959.21
2021-10-11 20:20:07,895 - INFO - joeynmt.training - EPOCH 7
2021-10-11 20:20:38,723 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:20:38,723 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:20:38,723 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:20:38,769 - INFO - joeynmt.training - Example #0
2021-10-11 20:20:38,769 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:20:38,770 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:20:38,770 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:20:38,770 - INFO - joeynmt.training - Example #1
2021-10-11 20:20:38,770 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:20:38,770 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:20:38,770 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember it was a fourth of the door of the door of the door ."
2021-10-11 20:20:38,770 - INFO - joeynmt.training - Example #2
2021-10-11 20:20:38,770 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:20:38,770 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:20:38,771 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBP in my own massive psychology .
2021-10-11 20:20:38,771 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     6200: bleu:  11.81, loss: 458957.1562, ppl:  26.5814, duration: 30.3019s
2021-10-11 20:21:33,550 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:21:33,551 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:21:33,551 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:21:33,593 - INFO - joeynmt.training - Example #0
2021-10-11 20:21:33,594 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:21:33,594 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:21:33,594 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:21:33,594 - INFO - joeynmt.training - Example #1
2021-10-11 20:21:33,594 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:21:33,594 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:21:33,594 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember , as a doctor , I remember , the morning of my office was born in my home ."
2021-10-11 20:21:33,595 - INFO - joeynmt.training - Example #2
2021-10-11 20:21:33,595 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:21:33,595 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:21:33,595 - INFO - joeynmt.training - 	Hypothesis: "My father listen listen to the news BBP , BBP in my own red radio ."
2021-10-11 20:21:33,595 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     6300: bleu:  11.40, loss: 458339.6250, ppl:  26.4643, duration: 32.0067s
2021-10-11 20:22:25,219 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:22:25,219 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:22:25,219 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:22:25,261 - INFO - joeynmt.training - Example #0
2021-10-11 20:22:25,261 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:22:25,261 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:22:25,261 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:22:25,261 - INFO - joeynmt.training - Example #1
2021-10-11 20:22:25,261 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:22:25,262 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:22:25,262 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the door of the door of the door of the door of my house ."
2021-10-11 20:22:25,262 - INFO - joeynmt.training - Example #2
2021-10-11 20:22:25,262 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:22:25,262 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:22:25,262 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBP in my own red superradio .
2021-10-11 20:22:25,262 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     6400: bleu:  12.26, loss: 456644.3750, ppl:  26.1456, duration: 29.1253s
2021-10-11 20:23:17,220 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:23:17,221 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:23:17,221 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:23:17,258 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:23:18,210 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/6100.ckpt
2021-10-11 20:23:18,281 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/6100.ckpt
2021-10-11 20:23:18,282 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/6100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/6100.ckpt')
2021-10-11 20:23:18,286 - INFO - joeynmt.training - Example #0
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:23:18,287 - INFO - joeynmt.training - Example #1
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the end of the end of the souse of the father of the office ."
2021-10-11 20:23:18,287 - INFO - joeynmt.training - Example #2
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:23:18,287 - INFO - joeynmt.training - 	Hypothesis: My father listening ) BBP in your small philosophy .
2021-10-11 20:23:18,287 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     6500: bleu:  13.04, loss: 456495.5625, ppl:  26.1178, duration: 30.6694s
2021-10-11 20:24:11,880 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:24:11,880 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:24:11,881 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:24:11,924 - INFO - joeynmt.training - Example #0
2021-10-11 20:24:11,924 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:24:11,924 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:24:11,924 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:24:11,925 - INFO - joeynmt.training - Example #1
2021-10-11 20:24:11,925 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:24:11,925 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:24:11,925 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years , I remember the same day to the house of the house ."
2021-10-11 20:24:11,925 - INFO - joeynmt.training - Example #2
2021-10-11 20:24:11,925 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:24:11,925 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:24:11,925 - INFO - joeynmt.training - 	Hypothesis: "My father listen to the news BBC , in my own red radio psychology ."
2021-10-11 20:24:11,925 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     6600: bleu:  12.79, loss: 454358.0938, ppl:  25.7218, duration: 31.1865s
2021-10-11 20:25:04,520 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:25:04,520 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:25:04,520 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:25:04,566 - INFO - joeynmt.training - Example #0
2021-10-11 20:25:04,566 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:25:04,566 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:25:04,566 - INFO - joeynmt.training - 	Hypothesis: es
2021-10-11 20:25:04,567 - INFO - joeynmt.training - Example #1
2021-10-11 20:25:04,567 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:25:04,567 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:25:04,567 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the same thing that I was looking for a bird from the house of the house ."
2021-10-11 20:25:04,567 - INFO - joeynmt.training - Example #2
2021-10-11 20:25:04,567 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:25:04,567 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:25:04,567 - INFO - joeynmt.training - 	Hypothesis: "My father listening to the news BBC , in my own little 3D , a radio pile ."
2021-10-11 20:25:04,568 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     6700: bleu:  12.45, loss: 451557.2188, ppl:  25.2121, duration: 29.9520s
2021-10-11 20:26:00,054 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:26:00,055 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:26:00,055 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:26:00,098 - INFO - joeynmt.training - Example #0
2021-10-11 20:26:00,099 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:26:00,099 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:26:00,099 - INFO - joeynmt.training - 	Hypothesis: es
2021-10-11 20:26:00,099 - INFO - joeynmt.training - Example #1
2021-10-11 20:26:00,099 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:26:00,099 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:26:00,099 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the way I was looking at the morning of the door of the door of the door of the home ."
2021-10-11 20:26:00,100 - INFO - joeynmt.training - Example #2
2021-10-11 20:26:00,100 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:26:00,100 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:26:00,100 - INFO - joeynmt.training - 	Hypothesis: "My grandmother listening to the news B.B. , and I &apos;m going to hear the news of the small 3D ."
2021-10-11 20:26:00,100 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     6800: bleu:  11.45, loss: 451308.7812, ppl:  25.1673, duration: 32.5174s
2021-10-11 20:26:53,509 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:26:53,509 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:26:53,509 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:26:53,554 - INFO - joeynmt.training - Example #0
2021-10-11 20:26:53,554 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:26:53,555 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:26:53,555 - INFO - joeynmt.training - 	Hypothesis: s
2021-10-11 20:26:53,555 - INFO - joeynmt.training - Example #1
2021-10-11 20:26:53,555 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:26:53,555 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:26:53,556 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the door of the morning of the door of the door of the movie ."
2021-10-11 20:26:53,556 - INFO - joeynmt.training - Example #2
2021-10-11 20:26:53,556 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:26:53,556 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:26:53,556 - INFO - joeynmt.training - 	Hypothesis: My father listen listen BBBT in your own red radio .
2021-10-11 20:26:53,557 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     6900: bleu:  12.32, loss: 449092.3438, ppl:  24.7718, duration: 30.9318s
2021-10-11 20:27:16,205 - INFO - joeynmt.training - Epoch   7, Step:     7000, Batch Loss:    70.352089, Tokens per Sec:     8033, Lr: 0.000300
2021-10-11 20:27:46,486 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:27:46,486 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:27:46,486 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:27:46,528 - INFO - joeynmt.training - Example #0
2021-10-11 20:27:46,529 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:27:46,529 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:27:46,529 - INFO - joeynmt.training - 	Hypothesis: s
2021-10-11 20:27:46,529 - INFO - joeynmt.training - Example #1
2021-10-11 20:27:46,529 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:27:46,529 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:27:46,530 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the morning of the trip of the house of the house of the house of my house ."
2021-10-11 20:27:46,530 - INFO - joeynmt.training - Example #2
2021-10-11 20:27:46,530 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:27:46,530 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:27:46,530 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBP on my own radio page .
2021-10-11 20:27:46,530 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     7000: bleu:  12.50, loss: 448352.2812, ppl:  24.6411, duration: 30.3242s
2021-10-11 20:28:37,547 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:28:37,547 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:28:37,547 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:28:37,584 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:28:38,731 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/6500.ckpt
2021-10-11 20:28:38,816 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/6500.ckpt
2021-10-11 20:28:38,816 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/6500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/6500.ckpt')
2021-10-11 20:28:38,822 - INFO - joeynmt.training - Example #0
2021-10-11 20:28:38,822 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:28:38,823 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:28:38,823 - INFO - joeynmt.training - 	Hypothesis: es
2021-10-11 20:28:38,823 - INFO - joeynmt.training - Example #1
2021-10-11 20:28:38,823 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:28:38,823 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:28:38,823 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember I was looking at the morning of the door of the home ."
2021-10-11 20:28:38,824 - INFO - joeynmt.training - Example #2
2021-10-11 20:28:38,824 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:28:38,824 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:28:38,824 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBP on my own radio wild .
2021-10-11 20:28:38,824 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     7100: bleu:  13.75, loss: 449226.8750, ppl:  24.7956, duration: 29.6128s
2021-10-11 20:29:31,297 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:29:31,297 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:29:31,297 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:29:31,338 - INFO - joeynmt.training - Example #0
2021-10-11 20:29:31,339 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:29:31,339 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:29:31,339 - INFO - joeynmt.training - 	Hypothesis: s .
2021-10-11 20:29:31,339 - INFO - joeynmt.training - Example #1
2021-10-11 20:29:31,339 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:29:31,339 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:29:31,339 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember , as a doctor , I remember a fourth of the monkey of my home ."
2021-10-11 20:29:31,340 - INFO - joeynmt.training - Example #2
2021-10-11 20:29:31,340 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:29:31,340 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:29:31,340 - INFO - joeynmt.training - 	Hypothesis: My father listen son BBC in my small photos .
2021-10-11 20:29:31,340 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step     7200: bleu:  12.86, loss: 445929.1875, ppl:  24.2180, duration: 29.9130s
2021-10-11 20:29:40,361 - INFO - joeynmt.training - Epoch   7: total training loss 69224.58
2021-10-11 20:29:40,361 - INFO - joeynmt.training - EPOCH 8
2021-10-11 20:30:26,978 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:30:26,979 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:30:26,979 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:30:27,024 - INFO - joeynmt.training - Example #0
2021-10-11 20:30:27,025 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:30:27,025 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:30:27,025 - INFO - joeynmt.training - 	Hypothesis: ers .
2021-10-11 20:30:27,025 - INFO - joeynmt.training - Example #1
2021-10-11 20:30:27,026 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:30:27,026 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:30:27,026 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the morning of the door of the house of the house ."
2021-10-11 20:30:27,026 - INFO - joeynmt.training - Example #2
2021-10-11 20:30:27,027 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:30:27,027 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:30:27,027 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBP in my own massive psychology .
2021-10-11 20:30:27,027 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     7300: bleu:  12.16, loss: 444200.3125, ppl:  23.9206, duration: 31.7373s
2021-10-11 20:31:19,680 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:31:19,681 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:31:19,681 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:31:19,726 - INFO - joeynmt.training - Example #0
2021-10-11 20:31:19,727 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:31:19,727 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:31:19,728 - INFO - joeynmt.training - 	Hypothesis: es
2021-10-11 20:31:19,728 - INFO - joeynmt.training - Example #1
2021-10-11 20:31:19,728 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:31:19,728 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:31:19,728 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the morning of the site of the movie of the audience ."
2021-10-11 20:31:19,729 - INFO - joeynmt.training - Example #2
2021-10-11 20:31:19,729 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:31:19,729 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:31:19,729 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBP in my own little radio magnetic .
2021-10-11 20:31:19,729 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     7400: bleu:  13.02, loss: 444229.0625, ppl:  23.9256, duration: 30.1470s
2021-10-11 20:32:12,346 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:32:12,347 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:32:12,347 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:32:12,390 - INFO - joeynmt.training - Example #0
2021-10-11 20:32:12,390 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:32:12,390 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:32:12,391 - INFO - joeynmt.training - 	Hypothesis: es
2021-10-11 20:32:12,391 - INFO - joeynmt.training - Example #1
2021-10-11 20:32:12,391 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:32:12,391 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:32:12,391 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember like a bird of the morning of the day of the day of the first time ."
2021-10-11 20:32:12,391 - INFO - joeynmt.training - Example #2
2021-10-11 20:32:12,391 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:32:12,391 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:32:12,391 - INFO - joeynmt.training - 	Hypothesis: My father listen listen BBC in my little bit of a small page .
2021-10-11 20:32:12,391 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     7500: bleu:  13.21, loss: 442586.0312, ppl:  23.6462, duration: 29.6370s
2021-10-11 20:33:05,434 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:33:05,435 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:33:05,435 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:33:05,477 - INFO - joeynmt.training - Example #0
2021-10-11 20:33:05,478 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:33:05,478 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:33:05,478 - INFO - joeynmt.training - 	Hypothesis: es
2021-10-11 20:33:05,478 - INFO - joeynmt.training - Example #1
2021-10-11 20:33:05,478 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:33:05,478 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:33:05,478 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember like a morning of a morning of a dog , I was going to be a kid of a door of a dad ."
2021-10-11 20:33:05,479 - INFO - joeynmt.training - Example #2
2021-10-11 20:33:05,479 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:33:05,479 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:33:05,479 - INFO - joeynmt.training - 	Hypothesis: "My father listen , BBC in my small , radio radio ."
2021-10-11 20:33:05,479 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     7600: bleu:  13.21, loss: 440287.0312, ppl:  23.2609, duration: 30.5979s
2021-10-11 20:33:58,875 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:33:58,876 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:33:58,876 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:33:58,917 - INFO - joeynmt.training - Example #0
2021-10-11 20:33:58,918 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:33:58,918 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:33:58,918 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:33:58,918 - INFO - joeynmt.training - Example #1
2021-10-11 20:33:58,918 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:33:58,918 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:33:58,918 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the end of the morning of the door of the door of the door of the home ."
2021-10-11 20:33:58,919 - INFO - joeynmt.training - Example #2
2021-10-11 20:33:58,919 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:33:58,919 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:33:58,919 - INFO - joeynmt.training - 	Hypothesis: "My father listen to the news BBC , in my own big , radical distribution ."
2021-10-11 20:33:58,919 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     7700: bleu:  12.96, loss: 440350.9375, ppl:  23.2715, duration: 30.7712s
2021-10-11 20:34:50,467 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:34:50,467 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:34:50,467 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:34:50,509 - INFO - joeynmt.training - Example #0
2021-10-11 20:34:50,509 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:34:50,509 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:34:50,510 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:34:50,510 - INFO - joeynmt.training - Example #1
2021-10-11 20:34:50,510 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:34:50,510 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:34:50,510 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the south of the dad of the office ."
2021-10-11 20:34:50,510 - INFO - joeynmt.training - Example #2
2021-10-11 20:34:50,510 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:34:50,510 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:34:50,510 - INFO - joeynmt.training - 	Hypothesis: My father listening to BBC to my neighborhood radio .
2021-10-11 20:34:50,511 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     7800: bleu:  13.69, loss: 440646.2812, ppl:  23.3207, duration: 28.8037s
2021-10-11 20:35:44,172 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:35:44,173 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:35:44,173 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:35:44,213 - INFO - joeynmt.training - Example #0
2021-10-11 20:35:44,214 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:35:44,214 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:35:44,214 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:35:44,214 - INFO - joeynmt.training - Example #1
2021-10-11 20:35:44,214 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:35:44,214 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:35:44,215 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the way I was walking up with the end of the day of the wills of the audience ."
2021-10-11 20:35:44,215 - INFO - joeynmt.training - Example #2
2021-10-11 20:35:44,215 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:35:44,215 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:35:44,215 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBP in my own radio wave .
2021-10-11 20:35:44,215 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     7900: bleu:  13.42, loss: 436933.0312, ppl:  22.7099, duration: 31.1561s
2021-10-11 20:36:07,837 - INFO - joeynmt.training - Epoch   8, Step:     8000, Batch Loss:    62.419628, Tokens per Sec:     8017, Lr: 0.000300
2021-10-11 20:36:38,038 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:36:38,038 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:36:38,038 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:36:38,086 - INFO - joeynmt.training - Example #0
2021-10-11 20:36:38,086 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:36:38,086 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:36:38,086 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:36:38,087 - INFO - joeynmt.training - Example #1
2021-10-11 20:36:38,087 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:36:38,087 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:36:38,087 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember like a fourth of the day of the door of the first time to the house ."
2021-10-11 20:36:38,087 - INFO - joeynmt.training - Example #2
2021-10-11 20:36:38,087 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:36:38,088 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:36:38,088 - INFO - joeynmt.training - 	Hypothesis: My father listen listen BBC to my neighborhood radio .
2021-10-11 20:36:38,088 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     8000: bleu:  13.16, loss: 436394.9062, ppl:  22.6227, duration: 30.2496s
2021-10-11 20:37:30,329 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:37:30,330 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:37:30,330 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:37:30,373 - INFO - joeynmt.training - Example #0
2021-10-11 20:37:30,374 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:37:30,374 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:37:30,374 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-11 20:37:30,374 - INFO - joeynmt.training - Example #1
2021-10-11 20:37:30,374 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:37:30,374 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:37:30,374 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the morning of the father of the office of the house ."
2021-10-11 20:37:30,374 - INFO - joeynmt.training - Example #2
2021-10-11 20:37:30,374 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:37:30,374 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:37:30,375 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBP in my own radio radio .
2021-10-11 20:37:30,375 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     8100: bleu:  13.58, loss: 436367.2812, ppl:  22.6183, duration: 29.7019s
2021-10-11 20:38:23,910 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:38:23,910 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:38:23,911 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:38:23,954 - INFO - joeynmt.training - Example #0
2021-10-11 20:38:23,954 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:38:23,955 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:38:23,955 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:38:23,955 - INFO - joeynmt.training - Example #1
2021-10-11 20:38:23,955 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:38:23,955 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:38:23,955 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember like a morning of a morning of the father of the father of the father of the father of the father of the father of the father ."
2021-10-11 20:38:23,955 - INFO - joeynmt.training - Example #2
2021-10-11 20:38:23,955 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:38:23,955 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:38:23,955 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBP to his small superradio poor .
2021-10-11 20:38:23,956 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step     8200: bleu:  12.75, loss: 435711.6875, ppl:  22.5125, duration: 31.0945s
2021-10-11 20:38:38,812 - INFO - joeynmt.training - Epoch   8: total training loss 66156.63
2021-10-11 20:38:38,813 - INFO - joeynmt.training - EPOCH 9
2021-10-11 20:39:16,033 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:39:16,033 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:39:16,033 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:39:16,071 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:39:16,975 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/7100.ckpt
2021-10-11 20:39:17,048 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/7100.ckpt
2021-10-11 20:39:17,048 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/7100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/7100.ckpt')
2021-10-11 20:39:17,053 - INFO - joeynmt.training - Example #0
2021-10-11 20:39:17,053 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:39:17,054 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:39:17,054 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:39:17,054 - INFO - joeynmt.training - Example #1
2021-10-11 20:39:17,054 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:39:17,054 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:39:17,054 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember that the morning of the father of the father of the father was going to be the first time ."
2021-10-11 20:39:17,054 - INFO - joeynmt.training - Example #2
2021-10-11 20:39:17,054 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:39:17,054 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:39:17,054 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small superradio .
2021-10-11 20:39:17,054 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     8300: bleu:  13.99, loss: 430231.6562, ppl:  21.6479, duration: 30.5213s
2021-10-11 20:40:09,046 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:40:09,046 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:40:09,046 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:40:09,089 - INFO - joeynmt.training - Example #0
2021-10-11 20:40:09,089 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:40:09,089 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:40:09,089 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:40:09,089 - INFO - joeynmt.training - Example #1
2021-10-11 20:40:09,090 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:40:09,090 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:40:09,090 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember that I was looking at the morning of the door of the background ."
2021-10-11 20:40:09,090 - INFO - joeynmt.training - Example #2
2021-10-11 20:40:09,090 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:40:09,090 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:40:09,090 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news B.B. on my own radio magnet .
2021-10-11 20:40:09,090 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     8400: bleu:  13.86, loss: 429385.8125, ppl:  21.5174, duration: 29.4928s
2021-10-11 20:41:01,426 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:41:01,426 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:41:01,426 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:41:01,472 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:41:02,374 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/8300.ckpt
2021-10-11 20:41:02,445 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/8300.ckpt
2021-10-11 20:41:02,445 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/8300.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/8300.ckpt')
2021-10-11 20:41:02,449 - INFO - joeynmt.training - Example #0
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:41:02,450 - INFO - joeynmt.training - Example #1
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the way that the morning was going to be the father of the office in my house ."
2021-10-11 20:41:02,450 - INFO - joeynmt.training - Example #2
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:41:02,450 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC in my small superradio .
2021-10-11 20:41:02,450 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     8500: bleu:  14.04, loss: 428736.7812, ppl:  21.4178, duration: 30.7686s
2021-10-11 20:41:54,253 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:41:54,254 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:41:54,254 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:41:54,291 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:41:55,212 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/8500.ckpt
2021-10-11 20:41:55,284 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/8500.ckpt
2021-10-11 20:41:55,284 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/8500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/8500.ckpt')
2021-10-11 20:41:55,289 - INFO - joeynmt.training - Example #0
2021-10-11 20:41:55,289 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:41:55,289 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:41:55,289 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:41:55,289 - INFO - joeynmt.training - Example #1
2021-10-11 20:41:55,289 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:41:55,289 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:41:55,289 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the day of the day of the day of the office of the office ."
2021-10-11 20:41:55,289 - INFO - joeynmt.training - Example #2
2021-10-11 20:41:55,289 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:41:55,290 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:41:55,290 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 20:41:55,290 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     8600: bleu:  14.26, loss: 427602.3438, ppl:  21.2448, duration: 30.1554s
2021-10-11 20:42:47,584 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:42:47,584 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:42:47,584 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:42:47,622 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:42:48,510 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/8600.ckpt
2021-10-11 20:42:48,579 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/8600.ckpt
2021-10-11 20:42:48,580 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/8600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/8600.ckpt')
2021-10-11 20:42:48,584 - INFO - joeynmt.training - Example #0
2021-10-11 20:42:48,584 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:42:48,585 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:42:48,585 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:42:48,585 - INFO - joeynmt.training - Example #1
2021-10-11 20:42:48,585 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:42:48,585 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:42:48,585 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember how to the morning of the father of the father of the father of the father of the home ."
2021-10-11 20:42:48,585 - INFO - joeynmt.training - Example #2
2021-10-11 20:42:48,585 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:42:48,585 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:42:48,585 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC in my own radio policy .
2021-10-11 20:42:48,585 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     8700: bleu:  14.84, loss: 427294.0000, ppl:  21.1981, duration: 30.7039s
2021-10-11 20:43:39,587 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:43:39,587 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:43:39,587 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:43:39,632 - INFO - joeynmt.training - Example #0
2021-10-11 20:43:39,633 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:43:39,633 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:43:39,633 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:43:39,633 - INFO - joeynmt.training - Example #1
2021-10-11 20:43:39,633 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:43:39,633 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:43:39,633 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the way I was going to the end of the day of the door of the father of the house ."
2021-10-11 20:43:39,633 - INFO - joeynmt.training - Example #2
2021-10-11 20:43:39,634 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:43:39,634 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:43:39,634 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small page of radio radio .
2021-10-11 20:43:39,634 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     8800: bleu:  14.42, loss: 426801.0000, ppl:  21.1235, duration: 28.5000s
2021-10-11 20:44:30,840 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:44:30,840 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:44:30,840 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:44:30,884 - INFO - joeynmt.training - Example #0
2021-10-11 20:44:30,885 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:44:30,885 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:44:30,885 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:44:30,885 - INFO - joeynmt.training - Example #1
2021-10-11 20:44:30,885 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:44:30,885 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:44:30,885 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember like a morning of a morning of the first time I was in my house ."
2021-10-11 20:44:30,885 - INFO - joeynmt.training - Example #2
2021-10-11 20:44:30,886 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:44:30,886 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:44:30,886 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio phosphorus .
2021-10-11 20:44:30,886 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     8900: bleu:  14.36, loss: 426839.2812, ppl:  21.1293, duration: 28.8216s
2021-10-11 20:44:53,424 - INFO - joeynmt.training - Epoch   9, Step:     9000, Batch Loss:    67.948219, Tokens per Sec:     8081, Lr: 0.000150
2021-10-11 20:45:23,355 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:45:23,355 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:45:23,355 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:45:23,399 - INFO - joeynmt.training - Example #0
2021-10-11 20:45:23,399 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:45:23,399 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:45:23,399 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:45:23,399 - INFO - joeynmt.training - Example #1
2021-10-11 20:45:23,400 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:45:23,400 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:45:23,400 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 20:45:23,400 - INFO - joeynmt.training - Example #2
2021-10-11 20:45:23,400 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:45:23,400 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:45:23,400 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my neighborhood radio .
2021-10-11 20:45:23,400 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     9000: bleu:  13.68, loss: 425955.2188, ppl:  20.9962, duration: 29.9753s
2021-10-11 20:46:14,643 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:46:14,643 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:46:14,643 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:46:14,687 - INFO - joeynmt.training - Example #0
2021-10-11 20:46:14,688 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:46:14,688 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:46:14,688 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:46:14,688 - INFO - joeynmt.training - Example #1
2021-10-11 20:46:14,688 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:46:14,688 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:46:14,688 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the morning of the father of the office of the house ."
2021-10-11 20:46:14,688 - INFO - joeynmt.training - Example #2
2021-10-11 20:46:14,689 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:46:14,689 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:46:14,689 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC in my small radio .
2021-10-11 20:46:14,689 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     9100: bleu:  14.71, loss: 425272.2188, ppl:  20.8940, duration: 28.5429s
2021-10-11 20:47:05,935 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:47:05,935 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:47:05,935 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:47:05,979 - INFO - joeynmt.training - Example #0
2021-10-11 20:47:05,979 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:47:05,979 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:47:05,980 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:47:05,980 - INFO - joeynmt.training - Example #1
2021-10-11 20:47:05,980 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:47:05,980 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:47:05,980 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 20:47:05,980 - INFO - joeynmt.training - Example #2
2021-10-11 20:47:05,980 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:47:05,980 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:47:05,980 - INFO - joeynmt.training - 	Hypothesis: My father listen son to BBC to my small radio pocket .
2021-10-11 20:47:05,980 - INFO - joeynmt.training - Validation result (greedy) at epoch   9, step     9200: bleu:  14.68, loss: 424649.9062, ppl:  20.8012, duration: 28.7112s
2021-10-11 20:47:27,705 - INFO - joeynmt.training - Epoch   9: total training loss 62441.26
2021-10-11 20:47:27,705 - INFO - joeynmt.training - EPOCH 10
2021-10-11 20:47:58,268 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:47:58,268 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:47:58,268 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:47:58,311 - INFO - joeynmt.training - Example #0
2021-10-11 20:47:58,311 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:47:58,311 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:47:58,311 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:47:58,311 - INFO - joeynmt.training - Example #1
2021-10-11 20:47:58,312 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:47:58,312 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:47:58,312 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father , I remember a little bit of the door of the office of the office ."
2021-10-11 20:47:58,312 - INFO - joeynmt.training - Example #2
2021-10-11 20:47:58,312 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:47:58,312 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:47:58,312 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my neighborhood radio .
2021-10-11 20:47:58,312 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     9300: bleu:  13.89, loss: 423432.4688, ppl:  20.6210, duration: 29.8300s
2021-10-11 20:48:49,951 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:48:49,952 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:48:49,952 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:48:49,995 - INFO - joeynmt.training - Example #0
2021-10-11 20:48:49,995 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:48:49,995 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:48:49,995 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:48:49,995 - INFO - joeynmt.training - Example #1
2021-10-11 20:48:49,996 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:48:49,996 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:48:49,996 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the way that the morning was going to the end of the door of the office in my house ."
2021-10-11 20:48:49,996 - INFO - joeynmt.training - Example #2
2021-10-11 20:48:49,996 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:48:49,996 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:48:49,996 - INFO - joeynmt.training - 	Hypothesis: My father listen to BBC to my neighborhood radio .
2021-10-11 20:48:49,996 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     9400: bleu:  14.43, loss: 424035.5625, ppl:  20.7101, duration: 29.0844s
2021-10-11 20:49:42,003 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:49:42,003 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:49:42,003 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:49:42,045 - INFO - joeynmt.training - Example #0
2021-10-11 20:49:42,045 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:49:42,045 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:49:42,045 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:49:42,045 - INFO - joeynmt.training - Example #1
2021-10-11 20:49:42,045 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:49:42,046 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:49:42,046 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s first representation of the house ."
2021-10-11 20:49:42,046 - INFO - joeynmt.training - Example #2
2021-10-11 20:49:42,046 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:49:42,046 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:49:42,046 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my neighborhood radio .
2021-10-11 20:49:42,046 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     9500: bleu:  14.42, loss: 423598.8125, ppl:  20.6456, duration: 29.4892s
2021-10-11 20:50:34,793 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:50:34,793 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:50:34,794 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:50:34,845 - INFO - joeynmt.training - Example #0
2021-10-11 20:50:34,845 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:50:34,845 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:50:34,845 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:50:34,845 - INFO - joeynmt.training - Example #1
2021-10-11 20:50:34,845 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:50:34,845 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:50:34,846 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s daughter in my house ."
2021-10-11 20:50:34,846 - INFO - joeynmt.training - Example #2
2021-10-11 20:50:34,846 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:50:34,846 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:50:34,846 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small page of radio policy .
2021-10-11 20:50:34,846 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     9600: bleu:  14.45, loss: 424807.2500, ppl:  20.8247, duration: 28.5656s
2021-10-11 20:51:27,444 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:51:27,445 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:51:27,445 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:51:27,490 - INFO - joeynmt.training - Example #0
2021-10-11 20:51:27,490 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:51:27,491 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:51:27,491 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:51:27,491 - INFO - joeynmt.training - Example #1
2021-10-11 20:51:27,491 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:51:27,491 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:51:27,491 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember , as a doctor , the morning of the father of the father of the office , the dad of the audience ."
2021-10-11 20:51:27,491 - INFO - joeynmt.training - Example #2
2021-10-11 20:51:27,491 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:51:27,492 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:51:27,492 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 20:51:27,492 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     9700: bleu:  14.47, loss: 423906.8125, ppl:  20.6911, duration: 29.9978s
2021-10-11 20:52:18,399 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:52:18,400 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:52:18,400 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:52:18,445 - INFO - joeynmt.training - Example #0
2021-10-11 20:52:18,445 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:52:18,445 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:52:18,446 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:52:18,446 - INFO - joeynmt.training - Example #1
2021-10-11 20:52:18,446 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:52:18,446 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:52:18,446 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s first invent in my house ."
2021-10-11 20:52:18,446 - INFO - joeynmt.training - Example #2
2021-10-11 20:52:18,446 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:52:18,446 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:52:18,446 - INFO - joeynmt.training - 	Hypothesis: My father listening to BBC to my small radio wave .
2021-10-11 20:52:18,447 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     9800: bleu:  14.80, loss: 423154.9688, ppl:  20.5802, duration: 28.3369s
2021-10-11 20:53:09,425 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:53:09,426 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:53:09,426 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:53:09,465 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:53:10,368 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/8700.ckpt
2021-10-11 20:53:10,439 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/8700.ckpt
2021-10-11 20:53:10,439 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/8700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/8700.ckpt')
2021-10-11 20:53:10,444 - INFO - joeynmt.training - Example #0
2021-10-11 20:53:10,444 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:53:10,444 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:53:10,444 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:53:10,445 - INFO - joeynmt.training - Example #1
2021-10-11 20:53:10,445 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:53:10,445 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:53:10,445 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s first invention in my house ."
2021-10-11 20:53:10,445 - INFO - joeynmt.training - Example #2
2021-10-11 20:53:10,445 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:53:10,445 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:53:10,445 - INFO - joeynmt.training - 	Hypothesis: My father listen to BBC to my small radio wave .
2021-10-11 20:53:10,445 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step     9900: bleu:  14.90, loss: 420527.7500, ppl:  20.1974, duration: 29.3796s
2021-10-11 20:53:33,119 - INFO - joeynmt.training - Epoch  10, Step:    10000, Batch Loss:    57.682526, Tokens per Sec:     7963, Lr: 0.000075
2021-10-11 20:54:03,061 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:54:03,062 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:54:03,062 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:54:03,106 - INFO - joeynmt.training - Example #0
2021-10-11 20:54:03,106 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:54:03,106 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:54:03,106 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:54:03,106 - INFO - joeynmt.training - Example #1
2021-10-11 20:54:03,107 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:54:03,107 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:54:03,107 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s first invention in my house ."
2021-10-11 20:54:03,107 - INFO - joeynmt.training - Example #2
2021-10-11 20:54:03,107 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:54:03,107 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:54:03,107 - INFO - joeynmt.training - 	Hypothesis: My father listen to BBC to my small radio policy .
2021-10-11 20:54:03,107 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    10000: bleu:  14.85, loss: 419301.6562, ppl:  20.0211, duration: 29.9877s
2021-10-11 20:54:55,467 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:54:55,467 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:54:55,467 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:54:55,510 - INFO - joeynmt.training - Example #0
2021-10-11 20:54:55,510 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:54:55,510 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:54:55,510 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:54:55,511 - INFO - joeynmt.training - Example #1
2021-10-11 20:54:55,511 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:54:55,511 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:54:55,511 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s first representation of the house ."
2021-10-11 20:54:55,511 - INFO - joeynmt.training - Example #2
2021-10-11 20:54:55,511 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:54:55,511 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:54:55,511 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 20:54:55,511 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    10100: bleu:  14.87, loss: 419096.7188, ppl:  19.9918, duration: 29.6452s
2021-10-11 20:55:47,732 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:55:47,733 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:55:47,733 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:55:47,775 - INFO - joeynmt.training - Example #0
2021-10-11 20:55:47,775 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:55:47,775 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:55:47,775 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:55:47,775 - INFO - joeynmt.training - Example #1
2021-10-11 20:55:47,776 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:55:47,776 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:55:47,776 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the morning of the monkey of the office ."
2021-10-11 20:55:47,776 - INFO - joeynmt.training - Example #2
2021-10-11 20:55:47,776 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:55:47,776 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:55:47,776 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small page .
2021-10-11 20:55:47,776 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    10200: bleu:  14.72, loss: 418699.0000, ppl:  19.9351, duration: 29.5637s
2021-10-11 20:56:39,850 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:56:39,850 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:56:39,851 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:56:39,893 - INFO - joeynmt.training - Example #0
2021-10-11 20:56:39,893 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:56:39,893 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:56:39,894 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:56:39,894 - INFO - joeynmt.training - Example #1
2021-10-11 20:56:39,894 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:56:39,894 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:56:39,894 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 20:56:39,894 - INFO - joeynmt.training - Example #2
2021-10-11 20:56:39,894 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:56:39,894 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:56:39,894 - INFO - joeynmt.training - 	Hypothesis: My father listen to BBC to my small radio policy .
2021-10-11 20:56:39,895 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step    10300: bleu:  14.82, loss: 418414.8125, ppl:  19.8946, duration: 29.3841s
2021-10-11 20:56:47,608 - INFO - joeynmt.training - Epoch  10: total training loss 60693.95
2021-10-11 20:56:47,608 - INFO - joeynmt.training - EPOCH 11
2021-10-11 20:57:32,888 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:57:32,888 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:57:32,888 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:57:32,930 - INFO - joeynmt.training - Example #0
2021-10-11 20:57:32,931 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:57:32,931 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:57:32,931 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:57:32,931 - INFO - joeynmt.training - Example #1
2021-10-11 20:57:32,931 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:57:32,931 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:57:32,931 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s going to the end of the door of the office of the office ."
2021-10-11 20:57:32,931 - INFO - joeynmt.training - Example #2
2021-10-11 20:57:32,932 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:57:32,932 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:57:32,932 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 20:57:32,932 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    10400: bleu:  14.70, loss: 419162.7188, ppl:  20.0013, duration: 30.2450s
2021-10-11 20:58:25,951 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:58:25,952 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:58:25,952 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:58:25,992 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 20:58:26,932 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/9900.ckpt
2021-10-11 20:58:27,002 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/9900.ckpt
2021-10-11 20:58:27,002 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/9900.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/9900.ckpt')
2021-10-11 20:58:27,007 - INFO - joeynmt.training - Example #0
2021-10-11 20:58:27,007 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:58:27,007 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:58:27,007 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:58:27,007 - INFO - joeynmt.training - Example #1
2021-10-11 20:58:27,007 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:58:27,007 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:58:27,007 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the office of the house ."
2021-10-11 20:58:27,007 - INFO - joeynmt.training - Example #2
2021-10-11 20:58:27,007 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:58:27,008 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:58:27,008 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 20:58:27,008 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    10500: bleu:  15.01, loss: 419204.2812, ppl:  20.0072, duration: 29.7273s
2021-10-11 20:59:19,422 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 20:59:19,422 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 20:59:19,422 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 20:59:19,468 - INFO - joeynmt.training - Example #0
2021-10-11 20:59:19,468 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 20:59:19,469 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 20:59:19,469 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 20:59:19,469 - INFO - joeynmt.training - Example #1
2021-10-11 20:59:19,469 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 20:59:19,469 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 20:59:19,469 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office of the office ."
2021-10-11 20:59:19,469 - INFO - joeynmt.training - Example #2
2021-10-11 20:59:19,469 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 20:59:19,469 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 20:59:19,470 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my neighborhood radio .
2021-10-11 20:59:19,470 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    10600: bleu:  14.55, loss: 418822.4062, ppl:  19.9527, duration: 29.8711s
2021-10-11 21:00:11,063 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:00:11,063 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:00:11,064 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:00:11,103 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:00:12,019 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/10500.ckpt
2021-10-11 21:00:12,089 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/10500.ckpt
2021-10-11 21:00:12,090 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/10500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/10500.ckpt')
2021-10-11 21:00:12,094 - INFO - joeynmt.training - Example #0
2021-10-11 21:00:12,094 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:00:12,095 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:00:12,095 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:00:12,095 - INFO - joeynmt.training - Example #1
2021-10-11 21:00:12,095 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:00:12,095 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:00:12,095 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the door of the door of the office of the house ."
2021-10-11 21:00:12,095 - INFO - joeynmt.training - Example #2
2021-10-11 21:00:12,095 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:00:12,095 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:00:12,095 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:00:12,095 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    10700: bleu:  15.15, loss: 418972.7812, ppl:  19.9741, duration: 30.2686s
2021-10-11 21:01:03,948 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:01:03,948 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:01:03,948 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:01:03,997 - INFO - joeynmt.training - Example #0
2021-10-11 21:01:03,998 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:01:03,998 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:01:03,998 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:01:03,998 - INFO - joeynmt.training - Example #1
2021-10-11 21:01:03,998 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:01:03,998 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:01:03,998 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s first representation of my house ."
2021-10-11 21:01:03,998 - INFO - joeynmt.training - Example #2
2021-10-11 21:01:03,999 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:01:03,999 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:01:03,999 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:01:03,999 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    10800: bleu:  14.95, loss: 417891.3438, ppl:  19.8203, duration: 29.2917s
2021-10-11 21:01:55,842 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:01:55,843 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:01:55,843 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:01:55,888 - INFO - joeynmt.training - Example #0
2021-10-11 21:01:55,888 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:01:55,888 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:01:55,889 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:01:55,889 - INFO - joeynmt.training - Example #1
2021-10-11 21:01:55,889 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:01:55,889 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:01:55,889 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the revenue of the office of the office ."
2021-10-11 21:01:55,889 - INFO - joeynmt.training - Example #2
2021-10-11 21:01:55,889 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:01:55,889 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:01:55,889 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:01:55,889 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    10900: bleu:  14.78, loss: 418089.5000, ppl:  19.8484, duration: 29.5389s
2021-10-11 21:02:18,941 - INFO - joeynmt.training - Epoch  11, Step:    11000, Batch Loss:    60.486752, Tokens per Sec:     8001, Lr: 0.000075
2021-10-11 21:02:47,976 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:02:47,977 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:02:47,977 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:02:48,020 - INFO - joeynmt.training - Example #0
2021-10-11 21:02:48,020 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:02:48,020 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:02:48,020 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:02:48,020 - INFO - joeynmt.training - Example #1
2021-10-11 21:02:48,021 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:02:48,021 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:02:48,021 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the background ."
2021-10-11 21:02:48,021 - INFO - joeynmt.training - Example #2
2021-10-11 21:02:48,021 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:02:48,021 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:02:48,021 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC in my small radio .
2021-10-11 21:02:48,021 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    11000: bleu:  14.88, loss: 417650.0312, ppl:  19.7862, duration: 29.0799s
2021-10-11 21:03:39,622 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:03:39,623 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:03:39,623 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:03:39,666 - INFO - joeynmt.training - Example #0
2021-10-11 21:03:39,666 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:03:39,666 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:03:39,666 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:03:39,666 - INFO - joeynmt.training - Example #1
2021-10-11 21:03:39,666 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:03:39,667 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:03:39,667 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the door of the door of the office of the house ."
2021-10-11 21:03:39,667 - INFO - joeynmt.training - Example #2
2021-10-11 21:03:39,667 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:03:39,667 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:03:39,667 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:03:39,667 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    11100: bleu:  14.89, loss: 417611.7500, ppl:  19.7808, duration: 28.9805s
2021-10-11 21:04:30,213 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:04:30,213 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:04:30,213 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:04:30,251 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:04:31,377 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/10700.ckpt
2021-10-11 21:04:31,448 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/10700.ckpt
2021-10-11 21:04:31,448 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/10700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/10700.ckpt')
2021-10-11 21:04:31,453 - INFO - joeynmt.training - Example #0
2021-10-11 21:04:31,453 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:04:31,453 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:04:31,453 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:04:31,453 - INFO - joeynmt.training - Example #1
2021-10-11 21:04:31,453 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:04:31,453 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:04:31,453 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the door of the office of the home ."
2021-10-11 21:04:31,453 - INFO - joeynmt.training - Example #2
2021-10-11 21:04:31,453 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:04:31,454 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:04:31,454 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC in my small radio .
2021-10-11 21:04:31,454 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    11200: bleu:  15.20, loss: 417845.6562, ppl:  19.8139, duration: 29.1665s
2021-10-11 21:05:23,101 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:05:23,101 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:05:23,101 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:05:23,142 - INFO - joeynmt.training - Example #0
2021-10-11 21:05:23,142 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:05:23,142 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:05:23,142 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:05:23,142 - INFO - joeynmt.training - Example #1
2021-10-11 21:05:23,143 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:05:23,143 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:05:23,143 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the office of the audience ."
2021-10-11 21:05:23,143 - INFO - joeynmt.training - Example #2
2021-10-11 21:05:23,143 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:05:23,143 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:05:23,143 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio .
2021-10-11 21:05:23,143 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step    11300: bleu:  15.08, loss: 416694.9062, ppl:  19.6516, duration: 29.1149s
2021-10-11 21:05:37,578 - INFO - joeynmt.training - Epoch  11: total training loss 59231.83
2021-10-11 21:05:37,579 - INFO - joeynmt.training - EPOCH 12
2021-10-11 21:06:15,441 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:06:15,442 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:06:15,442 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:06:15,484 - INFO - joeynmt.training - Example #0
2021-10-11 21:06:15,484 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:06:15,484 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:06:15,484 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:06:15,484 - INFO - joeynmt.training - Example #1
2021-10-11 21:06:15,485 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:06:15,485 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:06:15,485 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 21:06:15,485 - INFO - joeynmt.training - Example #2
2021-10-11 21:06:15,485 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:06:15,485 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:06:15,485 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC in my neighborhood radio .
2021-10-11 21:06:15,485 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    11400: bleu:  14.96, loss: 417645.8125, ppl:  19.7856, duration: 29.8947s
2021-10-11 21:07:06,409 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:07:06,410 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:07:06,410 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:07:06,461 - INFO - joeynmt.training - Example #0
2021-10-11 21:07:06,461 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:07:06,461 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:07:06,461 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:07:06,462 - INFO - joeynmt.training - Example #1
2021-10-11 21:07:06,462 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:07:06,462 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:07:06,462 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s first reported to my house ."
2021-10-11 21:07:06,462 - INFO - joeynmt.training - Example #2
2021-10-11 21:07:06,462 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:07:06,462 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:07:06,462 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio pocket .
2021-10-11 21:07:06,462 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    11500: bleu:  15.00, loss: 417519.1250, ppl:  19.7677, duration: 28.3841s
2021-10-11 21:07:58,128 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:07:58,129 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:07:58,129 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:07:58,172 - INFO - joeynmt.training - Example #0
2021-10-11 21:07:58,173 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:07:58,173 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:07:58,173 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:07:58,173 - INFO - joeynmt.training - Example #1
2021-10-11 21:07:58,173 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:07:58,173 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:07:58,174 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember that I was going to take a dog from a dog of office in my house ."
2021-10-11 21:07:58,174 - INFO - joeynmt.training - Example #2
2021-10-11 21:07:58,174 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:07:58,174 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:07:58,174 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio pocket .
2021-10-11 21:07:58,174 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    11600: bleu:  14.73, loss: 417051.4375, ppl:  19.7017, duration: 29.1742s
2021-10-11 21:08:49,527 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:08:49,527 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:08:49,527 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:08:49,571 - INFO - joeynmt.training - Example #0
2021-10-11 21:08:49,571 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:08:49,571 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:08:49,572 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:08:49,572 - INFO - joeynmt.training - Example #1
2021-10-11 21:08:49,572 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:08:49,572 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:08:49,572 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s office in my house ."
2021-10-11 21:08:49,572 - INFO - joeynmt.training - Example #2
2021-10-11 21:08:49,572 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:08:49,572 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:08:49,572 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC in my own radio radio .
2021-10-11 21:08:49,572 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    11700: bleu:  14.88, loss: 417279.3125, ppl:  19.7338, duration: 28.7684s
2021-10-11 21:09:42,012 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:09:42,013 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:09:42,013 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:09:42,056 - INFO - joeynmt.training - Example #0
2021-10-11 21:09:42,056 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:09:42,056 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:09:42,056 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:09:42,056 - INFO - joeynmt.training - Example #1
2021-10-11 21:09:42,057 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:09:42,057 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:09:42,057 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the door of the office of the house ."
2021-10-11 21:09:42,057 - INFO - joeynmt.training - Example #2
2021-10-11 21:09:42,057 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:09:42,057 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:09:42,057 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:09:42,057 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    11800: bleu:  14.72, loss: 416326.8125, ppl:  19.6000, duration: 29.8800s
2021-10-11 21:10:34,387 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:10:34,387 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:10:34,387 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:10:34,432 - INFO - joeynmt.training - Example #0
2021-10-11 21:10:34,432 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:10:34,432 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:10:34,432 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:10:34,432 - INFO - joeynmt.training - Example #1
2021-10-11 21:10:34,432 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:10:34,433 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:10:34,433 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s report of the background ."
2021-10-11 21:10:34,433 - INFO - joeynmt.training - Example #2
2021-10-11 21:10:34,433 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:10:34,433 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:10:34,433 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:10:34,433 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    11900: bleu:  14.94, loss: 416436.8125, ppl:  19.6154, duration: 29.9068s
2021-10-11 21:10:57,281 - INFO - joeynmt.training - Epoch  12, Step:    12000, Batch Loss:    53.138050, Tokens per Sec:     8049, Lr: 0.000075
2021-10-11 21:11:26,849 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:11:26,849 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:11:26,849 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:11:26,892 - INFO - joeynmt.training - Example #0
2021-10-11 21:11:26,892 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:11:26,892 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:11:26,892 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:11:26,892 - INFO - joeynmt.training - Example #1
2021-10-11 21:11:26,893 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:11:26,893 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:11:26,893 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the office of the office of the house ."
2021-10-11 21:11:26,893 - INFO - joeynmt.training - Example #2
2021-10-11 21:11:26,893 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:11:26,893 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:11:26,893 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:11:26,893 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    12000: bleu:  15.18, loss: 417024.9375, ppl:  19.6980, duration: 29.6117s
2021-10-11 21:12:18,786 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:12:18,786 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:12:18,786 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:12:18,829 - INFO - joeynmt.training - Example #0
2021-10-11 21:12:18,829 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:12:18,829 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:12:18,829 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:12:18,829 - INFO - joeynmt.training - Example #1
2021-10-11 21:12:18,829 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:12:18,830 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:12:18,830 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 21:12:18,830 - INFO - joeynmt.training - Example #2
2021-10-11 21:12:18,830 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:12:18,830 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:12:18,830 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my neighborhood radio .
2021-10-11 21:12:18,830 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    12100: bleu:  15.10, loss: 416123.7812, ppl:  19.5715, duration: 29.3080s
2021-10-11 21:13:10,435 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:13:10,435 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:13:10,435 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:13:10,473 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:13:11,405 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/11200.ckpt
2021-10-11 21:13:11,479 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/11200.ckpt
2021-10-11 21:13:11,480 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/11200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/11200.ckpt')
2021-10-11 21:13:11,484 - INFO - joeynmt.training - Example #0
2021-10-11 21:13:11,484 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:13:11,484 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:13:11,484 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:13:11,485 - INFO - joeynmt.training - Example #1
2021-10-11 21:13:11,485 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:13:11,485 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:13:11,485 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s first reported to my house ."
2021-10-11 21:13:11,485 - INFO - joeynmt.training - Example #2
2021-10-11 21:13:11,485 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:13:11,485 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:13:11,485 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:13:11,485 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    12200: bleu:  15.22, loss: 416408.7812, ppl:  19.6114, duration: 30.1942s
2021-10-11 21:14:03,352 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:14:03,352 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:14:03,352 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:14:03,393 - INFO - joeynmt.training - Example #0
2021-10-11 21:14:03,393 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:14:03,393 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:14:03,393 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:14:03,393 - INFO - joeynmt.training - Example #1
2021-10-11 21:14:03,393 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:14:03,394 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:14:03,394 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the office of the house ."
2021-10-11 21:14:03,394 - INFO - joeynmt.training - Example #2
2021-10-11 21:14:03,394 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:14:03,394 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:14:03,394 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:14:03,394 - INFO - joeynmt.training - Validation result (greedy) at epoch  12, step    12300: bleu:  14.82, loss: 415798.5000, ppl:  19.5261, duration: 29.2149s
2021-10-11 21:14:24,958 - INFO - joeynmt.training - Epoch  12: total training loss 58538.91
2021-10-11 21:14:24,959 - INFO - joeynmt.training - EPOCH 13
2021-10-11 21:14:55,517 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:14:55,517 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:14:55,518 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:14:55,559 - INFO - joeynmt.training - Example #0
2021-10-11 21:14:55,559 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:14:55,559 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:14:55,559 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:14:55,559 - INFO - joeynmt.training - Example #1
2021-10-11 21:14:55,560 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:14:55,560 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:14:55,560 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the background ."
2021-10-11 21:14:55,560 - INFO - joeynmt.training - Example #2
2021-10-11 21:14:55,560 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:14:55,560 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:14:55,560 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:14:55,560 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12400: bleu:  14.80, loss: 415320.1250, ppl:  19.4594, duration: 29.6254s
2021-10-11 21:15:48,059 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:15:48,059 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:15:48,059 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:15:48,104 - INFO - joeynmt.training - Example #0
2021-10-11 21:15:48,105 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:15:48,105 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:15:48,105 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:15:48,105 - INFO - joeynmt.training - Example #1
2021-10-11 21:15:48,105 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:15:48,105 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:15:48,105 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s revenue in my house ."
2021-10-11 21:15:48,106 - INFO - joeynmt.training - Example #2
2021-10-11 21:15:48,106 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:15:48,106 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:15:48,106 - INFO - joeynmt.training - 	Hypothesis: "My father listen listening to the news BBC , in my small radio , radio distribution ."
2021-10-11 21:15:48,106 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12500: bleu:  15.22, loss: 416776.0312, ppl:  19.6630, duration: 28.3889s
2021-10-11 21:16:40,752 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:16:40,752 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:16:40,752 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:16:40,798 - INFO - joeynmt.training - Example #0
2021-10-11 21:16:40,798 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:16:40,798 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:16:40,798 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:16:40,798 - INFO - joeynmt.training - Example #1
2021-10-11 21:16:40,798 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:16:40,799 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:16:40,799 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s first invited to my house ."
2021-10-11 21:16:40,799 - INFO - joeynmt.training - Example #2
2021-10-11 21:16:40,799 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:16:40,799 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:16:40,799 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:16:40,799 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12600: bleu:  14.93, loss: 415867.0625, ppl:  19.5357, duration: 29.8942s
2021-10-11 21:17:33,070 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:17:33,070 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:17:33,070 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:17:33,114 - INFO - joeynmt.training - Example #0
2021-10-11 21:17:33,114 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:17:33,114 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:17:33,115 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:17:33,115 - INFO - joeynmt.training - Example #1
2021-10-11 21:17:33,115 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:17:33,115 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:17:33,115 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the office of the house ."
2021-10-11 21:17:33,115 - INFO - joeynmt.training - Example #2
2021-10-11 21:17:33,115 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:17:33,115 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:17:33,115 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:17:33,116 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12700: bleu:  15.09, loss: 415312.2500, ppl:  19.4583, duration: 29.5292s
2021-10-11 21:18:25,630 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:18:25,630 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:18:25,630 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:18:25,675 - INFO - joeynmt.training - Example #0
2021-10-11 21:18:25,675 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:18:25,675 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:18:25,675 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:18:25,675 - INFO - joeynmt.training - Example #1
2021-10-11 21:18:25,675 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:18:25,676 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:18:25,676 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the office in my house ."
2021-10-11 21:18:25,676 - INFO - joeynmt.training - Example #2
2021-10-11 21:18:25,676 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:18:25,676 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:18:25,676 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:18:25,676 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12800: bleu:  15.04, loss: 415013.8750, ppl:  19.4169, duration: 29.7086s
2021-10-11 21:19:17,854 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:19:17,854 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:19:17,854 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:19:17,897 - INFO - joeynmt.training - Example #0
2021-10-11 21:19:17,898 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:19:17,898 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:19:17,898 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:19:17,898 - INFO - joeynmt.training - Example #1
2021-10-11 21:19:17,898 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:19:17,898 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:19:17,898 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s office in my house ."
2021-10-11 21:19:17,898 - INFO - joeynmt.training - Example #2
2021-10-11 21:19:17,899 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:19:17,899 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:19:17,899 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:19:17,899 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    12900: bleu:  15.08, loss: 414757.3438, ppl:  19.3813, duration: 29.2155s
2021-10-11 21:19:40,434 - INFO - joeynmt.training - Epoch  13, Step:    13000, Batch Loss:    64.225281, Tokens per Sec:     7895, Lr: 0.000075
2021-10-11 21:20:08,943 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:20:08,943 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:20:08,943 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:20:08,985 - INFO - joeynmt.training - Example #0
2021-10-11 21:20:08,986 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:20:08,986 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:20:08,986 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:20:08,986 - INFO - joeynmt.training - Example #1
2021-10-11 21:20:08,986 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:20:08,986 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:20:08,986 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s report of the office ."
2021-10-11 21:20:08,986 - INFO - joeynmt.training - Example #2
2021-10-11 21:20:08,987 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:20:08,987 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:20:08,987 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:20:08,987 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    13000: bleu:  15.14, loss: 414953.4688, ppl:  19.4085, duration: 28.5515s
2021-10-11 21:21:00,302 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:21:00,302 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:21:00,302 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:21:00,340 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:21:01,255 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/12200.ckpt
2021-10-11 21:21:01,325 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/12200.ckpt
2021-10-11 21:21:01,326 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/12200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/12200.ckpt')
2021-10-11 21:21:01,330 - INFO - joeynmt.training - Example #0
2021-10-11 21:21:01,330 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:21:01,330 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:21:01,331 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:21:01,331 - INFO - joeynmt.training - Example #1
2021-10-11 21:21:01,331 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:21:01,331 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:21:01,331 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 21:21:01,331 - INFO - joeynmt.training - Example #2
2021-10-11 21:21:01,331 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:21:01,331 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:21:01,331 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:21:01,331 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    13100: bleu:  15.26, loss: 415188.6250, ppl:  19.4412, duration: 29.8199s
2021-10-11 21:21:52,355 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:21:52,356 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:21:52,356 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:21:52,398 - INFO - joeynmt.training - Example #0
2021-10-11 21:21:52,398 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:21:52,398 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:21:52,398 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:21:52,398 - INFO - joeynmt.training - Example #1
2021-10-11 21:21:52,398 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:21:52,399 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:21:52,399 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the invention of the father of the office ."
2021-10-11 21:21:52,399 - INFO - joeynmt.training - Example #2
2021-10-11 21:21:52,399 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:21:52,399 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:21:52,399 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:21:52,399 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    13200: bleu:  15.02, loss: 414263.3125, ppl:  19.3130, duration: 28.7563s
2021-10-11 21:22:43,935 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:22:43,935 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:22:43,935 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:22:43,972 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:22:44,887 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/13100.ckpt
2021-10-11 21:22:44,955 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/13100.ckpt
2021-10-11 21:22:44,956 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/13100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/13100.ckpt')
2021-10-11 21:22:44,960 - INFO - joeynmt.training - Example #0
2021-10-11 21:22:44,960 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:22:44,960 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:22:44,960 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:22:44,960 - INFO - joeynmt.training - Example #1
2021-10-11 21:22:44,961 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:22:44,961 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:22:44,961 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office of the house ."
2021-10-11 21:22:44,961 - INFO - joeynmt.training - Example #2
2021-10-11 21:22:44,961 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:22:44,961 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:22:44,961 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC in my small radio policy .
2021-10-11 21:22:44,961 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    13300: bleu:  15.28, loss: 414537.7812, ppl:  19.3509, duration: 29.9145s
2021-10-11 21:23:36,225 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:23:36,226 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:23:36,226 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:23:36,268 - INFO - joeynmt.training - Example #0
2021-10-11 21:23:36,268 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:23:36,268 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:23:36,268 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:23:36,268 - INFO - joeynmt.training - Example #1
2021-10-11 21:23:36,269 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:23:36,269 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:23:36,269 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the door , I was going to come to my house ."
2021-10-11 21:23:36,269 - INFO - joeynmt.training - Example #2
2021-10-11 21:23:36,269 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:23:36,269 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:23:36,269 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC in my small radio policy .
2021-10-11 21:23:36,269 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step    13400: bleu:  15.28, loss: 414344.8438, ppl:  19.3243, duration: 28.8740s
2021-10-11 21:23:43,702 - INFO - joeynmt.training - Epoch  13: total training loss 57867.71
2021-10-11 21:23:43,702 - INFO - joeynmt.training - EPOCH 14
2021-10-11 21:24:27,324 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:24:27,324 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:24:27,324 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:24:27,362 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:24:28,279 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/13300.ckpt
2021-10-11 21:24:28,345 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/13300.ckpt
2021-10-11 21:24:28,345 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/13300.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/13300.ckpt')
2021-10-11 21:24:28,349 - INFO - joeynmt.training - Example #0
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:24:28,350 - INFO - joeynmt.training - Example #1
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the office of the office of my home ."
2021-10-11 21:24:28,350 - INFO - joeynmt.training - Example #2
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:24:28,350 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio pocket .
2021-10-11 21:24:28,351 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    13500: bleu:  15.31, loss: 414656.0938, ppl:  19.3673, duration: 29.4504s
2021-10-11 21:25:21,704 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:25:21,705 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:25:21,705 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:25:21,750 - INFO - joeynmt.training - Example #0
2021-10-11 21:25:21,750 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:25:21,750 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:25:21,750 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:25:21,750 - INFO - joeynmt.training - Example #1
2021-10-11 21:25:21,750 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:25:21,751 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:25:21,751 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law up from the door of the door of the home ."
2021-10-11 21:25:21,751 - INFO - joeynmt.training - Example #2
2021-10-11 21:25:21,751 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:25:21,751 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:25:21,751 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:25:21,751 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    13600: bleu:  14.88, loss: 414415.9375, ppl:  19.3341, duration: 28.9699s
2021-10-11 21:26:13,864 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:26:13,864 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:26:13,864 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:26:13,912 - INFO - joeynmt.training - Example #0
2021-10-11 21:26:13,912 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:26:13,912 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:26:13,912 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:26:13,912 - INFO - joeynmt.training - Example #1
2021-10-11 21:26:13,912 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:26:13,912 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:26:13,913 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office to my house ."
2021-10-11 21:26:13,913 - INFO - joeynmt.training - Example #2
2021-10-11 21:26:13,913 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:26:13,913 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:26:13,913 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small page of radio radio .
2021-10-11 21:26:13,913 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    13700: bleu:  15.15, loss: 414786.4375, ppl:  19.3854, duration: 29.3800s
2021-10-11 21:27:05,889 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:27:05,890 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:27:05,890 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:27:05,939 - INFO - joeynmt.training - Example #0
2021-10-11 21:27:05,940 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:27:05,940 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:27:05,940 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:27:05,940 - INFO - joeynmt.training - Example #1
2021-10-11 21:27:05,940 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:27:05,940 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:27:05,940 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the office , the dad of the office , I remember ."
2021-10-11 21:27:05,940 - INFO - joeynmt.training - Example #2
2021-10-11 21:27:05,941 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:27:05,941 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:27:05,941 - INFO - joeynmt.training - 	Hypothesis: My father listening to BBC to my small radio pocket .
2021-10-11 21:27:05,941 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    13800: bleu:  15.16, loss: 413739.2188, ppl:  19.2408, duration: 29.2866s
2021-10-11 21:27:57,725 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:27:57,725 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:27:57,725 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:27:57,765 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:27:58,830 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/13500.ckpt
2021-10-11 21:27:58,895 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/13500.ckpt
2021-10-11 21:27:58,896 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/13500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/13500.ckpt')
2021-10-11 21:27:58,900 - INFO - joeynmt.training - Example #0
2021-10-11 21:27:58,900 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:27:58,900 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:27:58,900 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:27:58,900 - INFO - joeynmt.training - Example #1
2021-10-11 21:27:58,901 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:27:58,901 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:27:58,901 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the door of the door ."
2021-10-11 21:27:58,901 - INFO - joeynmt.training - Example #2
2021-10-11 21:27:58,901 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:27:58,901 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:27:58,901 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio .
2021-10-11 21:27:58,901 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    13900: bleu:  15.31, loss: 413944.7812, ppl:  19.2691, duration: 30.2837s
2021-10-11 21:28:21,645 - INFO - joeynmt.training - Epoch  14, Step:    14000, Batch Loss:    57.489967, Tokens per Sec:     7954, Lr: 0.000075
2021-10-11 21:28:49,987 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:28:49,988 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:28:49,988 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:28:50,028 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:28:50,957 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/13900.ckpt
2021-10-11 21:28:51,026 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/13900.ckpt
2021-10-11 21:28:51,026 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/13900.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/13900.ckpt')
2021-10-11 21:28:51,031 - INFO - joeynmt.training - Example #0
2021-10-11 21:28:51,031 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:28:51,031 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:28:51,031 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:28:51,031 - INFO - joeynmt.training - Example #1
2021-10-11 21:28:51,032 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:28:51,032 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:28:51,032 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the office of the home ."
2021-10-11 21:28:51,032 - INFO - joeynmt.training - Example #2
2021-10-11 21:28:51,032 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:28:51,032 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:28:51,032 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio scene .
2021-10-11 21:28:51,032 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    14000: bleu:  15.40, loss: 413945.5625, ppl:  19.2692, duration: 29.3866s
2021-10-11 21:29:43,189 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:29:43,190 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:29:43,190 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:29:43,233 - INFO - joeynmt.training - Example #0
2021-10-11 21:29:43,234 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:29:43,234 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:29:43,234 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:29:43,234 - INFO - joeynmt.training - Example #1
2021-10-11 21:29:43,234 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:29:43,234 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:29:43,234 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 21:29:43,234 - INFO - joeynmt.training - Example #2
2021-10-11 21:29:43,234 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:29:43,235 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:29:43,235 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:29:43,235 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    14100: bleu:  15.21, loss: 413128.4062, ppl:  19.1570, duration: 29.1547s
2021-10-11 21:30:35,005 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:30:35,005 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:30:35,005 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:30:35,048 - INFO - joeynmt.training - Example #0
2021-10-11 21:30:35,048 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:30:35,049 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:30:35,049 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:30:35,049 - INFO - joeynmt.training - Example #1
2021-10-11 21:30:35,049 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:30:35,049 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:30:35,049 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember that he was born with a fourth bird of return ."
2021-10-11 21:30:35,049 - INFO - joeynmt.training - Example #2
2021-10-11 21:30:35,049 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:30:35,050 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:30:35,050 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:30:35,050 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    14200: bleu:  15.38, loss: 413342.4062, ppl:  19.1863, duration: 28.9447s
2021-10-11 21:31:26,013 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:31:26,013 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:31:26,013 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:31:26,057 - INFO - joeynmt.training - Example #0
2021-10-11 21:31:26,057 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:31:26,058 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:31:26,058 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:31:26,058 - INFO - joeynmt.training - Example #1
2021-10-11 21:31:26,058 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:31:26,058 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:31:26,058 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 21:31:26,058 - INFO - joeynmt.training - Example #2
2021-10-11 21:31:26,058 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:31:26,058 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:31:26,058 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio wave .
2021-10-11 21:31:26,059 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    14300: bleu:  15.02, loss: 413294.1562, ppl:  19.1797, duration: 28.5488s
2021-10-11 21:32:17,876 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:32:17,877 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:32:17,877 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:32:17,919 - INFO - joeynmt.training - Example #0
2021-10-11 21:32:17,920 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:32:17,920 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:32:17,920 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:32:17,920 - INFO - joeynmt.training - Example #1
2021-10-11 21:32:17,920 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:32:17,920 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:32:17,920 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped up to my house ."
2021-10-11 21:32:17,921 - INFO - joeynmt.training - Example #2
2021-10-11 21:32:17,921 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:32:17,921 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:32:17,921 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:32:17,921 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step    14400: bleu:  15.36, loss: 412945.1250, ppl:  19.1319, duration: 29.0435s
2021-10-11 21:32:32,150 - INFO - joeynmt.training - Epoch  14: total training loss 57227.04
2021-10-11 21:32:32,151 - INFO - joeynmt.training - EPOCH 15
2021-10-11 21:33:08,563 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:33:08,563 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:33:08,563 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:33:08,601 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:33:09,491 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/14000.ckpt
2021-10-11 21:33:09,562 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/14000.ckpt
2021-10-11 21:33:09,562 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/14000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/14000.ckpt')
2021-10-11 21:33:09,567 - INFO - joeynmt.training - Example #0
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:33:09,568 - INFO - joeynmt.training - Example #1
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the office of the house ."
2021-10-11 21:33:09,568 - INFO - joeynmt.training - Example #2
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:33:09,568 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:33:09,568 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    14500: bleu:  15.49, loss: 413664.6562, ppl:  19.2306, duration: 29.2933s
2021-10-11 21:34:01,064 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:34:01,065 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:34:01,065 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:34:01,106 - INFO - joeynmt.training - Example #0
2021-10-11 21:34:01,107 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:34:01,107 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:34:01,107 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:34:01,107 - INFO - joeynmt.training - Example #1
2021-10-11 21:34:01,107 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:34:01,107 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:34:01,107 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped up to my house ."
2021-10-11 21:34:01,107 - INFO - joeynmt.training - Example #2
2021-10-11 21:34:01,108 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:34:01,108 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:34:01,108 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:34:01,108 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    14600: bleu:  15.19, loss: 413865.5312, ppl:  19.2582, duration: 29.0849s
2021-10-11 21:34:52,283 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:34:52,283 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:34:52,283 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:34:52,326 - INFO - joeynmt.training - Example #0
2021-10-11 21:34:52,326 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:34:52,327 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:34:52,327 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:34:52,327 - INFO - joeynmt.training - Example #1
2021-10-11 21:34:52,327 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:34:52,327 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:34:52,328 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father ."
2021-10-11 21:34:52,328 - INFO - joeynmt.training - Example #2
2021-10-11 21:34:52,328 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:34:52,328 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:34:52,328 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:34:52,329 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    14700: bleu:  15.38, loss: 414151.1562, ppl:  19.2975, duration: 28.8933s
2021-10-11 21:35:45,660 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:35:45,661 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:35:45,661 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:35:45,705 - INFO - joeynmt.training - Example #0
2021-10-11 21:35:45,706 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:35:45,706 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:35:45,706 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:35:45,706 - INFO - joeynmt.training - Example #1
2021-10-11 21:35:45,706 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:35:45,706 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:35:45,707 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office to my house ."
2021-10-11 21:35:45,707 - INFO - joeynmt.training - Example #2
2021-10-11 21:35:45,707 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:35:45,707 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:35:45,707 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:35:45,707 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    14800: bleu:  15.28, loss: 413146.9062, ppl:  19.1595, duration: 28.6035s
2021-10-11 21:36:36,418 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:36:36,418 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:36:36,418 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:36:36,463 - INFO - joeynmt.training - Example #0
2021-10-11 21:36:36,463 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:36:36,464 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:36:36,464 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:36:36,464 - INFO - joeynmt.training - Example #1
2021-10-11 21:36:36,464 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:36:36,464 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:36:36,464 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s balloon , the soul of the office , the door of the house ."
2021-10-11 21:36:36,464 - INFO - joeynmt.training - Example #2
2021-10-11 21:36:36,464 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:36:36,464 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:36:36,464 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio radio .
2021-10-11 21:36:36,465 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    14900: bleu:  15.45, loss: 413161.3125, ppl:  19.1615, duration: 28.1064s
2021-10-11 21:36:59,141 - INFO - joeynmt.training - Epoch  15, Step:    15000, Batch Loss:    54.339790, Tokens per Sec:     7961, Lr: 0.000075
2021-10-11 21:37:27,937 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:37:27,937 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:37:27,937 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:37:27,982 - INFO - joeynmt.training - Example #0
2021-10-11 21:37:27,982 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:37:27,983 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:37:27,983 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:37:27,983 - INFO - joeynmt.training - Example #1
2021-10-11 21:37:27,983 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:37:27,983 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:37:27,983 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s first return ."
2021-10-11 21:37:27,983 - INFO - joeynmt.training - Example #2
2021-10-11 21:37:27,983 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:37:27,983 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:37:27,983 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my neighborhood radio .
2021-10-11 21:37:27,984 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    15000: bleu:  15.47, loss: 413143.1562, ppl:  19.1590, duration: 28.8418s
2021-10-11 21:38:20,156 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:38:20,156 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:38:20,156 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:38:20,201 - INFO - joeynmt.training - Example #0
2021-10-11 21:38:20,202 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:38:20,202 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:38:20,202 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:38:20,202 - INFO - joeynmt.training - Example #1
2021-10-11 21:38:20,202 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:38:20,202 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:38:20,202 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the door , and I was going to get the door of my home ."
2021-10-11 21:38:20,202 - INFO - joeynmt.training - Example #2
2021-10-11 21:38:20,202 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:38:20,203 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:38:20,203 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my neighborhood radio .
2021-10-11 21:38:20,203 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    15100: bleu:  15.25, loss: 412782.7500, ppl:  19.1097, duration: 29.4084s
2021-10-11 21:39:12,048 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:39:12,049 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:39:12,049 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:39:12,088 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:39:12,964 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/14500.ckpt
2021-10-11 21:39:13,034 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/14500.ckpt
2021-10-11 21:39:13,035 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/14500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/14500.ckpt')
2021-10-11 21:39:13,039 - INFO - joeynmt.training - Example #0
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:39:13,040 - INFO - joeynmt.training - Example #1
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of the house ."
2021-10-11 21:39:13,040 - INFO - joeynmt.training - Example #2
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:39:13,040 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio wave .
2021-10-11 21:39:13,040 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    15200: bleu:  15.58, loss: 412370.8125, ppl:  19.0535, duration: 30.0999s
2021-10-11 21:40:04,639 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:40:04,640 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:40:04,640 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:40:04,683 - INFO - joeynmt.training - Example #0
2021-10-11 21:40:04,684 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:40:04,684 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:40:04,684 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:40:04,684 - INFO - joeynmt.training - Example #1
2021-10-11 21:40:04,684 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:40:04,684 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:40:04,684 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s going to the morning of the office of the office ."
2021-10-11 21:40:04,685 - INFO - joeynmt.training - Example #2
2021-10-11 21:40:04,685 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:40:04,685 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:40:04,685 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:40:04,685 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    15300: bleu:  15.48, loss: 412181.5938, ppl:  19.0278, duration: 29.1560s
2021-10-11 21:40:55,610 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:40:55,610 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:40:55,610 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:40:55,654 - INFO - joeynmt.training - Example #0
2021-10-11 21:40:55,655 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:40:55,655 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:40:55,655 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:40:55,655 - INFO - joeynmt.training - Example #1
2021-10-11 21:40:55,655 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:40:55,655 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:40:55,655 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office of the home ."
2021-10-11 21:40:55,655 - INFO - joeynmt.training - Example #2
2021-10-11 21:40:55,656 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:40:55,656 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:40:55,656 - INFO - joeynmt.training - 	Hypothesis: My father listen to BBC to my small radio wave .
2021-10-11 21:40:55,656 - INFO - joeynmt.training - Validation result (greedy) at epoch  15, step    15400: bleu:  15.44, loss: 412777.2812, ppl:  19.1090, duration: 28.5010s
2021-10-11 21:41:17,000 - INFO - joeynmt.training - Epoch  15: total training loss 56613.92
2021-10-11 21:41:17,000 - INFO - joeynmt.training - EPOCH 16
2021-10-11 21:41:46,911 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:41:46,911 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:41:46,912 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:41:46,955 - INFO - joeynmt.training - Example #0
2021-10-11 21:41:46,955 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:41:46,955 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:41:46,955 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:41:46,955 - INFO - joeynmt.training - Example #1
2021-10-11 21:41:46,956 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:41:46,956 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:41:46,956 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office of the house ."
2021-10-11 21:41:46,956 - INFO - joeynmt.training - Example #2
2021-10-11 21:41:46,956 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:41:46,956 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:41:46,956 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:41:46,956 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    15500: bleu:  15.30, loss: 412036.8438, ppl:  19.0081, duration: 28.7345s
2021-10-11 21:42:38,848 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:42:38,848 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:42:38,848 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:42:38,892 - INFO - joeynmt.training - Example #0
2021-10-11 21:42:38,893 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:42:38,893 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:42:38,893 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:42:38,893 - INFO - joeynmt.training - Example #1
2021-10-11 21:42:38,893 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:42:38,893 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:42:38,893 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the door , the door of the door ."
2021-10-11 21:42:38,893 - INFO - joeynmt.training - Example #2
2021-10-11 21:42:38,893 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:42:38,893 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:42:38,894 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio wave .
2021-10-11 21:42:38,894 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    15600: bleu:  15.38, loss: 412488.5312, ppl:  19.0696, duration: 29.3992s
2021-10-11 21:43:29,756 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:43:29,756 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:43:29,756 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:43:29,798 - INFO - joeynmt.training - Example #0
2021-10-11 21:43:29,798 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:43:29,798 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:43:29,798 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:43:29,798 - INFO - joeynmt.training - Example #1
2021-10-11 21:43:29,798 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:43:29,799 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:43:29,799 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the door of the office of the office of the home ."
2021-10-11 21:43:29,799 - INFO - joeynmt.training - Example #2
2021-10-11 21:43:29,799 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:43:29,799 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:43:29,799 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio script .
2021-10-11 21:43:29,799 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    15700: bleu:  15.54, loss: 412377.8750, ppl:  19.0545, duration: 28.5549s
2021-10-11 21:44:21,926 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:44:21,926 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:44:21,926 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:44:21,969 - INFO - joeynmt.training - Example #0
2021-10-11 21:44:21,969 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:44:21,969 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:44:21,969 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:44:21,969 - INFO - joeynmt.training - Example #1
2021-10-11 21:44:21,969 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:44:21,969 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:44:21,970 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office , I was going to my house ."
2021-10-11 21:44:21,970 - INFO - joeynmt.training - Example #2
2021-10-11 21:44:21,970 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:44:21,970 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:44:21,970 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio wave .
2021-10-11 21:44:21,970 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    15800: bleu:  15.45, loss: 413153.3438, ppl:  19.1604, duration: 29.3543s
2021-10-11 21:45:13,768 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:45:13,768 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:45:13,768 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:45:13,810 - INFO - joeynmt.training - Example #0
2021-10-11 21:45:13,810 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:45:13,810 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:45:13,810 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:45:13,811 - INFO - joeynmt.training - Example #1
2021-10-11 21:45:13,811 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:45:13,811 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:45:13,811 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the office of the home ."
2021-10-11 21:45:13,811 - INFO - joeynmt.training - Example #2
2021-10-11 21:45:13,811 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:45:13,811 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:45:13,811 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:45:13,811 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    15900: bleu:  15.48, loss: 412747.8750, ppl:  19.1050, duration: 29.0124s
2021-10-11 21:45:36,813 - INFO - joeynmt.training - Epoch  16, Step:    16000, Batch Loss:    56.237885, Tokens per Sec:     8000, Lr: 0.000075
2021-10-11 21:46:07,460 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:46:07,460 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:46:07,460 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:46:07,503 - INFO - joeynmt.training - Example #0
2021-10-11 21:46:07,503 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:46:07,503 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:46:07,504 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:46:07,504 - INFO - joeynmt.training - Example #1
2021-10-11 21:46:07,504 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:46:07,504 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:46:07,504 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the office of the office ."
2021-10-11 21:46:07,504 - INFO - joeynmt.training - Example #2
2021-10-11 21:46:07,504 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:46:07,504 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:46:07,504 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio policy .
2021-10-11 21:46:07,504 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    16000: bleu:  15.30, loss: 412490.7500, ppl:  19.0699, duration: 30.6910s
2021-10-11 21:46:59,447 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:46:59,448 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:46:59,448 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:46:59,487 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:47:00,415 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/15200.ckpt
2021-10-11 21:47:00,489 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/15200.ckpt
2021-10-11 21:47:00,490 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/15200.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/15200.ckpt')
2021-10-11 21:47:00,494 - INFO - joeynmt.training - Example #0
2021-10-11 21:47:00,494 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:47:00,495 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:47:00,495 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:47:00,495 - INFO - joeynmt.training - Example #1
2021-10-11 21:47:00,495 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:47:00,495 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:47:00,495 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office ."
2021-10-11 21:47:00,495 - INFO - joeynmt.training - Example #2
2021-10-11 21:47:00,495 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:47:00,495 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:47:00,495 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:47:00,495 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    16100: bleu:  15.73, loss: 412004.8750, ppl:  19.0038, duration: 30.1889s
2021-10-11 21:47:52,378 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:47:52,379 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:47:52,379 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:47:52,423 - INFO - joeynmt.training - Example #0
2021-10-11 21:47:52,424 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:47:52,424 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:47:52,424 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:47:52,424 - INFO - joeynmt.training - Example #1
2021-10-11 21:47:52,424 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:47:52,424 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:47:52,424 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s balloon , and I was going to go to my house ."
2021-10-11 21:47:52,425 - INFO - joeynmt.training - Example #2
2021-10-11 21:47:52,425 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:47:52,425 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:47:52,425 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my neighborhood radio .
2021-10-11 21:47:52,425 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    16200: bleu:  15.40, loss: 412512.8750, ppl:  19.0729, duration: 29.4312s
2021-10-11 21:48:43,180 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:48:43,180 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:48:43,180 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:48:43,224 - INFO - joeynmt.training - Example #0
2021-10-11 21:48:43,225 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:48:43,225 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:48:43,225 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:48:43,225 - INFO - joeynmt.training - Example #1
2021-10-11 21:48:43,225 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:48:43,225 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:48:43,225 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped up to my house ."
2021-10-11 21:48:43,225 - INFO - joeynmt.training - Example #2
2021-10-11 21:48:43,225 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:48:43,226 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:48:43,226 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:48:43,226 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    16300: bleu:  15.52, loss: 412825.4375, ppl:  19.1156, duration: 28.1507s
2021-10-11 21:49:34,786 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:49:34,787 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:49:34,787 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:49:34,838 - INFO - joeynmt.training - Example #0
2021-10-11 21:49:34,839 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:49:34,839 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:49:34,839 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:49:34,839 - INFO - joeynmt.training - Example #1
2021-10-11 21:49:34,839 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:49:34,839 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:49:34,839 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my dad &apos;s first revenue to my house ."
2021-10-11 21:49:34,839 - INFO - joeynmt.training - Example #2
2021-10-11 21:49:34,840 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:49:34,840 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:49:34,840 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:49:34,840 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    16400: bleu:  15.48, loss: 411637.9375, ppl:  18.9540, duration: 29.1207s
2021-10-11 21:50:26,543 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:50:26,544 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:50:26,544 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:50:26,584 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:50:27,483 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/16100.ckpt
2021-10-11 21:50:27,551 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/16100.ckpt
2021-10-11 21:50:27,552 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/16100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/16100.ckpt')
2021-10-11 21:50:27,558 - INFO - joeynmt.training - Example #0
2021-10-11 21:50:27,558 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:50:27,558 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:50:27,558 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:50:27,558 - INFO - joeynmt.training - Example #1
2021-10-11 21:50:27,558 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:50:27,558 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:50:27,558 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember the morning of the father of the invention of the office in my house ."
2021-10-11 21:50:27,558 - INFO - joeynmt.training - Example #2
2021-10-11 21:50:27,559 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:50:27,559 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:50:27,559 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:50:27,559 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step    16500: bleu:  15.76, loss: 410925.6562, ppl:  18.8578, duration: 30.1498s
2021-10-11 21:50:34,944 - INFO - joeynmt.training - Epoch  16: total training loss 56019.54
2021-10-11 21:50:34,944 - INFO - joeynmt.training - EPOCH 17
2021-10-11 21:51:19,713 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:51:19,714 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:51:19,714 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:51:19,758 - INFO - joeynmt.training - Example #0
2021-10-11 21:51:19,759 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:51:19,759 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:51:19,759 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:51:19,759 - INFO - joeynmt.training - Example #1
2021-10-11 21:51:19,759 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:51:19,759 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:51:19,759 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office ."
2021-10-11 21:51:19,759 - INFO - joeynmt.training - Example #2
2021-10-11 21:51:19,760 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:51:19,760 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:51:19,760 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:51:19,760 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    16600: bleu:  15.68, loss: 411827.0312, ppl:  18.9796, duration: 29.2390s
2021-10-11 21:52:11,883 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:52:11,884 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:52:11,884 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:52:11,928 - INFO - joeynmt.training - Example #0
2021-10-11 21:52:11,929 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:52:11,929 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:52:11,929 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:52:11,929 - INFO - joeynmt.training - Example #1
2021-10-11 21:52:11,929 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:52:11,929 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:52:11,930 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the background ."
2021-10-11 21:52:11,930 - INFO - joeynmt.training - Example #2
2021-10-11 21:52:11,930 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:52:11,930 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:52:11,930 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio wave .
2021-10-11 21:52:11,930 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    16700: bleu:  15.33, loss: 410837.6562, ppl:  18.8459, duration: 29.6230s
2021-10-11 21:53:02,698 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:53:02,699 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:53:02,699 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:53:02,742 - INFO - joeynmt.training - Example #0
2021-10-11 21:53:02,743 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:53:02,743 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:53:02,743 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:53:02,743 - INFO - joeynmt.training - Example #1
2021-10-11 21:53:02,743 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:53:02,743 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:53:02,743 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s balloon from the door of the office ."
2021-10-11 21:53:02,743 - INFO - joeynmt.training - Example #2
2021-10-11 21:53:02,743 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:53:02,744 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:53:02,744 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:53:02,744 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    16800: bleu:  15.66, loss: 411992.0938, ppl:  19.0020, duration: 28.6044s
2021-10-11 21:53:53,972 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:53:53,973 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:53:53,973 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:53:54,017 - INFO - joeynmt.training - Example #0
2021-10-11 21:53:54,018 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:53:54,018 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:53:54,018 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:53:54,018 - INFO - joeynmt.training - Example #1
2021-10-11 21:53:54,018 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:53:54,018 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:53:54,018 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s balloon from the door of the office in my house ."
2021-10-11 21:53:54,018 - INFO - joeynmt.training - Example #2
2021-10-11 21:53:54,018 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:53:54,019 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:53:54,019 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio wave .
2021-10-11 21:53:54,019 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    16900: bleu:  15.36, loss: 411095.0312, ppl:  18.8806, duration: 28.7564s
2021-10-11 21:54:16,373 - INFO - joeynmt.training - Epoch  17, Step:    17000, Batch Loss:    51.569099, Tokens per Sec:     8148, Lr: 0.000075
2021-10-11 21:54:45,267 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:54:45,267 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:54:45,268 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:54:45,310 - INFO - joeynmt.training - Example #0
2021-10-11 21:54:45,311 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:54:45,311 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:54:45,311 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:54:45,311 - INFO - joeynmt.training - Example #1
2021-10-11 21:54:45,311 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:54:45,311 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:54:45,311 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the invention of my house ."
2021-10-11 21:54:45,311 - INFO - joeynmt.training - Example #2
2021-10-11 21:54:45,312 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:54:45,312 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:54:45,312 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my neighborhood radio .
2021-10-11 21:54:45,312 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    17000: bleu:  15.44, loss: 411718.7500, ppl:  18.9650, duration: 28.9380s
2021-10-11 21:55:36,142 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:55:36,143 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:55:36,143 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:55:36,185 - INFO - joeynmt.training - Example #0
2021-10-11 21:55:36,186 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:55:36,186 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:55:36,186 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:55:36,186 - INFO - joeynmt.training - Example #1
2021-10-11 21:55:36,186 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:55:36,186 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:55:36,186 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the office to my house ."
2021-10-11 21:55:36,186 - INFO - joeynmt.training - Example #2
2021-10-11 21:55:36,187 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:55:36,187 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:55:36,187 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 21:55:36,187 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    17100: bleu:  15.60, loss: 410737.3750, ppl:  18.8324, duration: 28.3328s
2021-10-11 21:56:27,679 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:56:27,679 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:56:27,679 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:56:27,722 - INFO - joeynmt.training - Example #0
2021-10-11 21:56:27,722 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:56:27,722 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:56:27,722 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:56:27,722 - INFO - joeynmt.training - Example #1
2021-10-11 21:56:27,722 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:56:27,723 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:56:27,723 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my house ."
2021-10-11 21:56:27,723 - INFO - joeynmt.training - Example #2
2021-10-11 21:56:27,723 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:56:27,723 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:56:27,723 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 21:56:27,723 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    17200: bleu:  15.41, loss: 410229.5000, ppl:  18.7642, duration: 29.0240s
2021-10-11 21:57:19,585 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:57:19,585 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:57:19,585 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:57:19,626 - INFO - joeynmt.training - Example #0
2021-10-11 21:57:19,627 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:57:19,627 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:57:19,627 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:57:19,627 - INFO - joeynmt.training - Example #1
2021-10-11 21:57:19,627 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:57:19,627 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:57:19,627 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the door of the door ."
2021-10-11 21:57:19,627 - INFO - joeynmt.training - Example #2
2021-10-11 21:57:19,628 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:57:19,628 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:57:19,628 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my neighborhood radio .
2021-10-11 21:57:19,628 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    17300: bleu:  15.18, loss: 410340.9062, ppl:  18.7791, duration: 29.3611s
2021-10-11 21:58:12,578 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:58:12,579 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:58:12,579 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:58:12,617 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 21:58:13,541 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/16500.ckpt
2021-10-11 21:58:13,619 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/16500.ckpt
2021-10-11 21:58:13,620 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/16500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/16500.ckpt')
2021-10-11 21:58:13,625 - INFO - joeynmt.training - Example #0
2021-10-11 21:58:13,625 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:58:13,625 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:58:13,625 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:58:13,625 - INFO - joeynmt.training - Example #1
2021-10-11 21:58:13,625 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:58:13,625 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:58:13,625 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the office of my house ."
2021-10-11 21:58:13,625 - INFO - joeynmt.training - Example #2
2021-10-11 21:58:13,626 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:58:13,626 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:58:13,626 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my neighborhood radio .
2021-10-11 21:58:13,626 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    17400: bleu:  15.79, loss: 410617.4062, ppl:  18.8163, duration: 31.5598s
2021-10-11 21:59:05,587 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:59:05,587 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:59:05,587 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:59:05,632 - INFO - joeynmt.training - Example #0
2021-10-11 21:59:05,633 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:59:05,633 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:59:05,633 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:59:05,633 - INFO - joeynmt.training - Example #1
2021-10-11 21:59:05,633 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:59:05,633 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:59:05,633 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s father &apos;s father &apos;s father &apos;s father &apos;s father , I remember my home ."
2021-10-11 21:59:05,633 - INFO - joeynmt.training - Example #2
2021-10-11 21:59:05,634 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:59:05,634 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:59:05,634 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 21:59:05,634 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step    17500: bleu:  15.36, loss: 410056.5000, ppl:  18.7410, duration: 29.2637s
2021-10-11 21:59:20,035 - INFO - joeynmt.training - Epoch  17: total training loss 55440.51
2021-10-11 21:59:20,036 - INFO - joeynmt.training - EPOCH 18
2021-10-11 21:59:57,748 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 21:59:57,748 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 21:59:57,748 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 21:59:57,794 - INFO - joeynmt.training - Example #0
2021-10-11 21:59:57,794 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 21:59:57,795 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 21:59:57,795 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 21:59:57,795 - INFO - joeynmt.training - Example #1
2021-10-11 21:59:57,795 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 21:59:57,795 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 21:59:57,795 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office of my home ."
2021-10-11 21:59:57,795 - INFO - joeynmt.training - Example #2
2021-10-11 21:59:57,795 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 21:59:57,795 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 21:59:57,795 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my neighborhood radio .
2021-10-11 21:59:57,796 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    17600: bleu:  15.74, loss: 410119.3750, ppl:  18.7494, duration: 29.3020s
2021-10-11 22:00:49,644 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:00:49,644 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:00:49,644 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:00:49,690 - INFO - joeynmt.training - Example #0
2021-10-11 22:00:49,691 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:00:49,691 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:00:49,691 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:00:49,691 - INFO - joeynmt.training - Example #1
2021-10-11 22:00:49,691 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:00:49,691 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:00:49,692 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s balloon from the door of the office of my home ."
2021-10-11 22:00:49,692 - INFO - joeynmt.training - Example #2
2021-10-11 22:00:49,692 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:00:49,692 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:00:49,692 - INFO - joeynmt.training - 	Hypothesis: My father listen to BBC to my small radio waves .
2021-10-11 22:00:49,692 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    17700: bleu:  15.74, loss: 410021.6875, ppl:  18.7363, duration: 29.2272s
2021-10-11 22:01:41,367 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:01:41,367 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:01:41,367 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:01:41,407 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 22:01:42,332 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/17400.ckpt
2021-10-11 22:01:42,398 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/17400.ckpt
2021-10-11 22:01:42,398 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/17400.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/17400.ckpt')
2021-10-11 22:01:42,403 - INFO - joeynmt.training - Example #0
2021-10-11 22:01:42,403 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:01:42,403 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:01:42,403 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:01:42,403 - INFO - joeynmt.training - Example #1
2021-10-11 22:01:42,403 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:01:42,403 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:01:42,403 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the office of my home ."
2021-10-11 22:01:42,404 - INFO - joeynmt.training - Example #2
2021-10-11 22:01:42,404 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:01:42,404 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:01:42,404 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 22:01:42,404 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    17800: bleu:  15.89, loss: 411025.4062, ppl:  18.8712, duration: 29.8822s
2021-10-11 22:02:33,402 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:02:33,402 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:02:33,402 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:02:33,443 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 22:02:34,406 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/17800.ckpt
2021-10-11 22:02:34,477 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/17800.ckpt
2021-10-11 22:02:34,477 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/17800.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/17800.ckpt')
2021-10-11 22:02:34,482 - INFO - joeynmt.training - Example #0
2021-10-11 22:02:34,482 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:02:34,482 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:02:34,482 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:02:34,482 - INFO - joeynmt.training - Example #1
2021-10-11 22:02:34,482 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:02:34,483 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:02:34,483 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office ."
2021-10-11 22:02:34,483 - INFO - joeynmt.training - Example #2
2021-10-11 22:02:34,483 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:02:34,483 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:02:34,483 - INFO - joeynmt.training - 	Hypothesis: My father listened to BBC to my small radio waves .
2021-10-11 22:02:34,483 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    17900: bleu:  15.92, loss: 410984.7812, ppl:  18.8657, duration: 29.1546s
2021-10-11 22:02:57,320 - INFO - joeynmt.training - Epoch  18, Step:    18000, Batch Loss:    52.661831, Tokens per Sec:     8014, Lr: 0.000075
2021-10-11 22:03:26,751 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:03:26,751 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:03:26,751 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:03:26,797 - INFO - joeynmt.training - Example #0
2021-10-11 22:03:26,797 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:03:26,797 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:03:26,798 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:03:26,798 - INFO - joeynmt.training - Example #1
2021-10-11 22:03:26,798 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:03:26,798 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:03:26,798 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my house ."
2021-10-11 22:03:26,798 - INFO - joeynmt.training - Example #2
2021-10-11 22:03:26,798 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:03:26,798 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:03:26,798 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio wave .
2021-10-11 22:03:26,799 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    18000: bleu:  15.59, loss: 410707.9688, ppl:  18.8284, duration: 29.4783s
2021-10-11 22:04:17,336 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:04:17,337 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:04:17,337 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:04:17,381 - INFO - joeynmt.training - Example #0
2021-10-11 22:04:17,381 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:04:17,381 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:04:17,382 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:04:17,382 - INFO - joeynmt.training - Example #1
2021-10-11 22:04:17,382 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:04:17,382 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:04:17,382 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the door ."
2021-10-11 22:04:17,382 - INFO - joeynmt.training - Example #2
2021-10-11 22:04:17,382 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:04:17,382 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:04:17,382 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio script .
2021-10-11 22:04:17,382 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    18100: bleu:  15.67, loss: 411132.3438, ppl:  18.8856, duration: 28.0289s
2021-10-11 22:05:09,040 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:05:09,041 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:05:09,041 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:05:09,085 - INFO - joeynmt.training - Example #0
2021-10-11 22:05:09,086 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:05:09,086 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:05:09,086 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:05:09,086 - INFO - joeynmt.training - Example #1
2021-10-11 22:05:09,086 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:05:09,086 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:05:09,086 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office ."
2021-10-11 22:05:09,086 - INFO - joeynmt.training - Example #2
2021-10-11 22:05:09,087 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:05:09,087 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:05:09,087 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 22:05:09,087 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    18200: bleu:  15.79, loss: 409842.9062, ppl:  18.7124, duration: 28.9780s
2021-10-11 22:05:58,837 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:05:58,837 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:05:58,837 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:05:58,875 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 22:06:00,075 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/17900.ckpt
2021-10-11 22:06:00,143 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/17900.ckpt
2021-10-11 22:06:00,143 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/17900.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/17900.ckpt')
2021-10-11 22:06:00,148 - INFO - joeynmt.training - Example #0
2021-10-11 22:06:00,148 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:06:00,148 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:06:00,148 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:06:00,148 - INFO - joeynmt.training - Example #1
2021-10-11 22:06:00,149 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:06:00,149 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:06:00,149 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s balloon from the door of the office ."
2021-10-11 22:06:00,149 - INFO - joeynmt.training - Example #2
2021-10-11 22:06:00,149 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:06:00,149 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:06:00,149 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio .
2021-10-11 22:06:00,149 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    18300: bleu:  15.98, loss: 411295.9062, ppl:  18.9077, duration: 28.6347s
2021-10-11 22:06:50,073 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:06:50,074 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:06:50,074 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:06:50,112 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-11 22:06:51,015 - INFO - joeynmt.helpers - delete models/FINAL_ru_r/18300.ckpt
2021-10-11 22:06:51,084 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/FINAL_ru_r/18300.ckpt
2021-10-11 22:06:51,084 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/FINAL_ru_r/18300.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/FINAL_ru_r/18300.ckpt')
2021-10-11 22:06:51,088 - INFO - joeynmt.training - Example #0
2021-10-11 22:06:51,088 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:06:51,089 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:06:51,089 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:06:51,089 - INFO - joeynmt.training - Example #1
2021-10-11 22:06:51,089 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:06:51,089 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:06:51,089 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s bomb in my home ."
2021-10-11 22:06:51,089 - INFO - joeynmt.training - Example #2
2021-10-11 22:06:51,089 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:06:51,089 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:06:51,089 - INFO - joeynmt.training - 	Hypothesis: My father listen to BBC to my neighborhood radio .
2021-10-11 22:06:51,090 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    18400: bleu:  16.20, loss: 410715.9375, ppl:  18.8295, duration: 28.5829s
2021-10-11 22:07:42,623 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:07:42,623 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:07:42,623 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:07:42,666 - INFO - joeynmt.training - Example #0
2021-10-11 22:07:42,667 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:07:42,667 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:07:42,667 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:07:42,667 - INFO - joeynmt.training - Example #1
2021-10-11 22:07:42,667 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:07:42,667 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:07:42,668 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s balloon from the door of the office ."
2021-10-11 22:07:42,668 - INFO - joeynmt.training - Example #2
2021-10-11 22:07:42,668 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:07:42,668 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:07:42,668 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my neighborhood radio .
2021-10-11 22:07:42,668 - INFO - joeynmt.training - Validation result (greedy) at epoch  18, step    18500: bleu:  15.76, loss: 410522.3438, ppl:  18.8035, duration: 28.8911s
2021-10-11 22:08:04,206 - INFO - joeynmt.training - Epoch  18: total training loss 54882.46
2021-10-11 22:08:04,207 - INFO - joeynmt.training - EPOCH 19
2021-10-11 22:08:34,505 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:08:34,505 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:08:34,505 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:08:34,548 - INFO - joeynmt.training - Example #0
2021-10-11 22:08:34,548 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:08:34,549 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:08:34,549 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:08:34,549 - INFO - joeynmt.training - Example #1
2021-10-11 22:08:34,549 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:08:34,549 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:08:34,549 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s balloon from the door of the office ."
2021-10-11 22:08:34,549 - INFO - joeynmt.training - Example #2
2021-10-11 22:08:34,549 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:08:34,549 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:08:34,550 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my neighborhood radio .
2021-10-11 22:08:34,550 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    18600: bleu:  15.78, loss: 409406.0312, ppl:  18.6541, duration: 28.9006s
2021-10-11 22:09:25,271 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:09:25,272 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:09:25,272 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:09:25,316 - INFO - joeynmt.training - Example #0
2021-10-11 22:09:25,316 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:09:25,316 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:09:25,316 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:09:25,316 - INFO - joeynmt.training - Example #1
2021-10-11 22:09:25,317 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:09:25,317 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:09:25,317 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s brush from the door of the office of the house ."
2021-10-11 22:09:25,317 - INFO - joeynmt.training - Example #2
2021-10-11 22:09:25,317 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:09:25,317 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:09:25,317 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio station .
2021-10-11 22:09:25,317 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    18700: bleu:  15.66, loss: 410231.8750, ppl:  18.7645, duration: 27.8891s
2021-10-11 22:10:16,170 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:10:16,170 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:10:16,170 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:10:16,220 - INFO - joeynmt.training - Example #0
2021-10-11 22:10:16,220 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:10:16,220 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:10:16,220 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:10:16,220 - INFO - joeynmt.training - Example #1
2021-10-11 22:10:16,221 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:10:16,221 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:10:16,221 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s brush from the door of the background ."
2021-10-11 22:10:16,221 - INFO - joeynmt.training - Example #2
2021-10-11 22:10:16,221 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:10:16,221 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:10:16,221 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio script .
2021-10-11 22:10:16,221 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    18800: bleu:  15.59, loss: 409592.5312, ppl:  18.6789, duration: 28.2734s
2021-10-11 22:11:10,174 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:11:10,174 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:11:10,174 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:11:10,218 - INFO - joeynmt.training - Example #0
2021-10-11 22:11:10,218 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:11:10,218 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:11:10,218 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:11:10,218 - INFO - joeynmt.training - Example #1
2021-10-11 22:11:10,219 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:11:10,219 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:11:10,219 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s law from the door of the office ."
2021-10-11 22:11:10,219 - INFO - joeynmt.training - Example #2
2021-10-11 22:11:10,219 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:11:10,219 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:11:10,219 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to BBC to my small radio policy .
2021-10-11 22:11:10,219 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    18900: bleu:  15.72, loss: 410001.8438, ppl:  18.7337, duration: 31.3531s
2021-10-11 22:11:32,997 - INFO - joeynmt.training - Epoch  19, Step:    19000, Batch Loss:    56.832424, Tokens per Sec:     7989, Lr: 0.000075
2021-10-11 22:12:02,234 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:12:02,234 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:12:02,235 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:12:02,283 - INFO - joeynmt.training - Example #0
2021-10-11 22:12:02,284 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:12:02,284 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:12:02,284 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:12:02,284 - INFO - joeynmt.training - Example #1
2021-10-11 22:12:02,284 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:12:02,285 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:12:02,285 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the revenue of the house ."
2021-10-11 22:12:02,285 - INFO - joeynmt.training - Example #2
2021-10-11 22:12:02,285 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:12:02,285 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:12:02,285 - INFO - joeynmt.training - 	Hypothesis: My father listened to BBC to my small radio wave .
2021-10-11 22:12:02,285 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    19000: bleu:  15.55, loss: 409731.4062, ppl:  18.6975, duration: 29.2874s
2021-10-11 22:12:52,905 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:12:52,905 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:12:52,905 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:12:52,952 - INFO - joeynmt.training - Example #0
2021-10-11 22:12:52,953 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:12:52,953 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:12:52,953 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:12:52,953 - INFO - joeynmt.training - Example #1
2021-10-11 22:12:52,953 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:12:52,953 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:12:52,953 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the door of my home ."
2021-10-11 22:12:52,953 - INFO - joeynmt.training - Example #2
2021-10-11 22:12:52,954 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:12:52,954 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:12:52,954 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio wave .
2021-10-11 22:12:52,954 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    19100: bleu:  15.91, loss: 410109.7812, ppl:  18.7481, duration: 28.0325s
2021-10-11 22:13:44,491 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:13:44,492 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:13:44,492 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:13:44,536 - INFO - joeynmt.training - Example #0
2021-10-11 22:13:44,537 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:13:44,537 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:13:44,537 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:13:44,537 - INFO - joeynmt.training - Example #1
2021-10-11 22:13:44,537 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:13:44,537 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:13:44,537 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my house ."
2021-10-11 22:13:44,537 - INFO - joeynmt.training - Example #2
2021-10-11 22:13:44,538 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:13:44,538 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:13:44,538 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 22:13:44,538 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    19200: bleu:  15.99, loss: 410053.1562, ppl:  18.7405, duration: 28.8052s
2021-10-11 22:14:35,565 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:14:35,566 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:14:35,566 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:14:35,611 - INFO - joeynmt.training - Example #0
2021-10-11 22:14:35,611 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:14:35,611 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:14:35,611 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:14:35,611 - INFO - joeynmt.training - Example #1
2021-10-11 22:14:35,612 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:14:35,612 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:14:35,612 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my house ."
2021-10-11 22:14:35,612 - INFO - joeynmt.training - Example #2
2021-10-11 22:14:35,612 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:14:35,612 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:14:35,612 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio waves .
2021-10-11 22:14:35,612 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    19300: bleu:  15.62, loss: 409492.6875, ppl:  18.6656, duration: 28.3071s
2021-10-11 22:15:25,989 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:15:25,990 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:15:25,990 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:15:26,035 - INFO - joeynmt.training - Example #0
2021-10-11 22:15:26,035 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:15:26,035 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:15:26,036 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:15:26,036 - INFO - joeynmt.training - Example #1
2021-10-11 22:15:26,036 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:15:26,036 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:15:26,036 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the office ."
2021-10-11 22:15:26,036 - INFO - joeynmt.training - Example #2
2021-10-11 22:15:26,036 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:15:26,036 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:15:26,036 - INFO - joeynmt.training - 	Hypothesis: My father listening to the news BBC to my small radio policy .
2021-10-11 22:15:26,036 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    19400: bleu:  15.84, loss: 409594.0312, ppl:  18.6791, duration: 27.8474s
2021-10-11 22:16:16,721 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:16:16,722 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:16:16,722 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:16:16,768 - INFO - joeynmt.training - Example #0
2021-10-11 22:16:16,768 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:16:16,769 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:16:16,769 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:16:16,769 - INFO - joeynmt.training - Example #1
2021-10-11 22:16:16,769 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:16:16,769 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:16:16,769 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office office ."
2021-10-11 22:16:16,769 - INFO - joeynmt.training - Example #2
2021-10-11 22:16:16,769 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:16:16,770 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:16:16,770 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio waves .
2021-10-11 22:16:16,770 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    19500: bleu:  15.75, loss: 410038.8125, ppl:  18.7386, duration: 28.0634s
2021-10-11 22:17:07,986 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:17:07,986 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:17:07,986 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:17:08,031 - INFO - joeynmt.training - Example #0
2021-10-11 22:17:08,031 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:17:08,032 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:17:08,032 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:17:08,032 - INFO - joeynmt.training - Example #1
2021-10-11 22:17:08,032 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:17:08,032 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:17:08,032 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the office ."
2021-10-11 22:17:08,032 - INFO - joeynmt.training - Example #2
2021-10-11 22:17:08,032 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:17:08,033 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:17:08,033 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio waves .
2021-10-11 22:17:08,033 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step    19600: bleu:  15.86, loss: 408719.8750, ppl:  18.5628, duration: 28.5870s
2021-10-11 22:17:15,011 - INFO - joeynmt.training - Epoch  19: total training loss 54329.65
2021-10-11 22:17:15,012 - INFO - joeynmt.training - EPOCH 20
2021-10-11 22:17:59,466 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:17:59,466 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:17:59,466 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:17:59,512 - INFO - joeynmt.training - Example #0
2021-10-11 22:17:59,512 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:17:59,512 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:17:59,512 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:17:59,512 - INFO - joeynmt.training - Example #1
2021-10-11 22:17:59,513 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:17:59,513 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:17:59,513 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my home ."
2021-10-11 22:17:59,513 - INFO - joeynmt.training - Example #2
2021-10-11 22:17:59,513 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:17:59,513 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:17:59,513 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio wave .
2021-10-11 22:17:59,513 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    19700: bleu:  15.82, loss: 408404.8750, ppl:  18.5211, duration: 28.9351s
2021-10-11 22:18:50,288 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:18:50,288 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:18:50,289 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:18:50,333 - INFO - joeynmt.training - Example #0
2021-10-11 22:18:50,334 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:18:50,334 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:18:50,334 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:18:50,335 - INFO - joeynmt.training - Example #1
2021-10-11 22:18:50,335 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:18:50,335 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:18:50,335 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office ."
2021-10-11 22:18:50,336 - INFO - joeynmt.training - Example #2
2021-10-11 22:18:50,336 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:18:50,336 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:18:50,336 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio wave .
2021-10-11 22:18:50,337 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    19800: bleu:  15.85, loss: 408403.0938, ppl:  18.5208, duration: 27.9794s
2021-10-11 22:19:43,713 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:19:43,713 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:19:43,713 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:19:43,757 - INFO - joeynmt.training - Example #0
2021-10-11 22:19:43,758 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:19:43,758 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:19:43,758 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:19:43,758 - INFO - joeynmt.training - Example #1
2021-10-11 22:19:43,758 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:19:43,758 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:19:43,758 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office ."
2021-10-11 22:19:43,758 - INFO - joeynmt.training - Example #2
2021-10-11 22:19:43,759 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:19:43,759 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:19:43,759 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio waves .
2021-10-11 22:19:43,759 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    19900: bleu:  15.75, loss: 408451.0938, ppl:  18.5272, duration: 28.3411s
2021-10-11 22:20:06,120 - INFO - joeynmt.training - Epoch  20, Step:    20000, Batch Loss:    53.843060, Tokens per Sec:     7902, Lr: 0.000037
2021-10-11 22:20:35,016 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:20:35,016 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:20:35,016 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:20:35,059 - INFO - joeynmt.training - Example #0
2021-10-11 22:20:35,059 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:20:35,060 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:20:35,060 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:20:35,060 - INFO - joeynmt.training - Example #1
2021-10-11 22:20:35,060 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:20:35,060 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:20:35,060 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my home ."
2021-10-11 22:20:35,060 - INFO - joeynmt.training - Example #2
2021-10-11 22:20:35,061 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:20:35,061 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:20:35,061 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio waves .
2021-10-11 22:20:35,061 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    20000: bleu:  15.68, loss: 407551.1875, ppl:  18.4084, duration: 28.9397s
2021-10-11 22:21:26,203 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:21:26,203 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:21:26,203 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:21:27,044 - INFO - joeynmt.training - Example #0
2021-10-11 22:21:27,045 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:21:27,045 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:21:27,045 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:21:27,046 - INFO - joeynmt.training - Example #1
2021-10-11 22:21:27,046 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:21:27,046 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:21:27,046 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my home ."
2021-10-11 22:21:27,046 - INFO - joeynmt.training - Example #2
2021-10-11 22:21:27,047 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:21:27,047 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:21:27,047 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio wave .
2021-10-11 22:21:27,047 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    20100: bleu:  15.70, loss: 408094.1250, ppl:  18.4800, duration: 29.1917s
2021-10-11 22:22:42,080 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:22:42,081 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:22:42,081 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:22:42,468 - INFO - joeynmt.training - Example #0
2021-10-11 22:22:42,469 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:22:42,469 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:22:42,469 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:22:42,469 - INFO - joeynmt.training - Example #1
2021-10-11 22:22:42,469 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:22:42,469 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:22:42,470 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my home ."
2021-10-11 22:22:42,470 - INFO - joeynmt.training - Example #2
2021-10-11 22:22:42,470 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:22:42,470 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:22:42,470 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 22:22:42,470 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    20200: bleu:  15.90, loss: 408249.6562, ppl:  18.5005, duration: 28.3781s
2021-10-11 22:23:35,986 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:23:35,986 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:23:35,986 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:23:36,030 - INFO - joeynmt.training - Example #0
2021-10-11 22:23:36,030 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:23:36,030 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:23:36,030 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:23:36,030 - INFO - joeynmt.training - Example #1
2021-10-11 22:23:36,031 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:23:36,031 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:23:36,031 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of my home ."
2021-10-11 22:23:36,031 - INFO - joeynmt.training - Example #2
2021-10-11 22:23:36,031 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:23:36,031 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:23:36,031 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio wave .
2021-10-11 22:23:36,031 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    20300: bleu:  15.64, loss: 408020.8750, ppl:  18.4703, duration: 28.5585s
2021-10-11 22:24:27,036 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:24:27,036 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:24:27,036 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:24:27,078 - INFO - joeynmt.training - Example #0
2021-10-11 22:24:27,078 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:24:27,078 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:24:27,078 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:24:27,078 - INFO - joeynmt.training - Example #1
2021-10-11 22:24:27,078 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:24:27,079 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:24:27,079 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father &apos;s lab from the door of the office of the office ."
2021-10-11 22:24:27,079 - INFO - joeynmt.training - Example #2
2021-10-11 22:24:27,079 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:24:27,079 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:24:27,079 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC on my little radio radio .
2021-10-11 22:24:27,079 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    20400: bleu:  15.71, loss: 407897.4062, ppl:  18.4540, duration: 28.5849s
2021-10-11 22:25:21,002 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:25:21,002 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:25:21,002 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:25:21,052 - INFO - joeynmt.training - Example #0
2021-10-11 22:25:21,053 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:25:21,053 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:25:21,053 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:25:21,053 - INFO - joeynmt.training - Example #1
2021-10-11 22:25:21,053 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:25:21,053 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:25:21,053 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped from the door of the office of the house ."
2021-10-11 22:25:21,054 - INFO - joeynmt.training - Example #2
2021-10-11 22:25:21,054 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:25:21,054 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:25:21,054 - INFO - joeynmt.training - 	Hypothesis: My father listen listening to the news BBC to my small radio policy .
2021-10-11 22:25:21,054 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    20500: bleu:  15.63, loss: 407369.0000, ppl:  18.3844, duration: 31.3831s
2021-10-11 22:26:12,844 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:26:12,844 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:26:12,844 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:26:12,890 - INFO - joeynmt.training - Example #0
2021-10-11 22:26:12,891 - INFO - joeynmt.training - 	Source:     ru_r
2021-10-11 22:26:12,891 - INFO - joeynmt.training - 	Reference:  en
2021-10-11 22:26:12,891 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-11 22:26:12,891 - INFO - joeynmt.training - Example #1
2021-10-11 22:26:12,891 - INFO - joeynmt.training - 	Source:     "Kogda mne bylo 11 let , ja pomnju , kak prosnulas' utrom ot radostnyh vozglasov v moem dome ."
2021-10-11 22:26:12,891 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-11 22:26:12,892 - INFO - joeynmt.training - 	Hypothesis: "When I was 11 years old , I remember my father dropped by the door of the office of the house ."
2021-10-11 22:26:12,892 - INFO - joeynmt.training - Example #2
2021-10-11 22:26:12,892 - INFO - joeynmt.training - 	Source:     Moj otets slushal novosti BBC na svoem nebol'shom serom radiopriemnike .
2021-10-11 22:26:12,892 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-11 22:26:12,892 - INFO - joeynmt.training - 	Hypothesis: My father listen to the news BBC to my small radio wave .
2021-10-11 22:26:12,892 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step    20600: bleu:  15.77, loss: 408224.3750, ppl:  18.4972, duration: 28.9564s
2021-10-11 22:26:26,941 - INFO - joeynmt.training - Epoch  20: total training loss 53429.88
2021-10-11 22:26:26,942 - INFO - joeynmt.training - Training ended after  20 epochs.
2021-10-11 22:26:26,942 - INFO - joeynmt.training - Best validation result (greedy) at step    18400:  16.20 eval_metric.
2021-10-11 22:26:26,969 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-11 22:26:26,969 - INFO - joeynmt.prediction - Loading model from models/FINAL_ru_r/18400.ckpt
2021-10-11 22:26:27,284 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-11 22:26:27,807 - INFO - joeynmt.model - Enc-dec model built.
2021-10-11 22:26:27,861 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/ru_r.en/val.bpe.en)...
2021-10-11 22:28:08,023 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:28:08,023 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:28:08,023 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:28:08,064 - INFO - joeynmt.prediction -  dev bleu[13a]:  17.53 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-11 22:28:08,880 - INFO - joeynmt.prediction - Translations saved to: models/FINAL_ru_r/00018400.hyps.dev
2021-10-11 22:28:08,881 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/ru_r.en/test.bpe.en)...
2021-10-11 22:29:57,742 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-11 22:29:57,742 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-11 22:29:57,742 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-11 22:29:57,788 - INFO - joeynmt.prediction - test bleu[13a]:  18.04 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-11 22:29:57,798 - INFO - joeynmt.prediction - Translations saved to: models/FINAL_ru_r/00018400.hyps.test
