2021-10-03 13:11:42,403 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-03 13:11:42,481 - INFO - joeynmt.data - Loading training data...
2021-10-03 13:11:42,585 - INFO - joeynmt.data - Building vocabulary...
2021-10-03 13:11:43,035 - INFO - joeynmt.data - Loading dev data...
2021-10-03 13:11:43,049 - INFO - joeynmt.data - Loading test data...
2021-10-03 13:11:43,060 - INFO - joeynmt.data - Data loaded.
2021-10-03 13:11:43,060 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:11:43,903 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:11:43,918 - INFO - joeynmt.training - Total params: 33662976
2021-10-03 13:11:48,720 - INFO - joeynmt.helpers - cfg.name                           : TRANS_tr_s
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.src                       : tr_s
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.trg                       : en_s
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/tr_s.en_s/train.bpe
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/tr_s.en_s/val.bpe
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/tr_s.en_s/test.bpe
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-03 13:11:48,721 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 10000
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 10000
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/tr_s.en_s/tr_s.en_s.vocab.txt
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/tr_s.en_s/tr_s.en_s.vocab.txt
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.training.patience              : 1
2021-10-03 13:11:48,722 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.epochs                : 100
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/TRANS_tr_s
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 60
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-10-03 13:11:48,723 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3
2021-10-03 13:11:48,724 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - Data set sizes: 
	train 5177,
	valid 551,
	test 602
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - First training example:
	[SRC] tr
	[TRG] en
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) a (9) &quot;
2021-10-03 13:11:48,725 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) a (9) &quot;
2021-10-03 13:11:48,726 - INFO - joeynmt.helpers - Number of Src words (types): 4136
2021-10-03 13:11:48,726 - INFO - joeynmt.helpers - Number of Trg words (types): 4136
2021-10-03 13:11:48,726 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4136),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4136))
2021-10-03 13:11:48,736 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-03 13:11:48,736 - INFO - joeynmt.training - EPOCH 1
2021-10-03 13:12:02,235 - INFO - joeynmt.training - Epoch   1: total training loss 9118.47
2021-10-03 13:12:02,236 - INFO - joeynmt.training - EPOCH 2
2021-10-03 13:12:15,271 - INFO - joeynmt.training - Epoch   2: total training loss 8389.75
2021-10-03 13:12:15,271 - INFO - joeynmt.training - EPOCH 3
2021-10-03 13:12:27,716 - INFO - joeynmt.training - Epoch   3: total training loss 8100.36
2021-10-03 13:12:27,717 - INFO - joeynmt.training - EPOCH 4
2021-10-03 13:12:40,049 - INFO - joeynmt.training - Epoch   4: total training loss 7760.59
2021-10-03 13:12:40,050 - INFO - joeynmt.training - EPOCH 5
2021-10-03 13:12:53,071 - INFO - joeynmt.training - Epoch   5: total training loss 7445.67
2021-10-03 13:12:53,072 - INFO - joeynmt.training - EPOCH 6
2021-10-03 13:13:05,592 - INFO - joeynmt.training - Epoch   6: total training loss 7209.70
2021-10-03 13:13:05,593 - INFO - joeynmt.training - EPOCH 7
2021-10-03 13:13:18,096 - INFO - joeynmt.training - Epoch   7: total training loss 7014.92
2021-10-03 13:13:18,096 - INFO - joeynmt.training - EPOCH 8
2021-10-03 13:13:30,451 - INFO - joeynmt.training - Epoch   8: total training loss 6844.82
2021-10-03 13:13:30,452 - INFO - joeynmt.training - EPOCH 9
2021-10-03 13:13:42,846 - INFO - joeynmt.training - Epoch   9: total training loss 6683.83
2021-10-03 13:13:42,847 - INFO - joeynmt.training - EPOCH 10
2021-10-03 13:13:55,225 - INFO - joeynmt.training - Epoch  10: total training loss 6534.57
2021-10-03 13:13:55,226 - INFO - joeynmt.training - EPOCH 11
2021-10-03 13:14:07,559 - INFO - joeynmt.training - Epoch  11: total training loss 6372.12
2021-10-03 13:14:07,560 - INFO - joeynmt.training - EPOCH 12
2021-10-03 13:14:20,432 - INFO - joeynmt.training - Epoch  12: total training loss 6219.37
2021-10-03 13:14:20,432 - INFO - joeynmt.training - EPOCH 13
2021-10-03 13:14:33,551 - INFO - joeynmt.training - Epoch  13: total training loss 6071.50
2021-10-03 13:14:33,551 - INFO - joeynmt.training - EPOCH 14
2021-10-03 13:14:47,074 - INFO - joeynmt.training - Epoch  14: total training loss 5926.63
2021-10-03 13:14:47,074 - INFO - joeynmt.training - EPOCH 15
2021-10-03 13:15:00,588 - INFO - joeynmt.training - Epoch  15: total training loss 5774.63
2021-10-03 13:15:00,589 - INFO - joeynmt.training - EPOCH 16
2021-10-03 13:15:05,349 - INFO - joeynmt.training - Epoch  16, Step:     1000, Batch Loss:    86.710892, Tokens per Sec:     8998, Lr: 0.000300
2021-10-03 13:15:17,679 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:15:17,679 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:15:17,679 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:15:17,683 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:15:19,150 - INFO - joeynmt.training - Example #0
2021-10-03 13:15:19,151 - INFO - joeynmt.training - 	Source:     tr
2021-10-03 13:15:19,151 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:15:19,151 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:15:19,151 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u015f' in position 86: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['1@@', '1', 'ya\u015f@@', '\u0131nday@@', 'ken', 'bir', 'sa@@', 'ba@@', 'h', 'ev@@', 'im@@', 'deki', 'sev@@', 'in@@', 'ç', 'ç@@', '\u0131\u011f@@', 'l\u0131@@', 'klar@@', '\u0131yla', 'u@@', 'yan@@', 'd\u0131\u011f@@', '\u0131m\u0131', 'hat\u0131r@@', 'l\u0131@@', 'yorum', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 70-71: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .',)
2021-10-03 13:15:19,156 - INFO - joeynmt.training - 	Source:     11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .
2021-10-03 13:15:19,157 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:15:19,157 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to get a cart of a man and I &apos;m going to get a cart .
2021-10-03 13:15:19,157 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 151: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"B@@', 'ab@@', 'am', 'B@@', 'B@@', 'C', 'H@@', 'ab@@', 'er', 'kan@@', 'al@@', '\u0131n\u0131', 'din@@', 'li@@', 'yordu', ';', 'o', 'uf@@', 'ak', ',', 'gr@@', 'i', 'rad@@', 'y@@', 'os@@', 'undan', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 87: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."',)
2021-10-03 13:15:19,158 - INFO - joeynmt.training - 	Source:     "Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."
2021-10-03 13:15:19,159 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:15:19,159 - INFO - joeynmt.training - 	Hypothesis: "In 191919196,000 , the Ho-..I, it &apos;s a very different ."
2021-10-03 13:15:19,159 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     1000: bleu:   5.00, loss: 73561.0469, ppl: 127.0578, duration: 13.8094s
2021-10-03 13:15:27,500 - INFO - joeynmt.training - Epoch  16: total training loss 5625.55
2021-10-03 13:15:27,500 - INFO - joeynmt.training - EPOCH 17
2021-10-03 13:15:40,977 - INFO - joeynmt.training - Epoch  17: total training loss 5475.26
2021-10-03 13:15:40,978 - INFO - joeynmt.training - EPOCH 18
2021-10-03 13:15:53,373 - INFO - joeynmt.training - Epoch  18: total training loss 5325.80
2021-10-03 13:15:53,373 - INFO - joeynmt.training - EPOCH 19
2021-10-03 13:16:05,772 - INFO - joeynmt.training - Epoch  19: total training loss 5174.02
2021-10-03 13:16:05,773 - INFO - joeynmt.training - EPOCH 20
2021-10-03 13:16:18,196 - INFO - joeynmt.training - Epoch  20: total training loss 5032.12
2021-10-03 13:16:18,196 - INFO - joeynmt.training - EPOCH 21
2021-10-03 13:16:31,789 - INFO - joeynmt.training - Epoch  21: total training loss 4891.08
2021-10-03 13:16:31,789 - INFO - joeynmt.training - EPOCH 22
2021-10-03 13:16:44,707 - INFO - joeynmt.training - Epoch  22: total training loss 4759.04
2021-10-03 13:16:44,707 - INFO - joeynmt.training - EPOCH 23
2021-10-03 13:16:57,048 - INFO - joeynmt.training - Epoch  23: total training loss 4628.98
2021-10-03 13:16:57,049 - INFO - joeynmt.training - EPOCH 24
2021-10-03 13:17:09,897 - INFO - joeynmt.training - Epoch  24: total training loss 4497.84
2021-10-03 13:17:09,897 - INFO - joeynmt.training - EPOCH 25
2021-10-03 13:17:22,869 - INFO - joeynmt.training - Epoch  25: total training loss 4380.86
2021-10-03 13:17:22,869 - INFO - joeynmt.training - EPOCH 26
2021-10-03 13:17:36,439 - INFO - joeynmt.training - Epoch  26: total training loss 4261.70
2021-10-03 13:17:36,439 - INFO - joeynmt.training - EPOCH 27
2021-10-03 13:17:49,013 - INFO - joeynmt.training - Epoch  27: total training loss 4132.66
2021-10-03 13:17:49,013 - INFO - joeynmt.training - EPOCH 28
2021-10-03 13:18:01,463 - INFO - joeynmt.training - Epoch  28: total training loss 4020.83
2021-10-03 13:18:01,463 - INFO - joeynmt.training - EPOCH 29
2021-10-03 13:18:14,464 - INFO - joeynmt.training - Epoch  29: total training loss 3899.25
2021-10-03 13:18:14,465 - INFO - joeynmt.training - EPOCH 30
2021-10-03 13:18:27,626 - INFO - joeynmt.training - Epoch  30: total training loss 3793.97
2021-10-03 13:18:27,627 - INFO - joeynmt.training - EPOCH 31
2021-10-03 13:18:37,657 - INFO - joeynmt.training - Epoch  31, Step:     2000, Batch Loss:    57.849880, Tokens per Sec:     8561, Lr: 0.000300
2021-10-03 13:18:47,641 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:18:47,641 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:18:47,641 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:18:47,646 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:18:49,169 - INFO - joeynmt.helpers - delete models/TRANS_tr_s/1000.ckpt
2021-10-03 13:18:49,278 - INFO - joeynmt.helpers - delete /home/lcur0007/joeynmt/models/TRANS_tr_s/1000.ckpt
2021-10-03 13:18:49,279 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0007/joeynmt/models/TRANS_tr_s/1000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0007/joeynmt/models/TRANS_tr_s/1000.ckpt')
2021-10-03 13:18:49,281 - INFO - joeynmt.training - Example #0
2021-10-03 13:18:49,281 - INFO - joeynmt.training - 	Source:     tr
2021-10-03 13:18:49,281 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:18:49,281 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:18:49,281 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u015f' in position 86: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['1@@', '1', 'ya\u015f@@', '\u0131nday@@', 'ken', 'bir', 'sa@@', 'ba@@', 'h', 'ev@@', 'im@@', 'deki', 'sev@@', 'in@@', 'ç', 'ç@@', '\u0131\u011f@@', 'l\u0131@@', 'klar@@', '\u0131yla', 'u@@', 'yan@@', 'd\u0131\u011f@@', '\u0131m\u0131', 'hat\u0131r@@', 'l\u0131@@', 'yorum', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 70-71: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .',)
2021-10-03 13:18:49,284 - INFO - joeynmt.training - 	Source:     11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .
2021-10-03 13:18:49,284 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:18:49,285 - INFO - joeynmt.training - 	Hypothesis: I was in a golfer in my wife I was in my wife . I can &apos;t know my own .
2021-10-03 13:18:49,285 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 151: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"B@@', 'ab@@', 'am', 'B@@', 'B@@', 'C', 'H@@', 'ab@@', 'er', 'kan@@', 'al@@', '\u0131n\u0131', 'din@@', 'li@@', 'yordu', ';', 'o', 'uf@@', 'ak', ',', 'gr@@', 'i', 'rad@@', 'y@@', 'os@@', 'undan', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 87: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."',)
2021-10-03 13:18:49,285 - INFO - joeynmt.training - 	Source:     "Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."
2021-10-03 13:18:49,285 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:18:49,285 - INFO - joeynmt.training - 	Hypothesis: "Newo Shanghai was , Shanghai was decade , but a lot of reasons ."
2021-10-03 13:18:49,285 - INFO - joeynmt.training - Validation result (greedy) at epoch  31, step     2000: bleu:   6.80, loss: 72505.6719, ppl: 118.5265, duration: 11.6276s
2021-10-03 13:18:52,199 - INFO - joeynmt.training - Epoch  31: total training loss 3688.14
2021-10-03 13:18:52,199 - INFO - joeynmt.training - EPOCH 32
2021-10-03 13:19:05,344 - INFO - joeynmt.training - Epoch  32: total training loss 3576.85
2021-10-03 13:19:05,344 - INFO - joeynmt.training - EPOCH 33
2021-10-03 13:19:17,991 - INFO - joeynmt.training - Epoch  33: total training loss 3465.94
2021-10-03 13:19:17,991 - INFO - joeynmt.training - EPOCH 34
2021-10-03 13:19:30,939 - INFO - joeynmt.training - Epoch  34: total training loss 3362.93
2021-10-03 13:19:30,939 - INFO - joeynmt.training - EPOCH 35
2021-10-03 13:19:43,376 - INFO - joeynmt.training - Epoch  35: total training loss 3257.55
2021-10-03 13:19:43,376 - INFO - joeynmt.training - EPOCH 36
2021-10-03 13:19:55,770 - INFO - joeynmt.training - Epoch  36: total training loss 3158.61
2021-10-03 13:19:55,770 - INFO - joeynmt.training - EPOCH 37
2021-10-03 13:20:08,191 - INFO - joeynmt.training - Epoch  37: total training loss 3070.86
2021-10-03 13:20:08,192 - INFO - joeynmt.training - EPOCH 38
2021-10-03 13:20:20,575 - INFO - joeynmt.training - Epoch  38: total training loss 2954.25
2021-10-03 13:20:20,575 - INFO - joeynmt.training - EPOCH 39
2021-10-03 13:20:32,926 - INFO - joeynmt.training - Epoch  39: total training loss 2865.03
2021-10-03 13:20:32,926 - INFO - joeynmt.training - EPOCH 40
2021-10-03 13:20:45,276 - INFO - joeynmt.training - Epoch  40: total training loss 2773.83
2021-10-03 13:20:45,277 - INFO - joeynmt.training - EPOCH 41
2021-10-03 13:20:57,683 - INFO - joeynmt.training - Epoch  41: total training loss 2677.75
2021-10-03 13:20:57,684 - INFO - joeynmt.training - EPOCH 42
2021-10-03 13:21:10,827 - INFO - joeynmt.training - Epoch  42: total training loss 2585.45
2021-10-03 13:21:10,827 - INFO - joeynmt.training - EPOCH 43
2021-10-03 13:21:23,097 - INFO - joeynmt.training - Epoch  43: total training loss 2504.53
2021-10-03 13:21:23,097 - INFO - joeynmt.training - EPOCH 44
2021-10-03 13:21:35,398 - INFO - joeynmt.training - Epoch  44: total training loss 2420.62
2021-10-03 13:21:35,399 - INFO - joeynmt.training - EPOCH 45
2021-10-03 13:21:47,706 - INFO - joeynmt.training - Epoch  45: total training loss 2327.91
2021-10-03 13:21:47,707 - INFO - joeynmt.training - EPOCH 46
2021-10-03 13:22:00,079 - INFO - joeynmt.training - Epoch  46: total training loss 2243.38
2021-10-03 13:22:00,079 - INFO - joeynmt.training - EPOCH 47
2021-10-03 13:22:02,014 - INFO - joeynmt.training - Epoch  47, Step:     3000, Batch Loss:    29.521429, Tokens per Sec:     8829, Lr: 0.000300
2021-10-03 13:22:11,562 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:22:11,562 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:22:11,562 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:22:11,567 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:22:12,853 - INFO - joeynmt.helpers - delete models/TRANS_tr_s/2000.ckpt
2021-10-03 13:22:12,956 - INFO - joeynmt.helpers - delete /home/lcur0007/joeynmt/models/TRANS_tr_s/2000.ckpt
2021-10-03 13:22:12,956 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0007/joeynmt/models/TRANS_tr_s/2000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0007/joeynmt/models/TRANS_tr_s/2000.ckpt')
2021-10-03 13:22:12,959 - INFO - joeynmt.training - Example #0
2021-10-03 13:22:12,959 - INFO - joeynmt.training - 	Source:     tr
2021-10-03 13:22:12,959 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:22:12,959 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:22:12,959 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u015f' in position 86: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['1@@', '1', 'ya\u015f@@', '\u0131nday@@', 'ken', 'bir', 'sa@@', 'ba@@', 'h', 'ev@@', 'im@@', 'deki', 'sev@@', 'in@@', 'ç', 'ç@@', '\u0131\u011f@@', 'l\u0131@@', 'klar@@', '\u0131yla', 'u@@', 'yan@@', 'd\u0131\u011f@@', '\u0131m\u0131', 'hat\u0131r@@', 'l\u0131@@', 'yorum', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 70-71: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .',)
2021-10-03 13:22:12,961 - INFO - joeynmt.training - 	Source:     11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .
2021-10-03 13:22:12,961 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:22:12,962 - INFO - joeynmt.training - 	Hypothesis: "And I was in fact , I would be career at my own stage fascinated by a dence ."
2021-10-03 13:22:12,962 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 151: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"B@@', 'ab@@', 'am', 'B@@', 'B@@', 'C', 'H@@', 'ab@@', 'er', 'kan@@', 'al@@', '\u0131n\u0131', 'din@@', 'li@@', 'yordu', ';', 'o', 'uf@@', 'ak', ',', 'gr@@', 'i', 'rad@@', 'y@@', 'os@@', 'undan', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 87: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."',)
2021-10-03 13:22:12,962 - INFO - joeynmt.training - 	Source:     "Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."
2021-10-03 13:22:12,962 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:22:12,962 - INFO - joeynmt.training - 	Hypothesis: "Newo Haper was , as Britney , as a number of warps on the same as brium card ."
2021-10-03 13:22:12,963 - INFO - joeynmt.training - Validation result (greedy) at epoch  47, step     3000: bleu:   7.11, loss: 81679.1172, ppl: 216.8678, duration: 10.9476s
2021-10-03 13:22:23,397 - INFO - joeynmt.training - Epoch  47: total training loss 2178.98
2021-10-03 13:22:23,397 - INFO - joeynmt.training - EPOCH 48
2021-10-03 13:22:36,493 - INFO - joeynmt.training - Epoch  48: total training loss 2099.89
2021-10-03 13:22:36,494 - INFO - joeynmt.training - EPOCH 49
2021-10-03 13:22:49,066 - INFO - joeynmt.training - Epoch  49: total training loss 2019.50
2021-10-03 13:22:49,067 - INFO - joeynmt.training - EPOCH 50
2021-10-03 13:23:01,349 - INFO - joeynmt.training - Epoch  50: total training loss 1950.15
2021-10-03 13:23:01,349 - INFO - joeynmt.training - EPOCH 51
2021-10-03 13:23:13,687 - INFO - joeynmt.training - Epoch  51: total training loss 1879.23
2021-10-03 13:23:13,688 - INFO - joeynmt.training - EPOCH 52
2021-10-03 13:23:26,468 - INFO - joeynmt.training - Epoch  52: total training loss 1800.53
2021-10-03 13:23:26,468 - INFO - joeynmt.training - EPOCH 53
2021-10-03 13:23:39,270 - INFO - joeynmt.training - Epoch  53: total training loss 1741.98
2021-10-03 13:23:39,271 - INFO - joeynmt.training - EPOCH 54
2021-10-03 13:23:51,633 - INFO - joeynmt.training - Epoch  54: total training loss 1678.76
2021-10-03 13:23:51,633 - INFO - joeynmt.training - EPOCH 55
2021-10-03 13:24:04,017 - INFO - joeynmt.training - Epoch  55: total training loss 1626.88
2021-10-03 13:24:04,017 - INFO - joeynmt.training - EPOCH 56
2021-10-03 13:24:16,822 - INFO - joeynmt.training - Epoch  56: total training loss 1566.71
2021-10-03 13:24:16,823 - INFO - joeynmt.training - EPOCH 57
2021-10-03 13:24:29,692 - INFO - joeynmt.training - Epoch  57: total training loss 1499.62
2021-10-03 13:24:29,693 - INFO - joeynmt.training - EPOCH 58
2021-10-03 13:24:42,444 - INFO - joeynmt.training - Epoch  58: total training loss 1451.66
2021-10-03 13:24:42,445 - INFO - joeynmt.training - EPOCH 59
2021-10-03 13:24:55,464 - INFO - joeynmt.training - Epoch  59: total training loss 1392.27
2021-10-03 13:24:55,464 - INFO - joeynmt.training - EPOCH 60
2021-10-03 13:25:07,746 - INFO - joeynmt.training - Epoch  60: total training loss 1344.22
2021-10-03 13:25:07,746 - INFO - joeynmt.training - EPOCH 61
2021-10-03 13:25:20,434 - INFO - joeynmt.training - Epoch  61: total training loss 1297.13
2021-10-03 13:25:20,434 - INFO - joeynmt.training - EPOCH 62
2021-10-03 13:25:27,718 - INFO - joeynmt.training - Epoch  62, Step:     4000, Batch Loss:    19.124073, Tokens per Sec:     8271, Lr: 0.000300
2021-10-03 13:25:36,813 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:25:36,814 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:25:36,814 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:25:36,818 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:25:38,082 - INFO - joeynmt.helpers - delete models/TRANS_tr_s/3000.ckpt
2021-10-03 13:25:38,185 - INFO - joeynmt.helpers - delete /home/lcur0007/joeynmt/models/TRANS_tr_s/3000.ckpt
2021-10-03 13:25:38,185 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0007/joeynmt/models/TRANS_tr_s/3000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0007/joeynmt/models/TRANS_tr_s/3000.ckpt')
2021-10-03 13:25:38,187 - INFO - joeynmt.training - Example #0
2021-10-03 13:25:38,187 - INFO - joeynmt.training - 	Source:     tr
2021-10-03 13:25:38,188 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:25:38,188 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:25:38,188 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u015f' in position 86: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['1@@', '1', 'ya\u015f@@', '\u0131nday@@', 'ken', 'bir', 'sa@@', 'ba@@', 'h', 'ev@@', 'im@@', 'deki', 'sev@@', 'in@@', 'ç', 'ç@@', '\u0131\u011f@@', 'l\u0131@@', 'klar@@', '\u0131yla', 'u@@', 'yan@@', 'd\u0131\u011f@@', '\u0131m\u0131', 'hat\u0131r@@', 'l\u0131@@', 'yorum', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 70-71: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .',)
2021-10-03 13:25:38,190 - INFO - joeynmt.training - 	Source:     11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .
2021-10-03 13:25:38,190 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:25:38,190 - INFO - joeynmt.training - 	Hypothesis: "When I was a stage fold , I fires in my mother , I can get a nine ."
2021-10-03 13:25:38,190 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 151: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"B@@', 'ab@@', 'am', 'B@@', 'B@@', 'C', 'H@@', 'ab@@', 'er', 'kan@@', 'al@@', '\u0131n\u0131', 'din@@', 'li@@', 'yordu', ';', 'o', 'uf@@', 'ak', ',', 'gr@@', 'i', 'rad@@', 'y@@', 'os@@', 'undan', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 87: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."',)
2021-10-03 13:25:38,191 - INFO - joeynmt.training - 	Source:     "Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."
2021-10-03 13:25:38,191 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:25:38,191 - INFO - joeynmt.training - 	Hypothesis: "Newo Hoshan Award , built the U.S. ; the number of burgh , death ."
2021-10-03 13:25:38,191 - INFO - joeynmt.training - Validation result (greedy) at epoch  62, step     4000: bleu:   7.24, loss: 93786.8203, ppl: 481.3946, duration: 10.4727s
2021-10-03 13:25:44,275 - INFO - joeynmt.training - Epoch  62: total training loss 1251.20
2021-10-03 13:25:44,275 - INFO - joeynmt.training - EPOCH 63
2021-10-03 13:25:56,707 - INFO - joeynmt.training - Epoch  63: total training loss 1204.72
2021-10-03 13:25:56,708 - INFO - joeynmt.training - EPOCH 64
2021-10-03 13:26:09,016 - INFO - joeynmt.training - Epoch  64: total training loss 1162.57
2021-10-03 13:26:09,016 - INFO - joeynmt.training - EPOCH 65
2021-10-03 13:26:21,278 - INFO - joeynmt.training - Epoch  65: total training loss 1125.10
2021-10-03 13:26:21,279 - INFO - joeynmt.training - EPOCH 66
2021-10-03 13:26:34,255 - INFO - joeynmt.training - Epoch  66: total training loss 1090.22
2021-10-03 13:26:34,255 - INFO - joeynmt.training - EPOCH 67
2021-10-03 13:26:46,615 - INFO - joeynmt.training - Epoch  67: total training loss 1051.27
2021-10-03 13:26:46,616 - INFO - joeynmt.training - EPOCH 68
2021-10-03 13:26:59,124 - INFO - joeynmt.training - Epoch  68: total training loss 1016.97
2021-10-03 13:26:59,124 - INFO - joeynmt.training - EPOCH 69
2021-10-03 13:27:11,453 - INFO - joeynmt.training - Epoch  69: total training loss 991.12
2021-10-03 13:27:11,453 - INFO - joeynmt.training - EPOCH 70
2021-10-03 13:27:23,968 - INFO - joeynmt.training - Epoch  70: total training loss 961.98
2021-10-03 13:27:23,968 - INFO - joeynmt.training - EPOCH 71
2021-10-03 13:27:36,543 - INFO - joeynmt.training - Epoch  71: total training loss 923.23
2021-10-03 13:27:36,543 - INFO - joeynmt.training - EPOCH 72
2021-10-03 13:27:48,881 - INFO - joeynmt.training - Epoch  72: total training loss 891.72
2021-10-03 13:27:48,881 - INFO - joeynmt.training - EPOCH 73
2021-10-03 13:28:01,176 - INFO - joeynmt.training - Epoch  73: total training loss 865.10
2021-10-03 13:28:01,176 - INFO - joeynmt.training - EPOCH 74
2021-10-03 13:28:13,595 - INFO - joeynmt.training - Epoch  74: total training loss 833.73
2021-10-03 13:28:13,596 - INFO - joeynmt.training - EPOCH 75
2021-10-03 13:28:26,781 - INFO - joeynmt.training - Epoch  75: total training loss 814.44
2021-10-03 13:28:26,781 - INFO - joeynmt.training - EPOCH 76
2021-10-03 13:28:39,326 - INFO - joeynmt.training - Epoch  76: total training loss 792.07
2021-10-03 13:28:39,327 - INFO - joeynmt.training - EPOCH 77
2021-10-03 13:28:51,398 - INFO - joeynmt.training - Epoch  77, Step:     5000, Batch Loss:    12.803226, Tokens per Sec:     8537, Lr: 0.000300
2021-10-03 13:29:00,187 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:29:00,187 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:29:00,187 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:29:00,194 - INFO - joeynmt.training - Example #0
2021-10-03 13:29:00,194 - INFO - joeynmt.training - 	Source:     tr
2021-10-03 13:29:00,194 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:29:00,194 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:29:00,194 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u015f' in position 86: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['1@@', '1', 'ya\u015f@@', '\u0131nday@@', 'ken', 'bir', 'sa@@', 'ba@@', 'h', 'ev@@', 'im@@', 'deki', 'sev@@', 'in@@', 'ç', 'ç@@', '\u0131\u011f@@', 'l\u0131@@', 'klar@@', '\u0131yla', 'u@@', 'yan@@', 'd\u0131\u011f@@', '\u0131m\u0131', 'hat\u0131r@@', 'l\u0131@@', 'yorum', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 70-71: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .',)
2021-10-03 13:29:00,197 - INFO - joeynmt.training - 	Source:     11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .
2021-10-03 13:29:00,197 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:29:00,197 - INFO - joeynmt.training - 	Hypothesis: "Once I was a time , I could put in fash grade in wealth grades ."
2021-10-03 13:29:00,197 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 151: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"B@@', 'ab@@', 'am', 'B@@', 'B@@', 'C', 'H@@', 'ab@@', 'er', 'kan@@', 'al@@', '\u0131n\u0131', 'din@@', 'li@@', 'yordu', ';', 'o', 'uf@@', 'ak', ',', 'gr@@', 'i', 'rad@@', 'y@@', 'os@@', 'undan', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 87: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."',)
2021-10-03 13:29:00,198 - INFO - joeynmt.training - 	Source:     "Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."
2021-10-03 13:29:00,198 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:29:00,198 - INFO - joeynmt.training - 	Hypothesis: "Neputh Proad , Business Western ball warps , player on the left ."
2021-10-03 13:29:00,198 - INFO - joeynmt.training - Validation result (greedy) at epoch  77, step     5000: bleu:   7.00, loss: 104113.5625, ppl: 950.3153, duration: 8.8001s
2021-10-03 13:29:01,179 - INFO - joeynmt.training - Epoch  77: total training loss 761.45
2021-10-03 13:29:01,180 - INFO - joeynmt.training - EPOCH 78
2021-10-03 13:29:14,069 - INFO - joeynmt.training - Epoch  78: total training loss 745.41
2021-10-03 13:29:14,070 - INFO - joeynmt.training - EPOCH 79
2021-10-03 13:29:27,468 - INFO - joeynmt.training - Epoch  79: total training loss 724.99
2021-10-03 13:29:27,468 - INFO - joeynmt.training - EPOCH 80
2021-10-03 13:29:40,133 - INFO - joeynmt.training - Epoch  80: total training loss 704.06
2021-10-03 13:29:40,134 - INFO - joeynmt.training - EPOCH 81
2021-10-03 13:29:52,677 - INFO - joeynmt.training - Epoch  81: total training loss 685.41
2021-10-03 13:29:52,678 - INFO - joeynmt.training - EPOCH 82
2021-10-03 13:30:05,099 - INFO - joeynmt.training - Epoch  82: total training loss 665.08
2021-10-03 13:30:05,100 - INFO - joeynmt.training - EPOCH 83
2021-10-03 13:30:17,602 - INFO - joeynmt.training - Epoch  83: total training loss 651.85
2021-10-03 13:30:17,602 - INFO - joeynmt.training - EPOCH 84
2021-10-03 13:30:30,004 - INFO - joeynmt.training - Epoch  84: total training loss 620.86
2021-10-03 13:30:30,005 - INFO - joeynmt.training - EPOCH 85
2021-10-03 13:30:42,376 - INFO - joeynmt.training - Epoch  85: total training loss 613.94
2021-10-03 13:30:42,376 - INFO - joeynmt.training - EPOCH 86
2021-10-03 13:30:55,766 - INFO - joeynmt.training - Epoch  86: total training loss 604.80
2021-10-03 13:30:55,766 - INFO - joeynmt.training - EPOCH 87
2021-10-03 13:31:08,778 - INFO - joeynmt.training - Epoch  87: total training loss 587.73
2021-10-03 13:31:08,778 - INFO - joeynmt.training - EPOCH 88
2021-10-03 13:31:21,555 - INFO - joeynmt.training - Epoch  88: total training loss 572.02
2021-10-03 13:31:21,556 - INFO - joeynmt.training - EPOCH 89
2021-10-03 13:31:34,969 - INFO - joeynmt.training - Epoch  89: total training loss 554.43
2021-10-03 13:31:34,969 - INFO - joeynmt.training - EPOCH 90
2021-10-03 13:31:47,484 - INFO - joeynmt.training - Epoch  90: total training loss 548.00
2021-10-03 13:31:47,484 - INFO - joeynmt.training - EPOCH 91
2021-10-03 13:32:00,342 - INFO - joeynmt.training - Epoch  91: total training loss 536.06
2021-10-03 13:32:00,342 - INFO - joeynmt.training - EPOCH 92
2021-10-03 13:32:12,797 - INFO - joeynmt.training - Epoch  92: total training loss 521.15
2021-10-03 13:32:12,797 - INFO - joeynmt.training - EPOCH 93
2021-10-03 13:32:16,561 - INFO - joeynmt.training - Epoch  93, Step:     6000, Batch Loss:     7.075418, Tokens per Sec:     9057, Lr: 0.000300
2021-10-03 13:32:25,716 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:32:25,716 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:32:25,716 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:32:25,723 - INFO - joeynmt.training - Example #0
2021-10-03 13:32:25,723 - INFO - joeynmt.training - 	Source:     tr
2021-10-03 13:32:25,723 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:32:25,723 - INFO - joeynmt.training - 	Hypothesis: en
2021-10-03 13:32:25,723 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u015f' in position 86: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['1@@', '1', 'ya\u015f@@', '\u0131nday@@', 'ken', 'bir', 'sa@@', 'ba@@', 'h', 'ev@@', 'im@@', 'deki', 'sev@@', 'in@@', 'ç', 'ç@@', '\u0131\u011f@@', 'l\u0131@@', 'klar@@', '\u0131yla', 'u@@', 'yan@@', 'd\u0131\u011f@@', '\u0131m\u0131', 'hat\u0131r@@', 'l\u0131@@', 'yorum', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 70-71: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .',)
2021-10-03 13:32:25,727 - INFO - joeynmt.training - 	Source:     11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .
2021-10-03 13:32:25,727 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:32:25,728 - INFO - joeynmt.training - 	Hypothesis: "I was a age of years old , I could spent in my own fassistent ."
2021-10-03 13:32:25,728 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 151: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"B@@', 'ab@@', 'am', 'B@@', 'B@@', 'C', 'H@@', 'ab@@', 'er', 'kan@@', 'al@@', '\u0131n\u0131', 'din@@', 'li@@', 'yordu', ';', 'o', 'uf@@', 'ak', ',', 'gr@@', 'i', 'rad@@', 'y@@', 'os@@', 'undan', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 87: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0007/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0007/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."',)
2021-10-03 13:32:25,729 - INFO - joeynmt.training - 	Source:     "Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."
2021-10-03 13:32:25,729 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:32:25,729 - INFO - joeynmt.training - 	Hypothesis: "Loffo Business Africa , Britrobon benefully , turned up ."
2021-10-03 13:32:25,730 - INFO - joeynmt.training - Validation result (greedy) at epoch  93, step     6000: bleu:   6.86, loss: 111082.0000, ppl: 1503.7643, duration: 9.1685s
2021-10-03 13:32:34,194 - INFO - joeynmt.training - Epoch  93: total training loss 488.22
2021-10-03 13:32:34,194 - INFO - joeynmt.training - EPOCH 94
2021-10-03 13:32:46,689 - INFO - joeynmt.training - Epoch  94: total training loss 430.84
2021-10-03 13:32:46,690 - INFO - joeynmt.training - EPOCH 95
2021-10-03 13:32:59,804 - INFO - joeynmt.training - Epoch  95: total training loss 414.68
2021-10-03 13:32:59,805 - INFO - joeynmt.training - EPOCH 96
2021-10-03 13:33:12,085 - INFO - joeynmt.training - Epoch  96: total training loss 400.56
2021-10-03 13:33:12,085 - INFO - joeynmt.training - EPOCH 97
2021-10-03 13:33:24,349 - INFO - joeynmt.training - Epoch  97: total training loss 386.93
2021-10-03 13:33:24,349 - INFO - joeynmt.training - EPOCH 98
2021-10-03 13:33:36,662 - INFO - joeynmt.training - Epoch  98: total training loss 376.84
2021-10-03 13:33:36,662 - INFO - joeynmt.training - EPOCH 99
2021-10-03 13:33:49,169 - INFO - joeynmt.training - Epoch  99: total training loss 373.06
2021-10-03 13:33:49,169 - INFO - joeynmt.training - EPOCH 100
2021-10-03 13:34:01,683 - INFO - joeynmt.training - Epoch 100: total training loss 367.93
2021-10-03 13:34:01,683 - INFO - joeynmt.training - Training ended after 100 epochs.
2021-10-03 13:34:01,683 - INFO - joeynmt.training - Best validation result (greedy) at step     4000:   7.24 eval_metric.
2021-10-03 13:34:01,712 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-03 13:34:01,713 - INFO - joeynmt.prediction - Loading model from models/TRANS_tr_s/4000.ckpt
2021-10-03 13:34:02,559 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:34:03,365 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:34:03,587 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/tr_s.en_s/val.bpe.en_s)...
2021-10-03 13:34:23,772 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:34:23,772 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:34:23,772 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:34:23,776 - INFO - joeynmt.prediction -  dev bleu[13a]:   7.15 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:34:23,778 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_tr_s/00004000.hyps.dev
2021-10-03 13:34:23,778 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/tr_s.en_s/test.bpe.en_s)...
2021-10-03 13:34:46,923 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:34:46,924 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:34:46,924 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:34:46,928 - INFO - joeynmt.prediction - test bleu[13a]:   8.57 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:34:46,931 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_tr_s/00004000.hyps.test
