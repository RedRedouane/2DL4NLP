2021-10-04 12:48:51,352 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-04 12:48:51,419 - INFO - joeynmt.data - Loading training data...
2021-10-04 12:48:51,523 - INFO - joeynmt.data - Building vocabulary...
2021-10-04 12:48:51,973 - INFO - joeynmt.data - Loading dev data...
2021-10-04 12:48:51,985 - INFO - joeynmt.data - Loading test data...
2021-10-04 12:48:51,997 - INFO - joeynmt.data - Data loaded.
2021-10-04 12:48:51,997 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-04 12:48:52,835 - INFO - joeynmt.model - Enc-dec model built.
2021-10-04 12:48:52,846 - INFO - joeynmt.training - Total params: 33662976
2021-10-04 12:48:57,395 - INFO - joeynmt.helpers - cfg.name                           : TRANS_tr_s_INIT
2021-10-04 12:48:57,396 - INFO - joeynmt.helpers - cfg.data.src                       : tr_s
2021-10-04 12:48:57,396 - INFO - joeynmt.helpers - cfg.data.trg                       : en_s
2021-10-04 12:48:57,396 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/tr_s.en_s/train.bpe
2021-10-04 12:48:57,396 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/tr_s.en_s/val.bpe
2021-10-04 12:48:57,396 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/tr_s.en_s/test.bpe
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 10000
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 10000
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/tr_s.en_s/tr_s.en_s.vocab.txt
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/tr_s.en_s/tr_s.en_s.vocab.txt
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-04 12:48:57,397 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.patience              : 1
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.epochs                : 20
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/TRANS_tr_s_INIT
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 60
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-04 12:48:57,398 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3
2021-10-04 12:48:57,399 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - Data set sizes: 
	train 5177,
	valid 551,
	test 602
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - First training example:
	[SRC] tr
	[TRG] en
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) a (9) &quot;
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) a (9) &quot;
2021-10-04 12:48:57,400 - INFO - joeynmt.helpers - Number of Src words (types): 4136
2021-10-04 12:48:57,401 - INFO - joeynmt.helpers - Number of Trg words (types): 4136
2021-10-04 12:48:57,401 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4136),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4136))
2021-10-04 12:48:57,411 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-04 12:48:57,411 - INFO - joeynmt.training - EPOCH 1
2021-10-04 12:49:09,601 - INFO - joeynmt.training - Epoch   1: total training loss 9118.47
2021-10-04 12:49:09,602 - INFO - joeynmt.training - EPOCH 2
2021-10-04 12:49:22,374 - INFO - joeynmt.training - Epoch   2: total training loss 8389.75
2021-10-04 12:49:22,374 - INFO - joeynmt.training - EPOCH 3
2021-10-04 12:49:35,565 - INFO - joeynmt.training - Epoch   3: total training loss 8100.36
2021-10-04 12:49:35,566 - INFO - joeynmt.training - EPOCH 4
2021-10-04 12:49:47,847 - INFO - joeynmt.training - Epoch   4: total training loss 7760.59
2021-10-04 12:49:47,848 - INFO - joeynmt.training - EPOCH 5
2021-10-04 12:50:01,188 - INFO - joeynmt.training - Epoch   5: total training loss 7445.67
2021-10-04 12:50:01,189 - INFO - joeynmt.training - EPOCH 6
2021-10-04 12:50:14,056 - INFO - joeynmt.training - Epoch   6: total training loss 7209.70
2021-10-04 12:50:14,057 - INFO - joeynmt.training - EPOCH 7
2021-10-04 12:50:26,433 - INFO - joeynmt.training - Epoch   7: total training loss 7014.92
2021-10-04 12:50:26,434 - INFO - joeynmt.training - EPOCH 8
2021-10-04 12:50:38,719 - INFO - joeynmt.training - Epoch   8: total training loss 6844.82
2021-10-04 12:50:38,720 - INFO - joeynmt.training - EPOCH 9
2021-10-04 12:50:51,073 - INFO - joeynmt.training - Epoch   9: total training loss 6683.83
2021-10-04 12:50:51,073 - INFO - joeynmt.training - EPOCH 10
2021-10-04 12:51:03,527 - INFO - joeynmt.training - Epoch  10: total training loss 6534.57
2021-10-04 12:51:03,527 - INFO - joeynmt.training - EPOCH 11
2021-10-04 12:51:16,857 - INFO - joeynmt.training - Epoch  11: total training loss 6372.12
2021-10-04 12:51:16,858 - INFO - joeynmt.training - EPOCH 12
2021-10-04 12:51:29,822 - INFO - joeynmt.training - Epoch  12: total training loss 6219.37
2021-10-04 12:51:29,823 - INFO - joeynmt.training - EPOCH 13
2021-10-04 12:51:43,099 - INFO - joeynmt.training - Epoch  13: total training loss 6071.50
2021-10-04 12:51:43,099 - INFO - joeynmt.training - EPOCH 14
2021-10-04 12:51:56,210 - INFO - joeynmt.training - Epoch  14: total training loss 5926.63
2021-10-04 12:51:56,210 - INFO - joeynmt.training - EPOCH 15
2021-10-04 12:52:08,477 - INFO - joeynmt.training - Epoch  15: total training loss 5774.63
2021-10-04 12:52:08,477 - INFO - joeynmt.training - EPOCH 16
2021-10-04 12:52:13,245 - INFO - joeynmt.training - Epoch  16, Step:     1000, Batch Loss:    86.710892, Tokens per Sec:     8986, Lr: 0.000300
2021-10-04 12:52:25,575 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-04 12:52:25,575 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-04 12:52:25,575 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-04 12:52:25,580 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-04 12:52:26,884 - INFO - joeynmt.training - Example #0
2021-10-04 12:52:26,885 - INFO - joeynmt.training - 	Source:     tr
2021-10-04 12:52:26,885 - INFO - joeynmt.training - 	Reference:  en
2021-10-04 12:52:26,885 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-04 12:52:26,885 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u015f' in position 86: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['1@@', '1', 'ya\u015f@@', '\u0131nday@@', 'ken', 'bir', 'sa@@', 'ba@@', 'h', 'ev@@', 'im@@', 'deki', 'sev@@', 'in@@', 'ç', 'ç@@', '\u0131\u011f@@', 'l\u0131@@', 'klar@@', '\u0131yla', 'u@@', 'yan@@', 'd\u0131\u011f@@', '\u0131m\u0131', 'hat\u0131r@@', 'l\u0131@@', 'yorum', '.'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 70-71: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .',)
2021-10-04 12:52:26,890 - INFO - joeynmt.training - 	Source:     11 ya\u015f\u0131ndayken bir sabah evimdeki sevinç ç\u0131\u011fl\u0131klar\u0131yla uyand\u0131\u011f\u0131m\u0131 hat\u0131rl\u0131yorum .
2021-10-04 12:52:26,891 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-04 12:52:26,891 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to get a cart of a man and I &apos;m going to get a cart .
2021-10-04 12:52:26,891 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 151: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['"B@@', 'ab@@', 'am', 'B@@', 'B@@', 'C', 'H@@', 'ab@@', 'er', 'kan@@', 'al@@', '\u0131n\u0131', 'din@@', 'li@@', 'yordu', ';', 'o', 'uf@@', 'ak', ',', 'gr@@', 'i', 'rad@@', 'y@@', 'os@@', 'undan', '."'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0131' in position 87: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('"Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."',)
2021-10-04 12:52:26,892 - INFO - joeynmt.training - 	Source:     "Babam BBC Haber kanal\u0131n\u0131 dinliyordu ; o ufak , gri radyosundan ."
2021-10-04 12:52:26,892 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-04 12:52:26,893 - INFO - joeynmt.training - 	Hypothesis: "In 191919196,000 , the Ho-..I, it &apos;s a very different ."
2021-10-04 12:52:26,893 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     1000: bleu:   5.00, loss: 73561.0469, ppl: 127.0578, duration: 13.6470s
2021-10-04 12:52:34,427 - INFO - joeynmt.training - Epoch  16: total training loss 5625.55
2021-10-04 12:52:34,428 - INFO - joeynmt.training - EPOCH 17
2021-10-04 12:52:46,689 - INFO - joeynmt.training - Epoch  17: total training loss 5475.26
2021-10-04 12:52:46,689 - INFO - joeynmt.training - EPOCH 18
2021-10-04 12:52:58,934 - INFO - joeynmt.training - Epoch  18: total training loss 5325.80
2021-10-04 12:52:58,934 - INFO - joeynmt.training - EPOCH 19
2021-10-04 12:53:11,299 - INFO - joeynmt.training - Epoch  19: total training loss 5174.02
2021-10-04 12:53:11,299 - INFO - joeynmt.training - EPOCH 20
2021-10-04 12:53:24,743 - INFO - joeynmt.training - Epoch  20: total training loss 5032.12
2021-10-04 12:53:24,743 - INFO - joeynmt.training - Training ended after  20 epochs.
2021-10-04 12:53:24,743 - INFO - joeynmt.training - Best validation result (greedy) at step     1000:   5.00 eval_metric.
2021-10-04 12:53:24,774 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-04 12:53:24,774 - INFO - joeynmt.prediction - Loading model from models/TRANS_tr_s_INIT/1000.ckpt
2021-10-04 12:53:25,638 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-04 12:53:26,459 - INFO - joeynmt.model - Enc-dec model built.
2021-10-04 12:53:26,650 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/tr_s.en_s/val.bpe.en_s)...
2021-10-04 12:53:51,815 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-04 12:53:51,815 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-04 12:53:51,815 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-04 12:53:51,818 - INFO - joeynmt.prediction -  dev bleu[13a]:   5.77 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-04 12:53:51,822 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_tr_s_INIT/00001000.hyps.dev
2021-10-04 12:53:51,822 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/tr_s.en_s/test.bpe.en_s)...
2021-10-04 12:54:20,033 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-04 12:54:20,033 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-04 12:54:20,033 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-04 12:54:20,037 - INFO - joeynmt.prediction - test bleu[13a]:   6.71 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-04 12:54:20,039 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_tr_s_INIT/00001000.hyps.test
