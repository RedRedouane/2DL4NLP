2021-10-03 13:32:42,952 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-03 13:32:43,014 - INFO - joeynmt.data - Loading training data...
2021-10-03 13:32:43,109 - INFO - joeynmt.data - Building vocabulary...
2021-10-03 13:32:43,535 - INFO - joeynmt.data - Loading dev data...
2021-10-03 13:32:43,547 - INFO - joeynmt.data - Loading test data...
2021-10-03 13:32:43,557 - INFO - joeynmt.data - Data loaded.
2021-10-03 13:32:43,557 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:32:44,096 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:32:44,102 - INFO - joeynmt.training - Total params: 24665088
2021-10-03 13:32:48,502 - INFO - joeynmt.helpers - cfg.name                           : LSTM_ka_r
2021-10-03 13:32:48,502 - INFO - joeynmt.helpers - cfg.data.src                       : ka_r
2021-10-03 13:32:48,502 - INFO - joeynmt.helpers - cfg.data.trg                       : en_s
2021-10-03 13:32:48,502 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/ka_r.en_s/train.bpe
2021-10-03 13:32:48,502 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/ka_r.en_s/val.bpe
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/ka_r.en_s/test.bpe
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 100000
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 100000
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/ka_r.en_s/ka_r.en_s.vocab.txt
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/ka_r.en_s/ka_r.en_s.vocab.txt
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 5e-07
2021-10-03 13:32:48,503 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.clip_grad_norm        : 1.0
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.patience              : 10
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.5
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.epochs                : 20
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 100
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/LSTM_ka_r
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100
2021-10-03 13:32:48,504 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.encoder.rnn_type         : lstm
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : False
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.encoder.bidirectional    : False
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.2
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 1
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.decoder.rnn_type         : lstm
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : False
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.decoder.emb_scale        : False
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 1024
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.2
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_dropout   : 0.2
2021-10-03 13:32:48,505 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 1
2021-10-03 13:32:48,506 - INFO - joeynmt.helpers - cfg.model.decoder.input_feeding    : True
2021-10-03 13:32:48,506 - INFO - joeynmt.helpers - cfg.model.decoder.init_hidden      : bridge
2021-10-03 13:32:48,506 - INFO - joeynmt.helpers - cfg.model.decoder.attention        : bahdanau
2021-10-03 13:32:48,506 - INFO - joeynmt.helpers - Data set sizes: 
	train 5158,
	valid 551,
	test 602
2021-10-03 13:32:48,506 - INFO - joeynmt.helpers - First training example:
	[SRC] ka
	[TRG] en
2021-10-03 13:32:48,506 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) is (9) a
2021-10-03 13:32:48,506 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) ." (7) the (8) is (9) a
2021-10-03 13:32:48,506 - INFO - joeynmt.helpers - Number of Src words (types): 4100
2021-10-03 13:32:48,507 - INFO - joeynmt.helpers - Number of Trg words (types): 4100
2021-10-03 13:32:48,507 - INFO - joeynmt.training - Model(
	encoder=RecurrentEncoder(LSTM(512, 512, batch_first=True)),
	decoder=RecurrentDecoder(rnn=LSTM(1536, 1024, batch_first=True), attention=BahdanauAttention),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4100),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4100))
2021-10-03 13:32:48,516 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-03 13:32:48,516 - INFO - joeynmt.training - EPOCH 1
2021-10-03 13:33:01,929 - INFO - joeynmt.training - Epoch   1: total training loss 9107.26
2021-10-03 13:33:01,929 - INFO - joeynmt.training - EPOCH 2
2021-10-03 13:33:11,749 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:33:11,750 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:33:11,750 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:33:11,754 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:33:12,674 - INFO - joeynmt.training - Example #0
2021-10-03 13:33:12,675 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:33:12,675 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:33:12,675 - INFO - joeynmt.training - 	Hypothesis: I is .
2021-10-03 13:33:12,675 - INFO - joeynmt.training - Example #1
2021-10-03 13:33:12,675 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:33:12,675 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:33:12,676 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s the the , , , , , , , , , , , , , , , , , , , , the the the ."
2021-10-03 13:33:12,676 - INFO - joeynmt.training - Example #2
2021-10-03 13:33:12,676 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:33:12,676 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:33:12,676 - INFO - joeynmt.training - 	Hypothesis: "And &apos;s the the , , , , , , , , , , , , , , , , , , , , the the the ."
2021-10-03 13:33:12,676 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      100: bleu:   2.40, loss: 93938.0859, ppl: 490.3982, duration: 3.4384s
2021-10-03 13:33:20,358 - INFO - joeynmt.training - Epoch   2: total training loss 8477.20
2021-10-03 13:33:20,358 - INFO - joeynmt.training - EPOCH 3
2021-10-03 13:33:33,881 - INFO - joeynmt.training - Epoch   3: total training loss 8219.92
2021-10-03 13:33:33,882 - INFO - joeynmt.training - EPOCH 4
2021-10-03 13:33:38,413 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:33:38,413 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:33:38,413 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:33:38,420 - INFO - joeynmt.training - Example #0
2021-10-03 13:33:38,420 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:33:38,420 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:33:38,420 - INFO - joeynmt.training - 	Hypothesis: ( , the .
2021-10-03 13:33:38,420 - INFO - joeynmt.training - Example #1
2021-10-03 13:33:38,421 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:33:38,421 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:33:38,421 - INFO - joeynmt.training - 	Hypothesis: "But &apos;s the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2021-10-03 13:33:38,421 - INFO - joeynmt.training - Example #2
2021-10-03 13:33:38,421 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:33:38,421 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:33:38,421 - INFO - joeynmt.training - 	Hypothesis: "But &apos;s the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the ."
2021-10-03 13:33:38,421 - INFO - joeynmt.training - Validation result (greedy) at epoch   4, step      200: bleu:   0.79, loss: 89966.0625, ppl: 377.3837, duration: 3.4756s
2021-10-03 13:33:54,888 - INFO - joeynmt.training - Epoch   4: total training loss 7964.28
2021-10-03 13:33:54,889 - INFO - joeynmt.training - EPOCH 5
2021-10-03 13:34:06,626 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:34:06,626 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:34:06,626 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:34:06,633 - INFO - joeynmt.training - Example #0
2021-10-03 13:34:06,633 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:34:06,633 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:34:06,633 - INFO - joeynmt.training - 	Hypothesis: ( Applause
2021-10-03 13:34:06,633 - INFO - joeynmt.training - Example #1
2021-10-03 13:34:06,633 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:34:06,634 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:34:06,634 - INFO - joeynmt.training - 	Hypothesis: "And I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I
2021-10-03 13:34:06,634 - INFO - joeynmt.training - Example #2
2021-10-03 13:34:06,634 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:34:06,634 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:34:06,634 - INFO - joeynmt.training - 	Hypothesis: "And I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I I
2021-10-03 13:34:06,634 - INFO - joeynmt.training - Validation result (greedy) at epoch   5, step      300: bleu:   0.08, loss: 86873.9297, ppl: 307.7649, duration: 3.4656s
2021-10-03 13:34:15,966 - INFO - joeynmt.training - Epoch   5: total training loss 7752.18
2021-10-03 13:34:15,967 - INFO - joeynmt.training - EPOCH 6
2021-10-03 13:34:29,406 - INFO - joeynmt.training - Epoch   6: total training loss 7444.49
2021-10-03 13:34:29,407 - INFO - joeynmt.training - EPOCH 7
2021-10-03 13:34:34,899 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:34:34,899 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:34:34,899 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:34:34,906 - INFO - joeynmt.training - Example #0
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:34:34,907 - INFO - joeynmt.training - Example #1
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Hypothesis: "And we &apos;re a little c-c-c-c-c-c-ction of the world , and we we have a same ."
2021-10-03 13:34:34,907 - INFO - joeynmt.training - Example #2
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:34:34,907 - INFO - joeynmt.training - 	Hypothesis: "And I have a little ction of the same c-ction of the world , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-03 13:34:34,908 - INFO - joeynmt.training - Validation result (greedy) at epoch   7, step      400: bleu:   2.11, loss: 81481.9766, ppl: 215.6673, duration: 3.4468s
2021-10-03 13:34:49,400 - INFO - joeynmt.training - Epoch   7: total training loss 7111.12
2021-10-03 13:34:49,400 - INFO - joeynmt.training - EPOCH 8
2021-10-03 13:35:02,283 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:35:02,283 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:35:02,283 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:35:02,291 - INFO - joeynmt.training - Example #0
2021-10-03 13:35:02,291 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:35:02,291 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:35:02,291 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:35:02,291 - INFO - joeynmt.training - Example #1
2021-10-03 13:35:02,291 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:35:02,291 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:35:02,291 - INFO - joeynmt.training - 	Hypothesis: "And I think , I think , I have a be , and I have a be , and I have a be ."
2021-10-03 13:35:02,292 - INFO - joeynmt.training - Example #2
2021-10-03 13:35:02,292 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:35:02,292 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:35:02,292 - INFO - joeynmt.training - 	Hypothesis: And I think to be a ctiy of the world of the world of the world of the world .
2021-10-03 13:35:02,292 - INFO - joeynmt.training - Validation result (greedy) at epoch   8, step      500: bleu:   1.56, loss: 79266.9062, ppl: 186.3549, duration: 3.4798s
2021-10-03 13:35:07,820 - INFO - joeynmt.training - Epoch   8: total training loss 6914.00
2021-10-03 13:35:07,820 - INFO - joeynmt.training - EPOCH 9
2021-10-03 13:35:21,255 - INFO - joeynmt.training - Epoch   9: total training loss 6767.22
2021-10-03 13:35:21,255 - INFO - joeynmt.training - EPOCH 10
2021-10-03 13:35:27,816 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:35:27,816 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:35:27,816 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:35:27,821 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:35:28,719 - INFO - joeynmt.helpers - delete models/LSTM_ka_r/100.ckpt
2021-10-03 13:35:28,793 - INFO - joeynmt.helpers - delete /home/lcur0006/joeynmt/models/LSTM_ka_r/100.ckpt
2021-10-03 13:35:28,793 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0006/joeynmt/models/LSTM_ka_r/100.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0006/joeynmt/models/LSTM_ka_r/100.ckpt')
2021-10-03 13:35:28,795 - INFO - joeynmt.training - Example #0
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:35:28,796 - INFO - joeynmt.training - Example #1
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Hypothesis: "And I &apos;ve have to the universe , I &apos;ve have a lot of the universe , I &apos;ve have a lot of the universe ."
2021-10-03 13:35:28,796 - INFO - joeynmt.training - Example #2
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:35:28,796 - INFO - joeynmt.training - 	Hypothesis: I &apos;m going to be a way that I &apos;m going to be a way .
2021-10-03 13:35:28,796 - INFO - joeynmt.training - Validation result (greedy) at epoch  10, step      600: bleu:   2.70, loss: 77778.3438, ppl: 168.9296, duration: 4.4502s
2021-10-03 13:35:40,361 - INFO - joeynmt.training - Epoch  10: total training loss 6611.50
2021-10-03 13:35:40,361 - INFO - joeynmt.training - EPOCH 11
2021-10-03 13:35:54,254 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:35:54,254 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:35:54,254 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:35:54,259 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:35:55,148 - INFO - joeynmt.helpers - delete models/LSTM_ka_r/600.ckpt
2021-10-03 13:35:55,219 - INFO - joeynmt.helpers - delete /home/lcur0006/joeynmt/models/LSTM_ka_r/600.ckpt
2021-10-03 13:35:55,220 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0006/joeynmt/models/LSTM_ka_r/600.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0006/joeynmt/models/LSTM_ka_r/600.ckpt')
2021-10-03 13:35:55,221 - INFO - joeynmt.training - Example #0
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:35:55,222 - INFO - joeynmt.training - Example #1
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Hypothesis: "And I think , I was going to be a lot of the world , I can be a lot of the world ."
2021-10-03 13:35:55,222 - INFO - joeynmt.training - Example #2
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:35:55,222 - INFO - joeynmt.training - 	Hypothesis: I was going to be a little world of the world , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-03 13:35:55,223 - INFO - joeynmt.training - Validation result (greedy) at epoch  11, step      700: bleu:   3.09, loss: 77447.0391, ppl: 165.2786, duration: 4.4289s
2021-10-03 13:36:01,529 - INFO - joeynmt.training - Epoch  11: total training loss 6486.16
2021-10-03 13:36:01,529 - INFO - joeynmt.training - EPOCH 12
2021-10-03 13:36:14,956 - INFO - joeynmt.training - Epoch  12: total training loss 6365.42
2021-10-03 13:36:14,957 - INFO - joeynmt.training - EPOCH 13
2021-10-03 13:36:22,512 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:36:22,512 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:36:22,512 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:36:22,517 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:36:23,428 - INFO - joeynmt.helpers - delete models/LSTM_ka_r/700.ckpt
2021-10-03 13:36:23,502 - INFO - joeynmt.helpers - delete /home/lcur0006/joeynmt/models/LSTM_ka_r/700.ckpt
2021-10-03 13:36:23,502 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0006/joeynmt/models/LSTM_ka_r/700.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0006/joeynmt/models/LSTM_ka_r/700.ckpt')
2021-10-03 13:36:23,539 - INFO - joeynmt.training - Example #0
2021-10-03 13:36:23,539 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:36:23,540 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:36:23,540 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:36:23,540 - INFO - joeynmt.training - Example #1
2021-10-03 13:36:23,540 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:36:23,540 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:36:23,540 - INFO - joeynmt.training - 	Hypothesis: "And I was a little bit of the world , I was a lot of the world , I was a lot of the world ."
2021-10-03 13:36:23,541 - INFO - joeynmt.training - Example #2
2021-10-03 13:36:23,541 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:36:23,541 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:36:23,541 - INFO - joeynmt.training - 	Hypothesis: And I was a couple of the first of the world . &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-03 13:36:23,541 - INFO - joeynmt.training - Validation result (greedy) at epoch  13, step      800: bleu:   3.21, loss: 76616.9375, ppl: 156.4736, duration: 4.4770s
2021-10-03 13:36:36,143 - INFO - joeynmt.training - Epoch  13: total training loss 6239.81
2021-10-03 13:36:36,144 - INFO - joeynmt.training - EPOCH 14
2021-10-03 13:36:51,107 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:36:51,108 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:36:51,108 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:36:51,112 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:36:52,001 - INFO - joeynmt.helpers - delete models/LSTM_ka_r/800.ckpt
2021-10-03 13:36:52,071 - INFO - joeynmt.helpers - delete /home/lcur0006/joeynmt/models/LSTM_ka_r/800.ckpt
2021-10-03 13:36:52,071 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0006/joeynmt/models/LSTM_ka_r/800.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0006/joeynmt/models/LSTM_ka_r/800.ckpt')
2021-10-03 13:36:52,073 - INFO - joeynmt.training - Example #0
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:36:52,074 - INFO - joeynmt.training - Example #1
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Hypothesis: "And I think , I think , I &apos;m a lot of the universe , I &apos;m a lot of the world ."
2021-10-03 13:36:52,074 - INFO - joeynmt.training - Example #2
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:36:52,074 - INFO - joeynmt.training - 	Hypothesis: And I &apos;m a little question of the first , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-03 13:36:52,074 - INFO - joeynmt.training - Validation result (greedy) at epoch  14, step      900: bleu:   3.35, loss: 75716.2500, ppl: 147.4497, duration: 4.4356s
2021-10-03 13:36:57,235 - INFO - joeynmt.training - Epoch  14: total training loss 6113.14
2021-10-03 13:36:57,236 - INFO - joeynmt.training - EPOCH 15
2021-10-03 13:37:10,660 - INFO - joeynmt.training - Epoch  15: total training loss 5998.78
2021-10-03 13:37:10,661 - INFO - joeynmt.training - EPOCH 16
2021-10-03 13:37:15,925 - INFO - joeynmt.training - Epoch  16, Step:     1000, Batch Loss:    89.469719, Tokens per Sec:     8082, Lr: 0.000300
2021-10-03 13:37:19,405 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:37:19,406 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:37:19,406 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:37:19,412 - INFO - joeynmt.training - Example #0
2021-10-03 13:37:19,412 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:37:19,412 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:37:19,412 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:37:19,412 - INFO - joeynmt.training - Example #1
2021-10-03 13:37:19,413 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:37:19,413 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:37:19,413 - INFO - joeynmt.training - 	Hypothesis: "And I think , I was going to be a couple of the world , I was going to be a lot of the world ."
2021-10-03 13:37:19,413 - INFO - joeynmt.training - Example #2
2021-10-03 13:37:19,413 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:37:19,413 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:37:19,413 - INFO - joeynmt.training - 	Hypothesis: And I was a couple of the first , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-03 13:37:19,413 - INFO - joeynmt.training - Validation result (greedy) at epoch  16, step     1000: bleu:   3.29, loss: 75412.7500, ppl: 144.5278, duration: 3.4875s
2021-10-03 13:37:30,928 - INFO - joeynmt.training - Epoch  16: total training loss 5881.69
2021-10-03 13:37:30,929 - INFO - joeynmt.training - EPOCH 17
2021-10-03 13:37:46,968 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:37:46,968 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:37:46,968 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:37:46,974 - INFO - joeynmt.training - Example #0
2021-10-03 13:37:46,975 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:37:46,975 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:37:46,975 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:37:46,975 - INFO - joeynmt.training - Example #1
2021-10-03 13:37:46,975 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:37:46,975 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:37:46,975 - INFO - joeynmt.training - 	Hypothesis: "And I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot; &quot; I said , &quot;
2021-10-03 13:37:46,975 - INFO - joeynmt.training - Example #2
2021-10-03 13:37:46,976 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:37:46,976 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:37:46,976 - INFO - joeynmt.training - 	Hypothesis: I was a little bit of the story , &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;
2021-10-03 13:37:46,976 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step     1100: bleu:   3.17, loss: 74999.7969, ppl: 140.6447, duration: 3.4807s
2021-10-03 13:37:52,893 - INFO - joeynmt.training - Epoch  17: total training loss 5778.70
2021-10-03 13:37:52,894 - INFO - joeynmt.training - EPOCH 18
2021-10-03 13:38:06,376 - INFO - joeynmt.training - Epoch  18: total training loss 5669.99
2021-10-03 13:38:06,377 - INFO - joeynmt.training - EPOCH 19
2021-10-03 13:38:15,887 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:38:15,887 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:38:15,887 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:38:15,892 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:38:16,799 - INFO - joeynmt.helpers - delete models/LSTM_ka_r/900.ckpt
2021-10-03 13:38:16,867 - INFO - joeynmt.helpers - delete /home/lcur0006/joeynmt/models/LSTM_ka_r/900.ckpt
2021-10-03 13:38:16,868 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0006/joeynmt/models/LSTM_ka_r/900.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0006/joeynmt/models/LSTM_ka_r/900.ckpt')
2021-10-03 13:38:16,870 - INFO - joeynmt.training - Example #0
2021-10-03 13:38:16,870 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:38:16,870 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:38:16,870 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:38:16,870 - INFO - joeynmt.training - Example #1
2021-10-03 13:38:16,870 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:38:16,870 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:38:16,870 - INFO - joeynmt.training - 	Hypothesis: "And I went to my end , I was going to be a couple of the world , I was going to be ."
2021-10-03 13:38:16,870 - INFO - joeynmt.training - Example #2
2021-10-03 13:38:16,871 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:38:16,871 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:38:16,871 - INFO - joeynmt.training - 	Hypothesis: I was going to show you a man who was a very ss-year-man .
2021-10-03 13:38:16,871 - INFO - joeynmt.training - Validation result (greedy) at epoch  19, step     1200: bleu:   4.42, loss: 75202.8203, ppl: 142.5406, duration: 4.2773s
2021-10-03 13:38:25,379 - INFO - joeynmt.training - Epoch  19: total training loss 5561.55
2021-10-03 13:38:25,379 - INFO - joeynmt.training - EPOCH 20
2021-10-03 13:38:42,229 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:38:42,229 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:38:42,229 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:38:42,235 - INFO - joeynmt.training - Example #0
2021-10-03 13:38:42,236 - INFO - joeynmt.training - 	Source:     ka
2021-10-03 13:38:42,236 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:38:42,236 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:38:42,236 - INFO - joeynmt.training - Example #1
2021-10-03 13:38:42,236 - INFO - joeynmt.training - 	Source:     "rodesac 11 tslis viyavi , me makhsovs sikharulis noTaze gaghvidzeba ert dilas ."
2021-10-03 13:38:42,236 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:38:42,236 - INFO - joeynmt.training - 	Hypothesis: "And I was a very part of the first , I was a lot of the world , I was a lot of ."
2021-10-03 13:38:42,236 - INFO - joeynmt.training - Example #2
2021-10-03 13:38:42,236 - INFO - joeynmt.training - 	Source:     mamachemi usmenda &quot; &quot; bibisis &quot; &quot; siakhleebs tavis paTara nacrisfer radioshi .
2021-10-03 13:38:42,237 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:38:42,237 - INFO - joeynmt.training - 	Hypothesis: I was a first . &quot; &quot; I was a couple of the first .
2021-10-03 13:38:42,237 - INFO - joeynmt.training - Validation result (greedy) at epoch  20, step     1300: bleu:   4.14, loss: 75139.7344, ppl: 141.9488, duration: 3.4389s
2021-10-03 13:38:43,557 - INFO - joeynmt.training - Epoch  20: total training loss 5448.13
2021-10-03 13:38:43,566 - INFO - joeynmt.training - Training ended after  20 epochs.
2021-10-03 13:38:43,566 - INFO - joeynmt.training - Best validation result (greedy) at step     1200:   4.42 eval_metric.
2021-10-03 13:38:43,596 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-03 13:38:43,596 - INFO - joeynmt.prediction - Loading model from models/LSTM_ka_r/1200.ckpt
2021-10-03 13:38:43,906 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:38:44,422 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:38:44,476 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/ka_r.en_s/val.bpe.en_s)...
2021-10-03 13:38:57,275 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:38:57,275 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:38:57,275 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:38:57,279 - INFO - joeynmt.prediction -  dev bleu[13a]:   4.68 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:38:57,281 - INFO - joeynmt.prediction - Translations saved to: models/LSTM_ka_r/00001200.hyps.dev
2021-10-03 13:38:57,281 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/ka_r.en_s/test.bpe.en_s)...
2021-10-03 13:39:12,132 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:39:12,133 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:39:12,133 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:39:12,137 - INFO - joeynmt.prediction - test bleu[13a]:   5.77 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:39:12,139 - INFO - joeynmt.prediction - Translations saved to: models/LSTM_ka_r/00001200.hyps.test
