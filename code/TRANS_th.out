2021-10-03 13:34:12,227 - INFO - root - Hello! This is Joey-NMT (version 1.3).
2021-10-03 13:34:12,276 - INFO - joeynmt.data - Loading training data...
2021-10-03 13:34:12,404 - INFO - joeynmt.data - Building vocabulary...
2021-10-03 13:34:12,874 - INFO - joeynmt.data - Loading dev data...
2021-10-03 13:34:12,888 - INFO - joeynmt.data - Loading test data...
2021-10-03 13:34:12,901 - INFO - joeynmt.data - Data loaded.
2021-10-03 13:34:12,901 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:34:13,735 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:34:13,750 - INFO - joeynmt.training - Total params: 33699840
2021-10-03 13:34:18,167 - INFO - joeynmt.helpers - cfg.name                           : TRANS_th
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.src                       : th
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.trg                       : en_s
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.train                     : ../2DL4NLP/all_data/th.en_s/train.bpe
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.dev                       : ../2DL4NLP/all_data/th.en_s/val.bpe
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.test                      : ../2DL4NLP/all_data/th.en_s/test.bpe
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.level                     : bpe
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 50
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.src_voc_min_freq          : 0
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.src_voc_limit             : 10000
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.trg_voc_min_freq          : 0
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.trg_voc_limit             : 10000
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : ../2DL4NLP/all_data/th.en_s/th.en_s.vocab.txt
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : ../2DL4NLP/all_data/th.en_s/th.en_s.vocab.txt
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0
2021-10-03 13:34:18,168 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.batch_size            : 80
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.patience              : 1
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : eval_metric
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.epochs                : 100
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 1000
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/TRANS_th
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.overwrite             : True
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.shuffle               : True
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 60
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2]
2021-10-03 13:34:18,169 - INFO - joeynmt.helpers - cfg.training.keep_best_ckpts       : 1
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.training.save_latest_ckpt      : True
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024
2021-10-03 13:34:18,170 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - Data set sizes: 
	train 4698,
	valid 551,
	test 602
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - First training example:
	[SRC] th
	[TRG] en
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e48' in position 114: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 835, in train
    trg_vocab=trg_vocab)
  File "/home/lcur0008/joeynmt/joeynmt/helpers.py", line 164, in log_data_info
    " ".join('(%d) %s' % (i, t) for i, t in enumerate(src_vocab.itos[:10])))
Message: 'First 10 words (src): %s'
Arguments: ('(0) <unk> (1) <pad> (2) <s> (3) </s> (4) \u0e48 (5) \u0e49 (6) , (7) \u0e47 (8) the (9) .',)
2021-10-03 13:34:18,171 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) \u0e48 (5) \u0e49 (6) , (7) \u0e47 (8) the (9) .
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e48' in position 114: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 835, in train
    trg_vocab=trg_vocab)
  File "/home/lcur0008/joeynmt/joeynmt/helpers.py", line 167, in log_data_info
    " ".join('(%d) %s' % (i, t) for i, t in enumerate(trg_vocab.itos[:10])))
Message: 'First 10 words (trg): %s'
Arguments: ('(0) <unk> (1) <pad> (2) <s> (3) </s> (4) \u0e48 (5) \u0e49 (6) , (7) \u0e47 (8) the (9) .',)
2021-10-03 13:34:18,174 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) \u0e48 (5) \u0e49 (6) , (7) \u0e47 (8) the (9) .
2021-10-03 13:34:18,174 - INFO - joeynmt.helpers - Number of Src words (types): 4208
2021-10-03 13:34:18,174 - INFO - joeynmt.helpers - Number of Trg words (types): 4208
2021-10-03 13:34:18,175 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=6, num_heads=4),
	decoder=TransformerDecoder(num_layers=6, num_heads=4),
	src_embed=Embeddings(embedding_dim=512, vocab_size=4208),
	trg_embed=Embeddings(embedding_dim=512, vocab_size=4208))
2021-10-03 13:34:18,184 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 80
	total batch size (w. parallel & accumulation): 80
2021-10-03 13:34:18,185 - INFO - joeynmt.training - EPOCH 1
2021-10-03 13:34:28,935 - INFO - joeynmt.training - Epoch   1: total training loss 7622.48
2021-10-03 13:34:28,936 - INFO - joeynmt.training - EPOCH 2
2021-10-03 13:34:39,580 - INFO - joeynmt.training - Epoch   2: total training loss 7007.61
2021-10-03 13:34:39,580 - INFO - joeynmt.training - EPOCH 3
2021-10-03 13:34:50,947 - INFO - joeynmt.training - Epoch   3: total training loss 6735.62
2021-10-03 13:34:50,948 - INFO - joeynmt.training - EPOCH 4
2021-10-03 13:35:02,753 - INFO - joeynmt.training - Epoch   4: total training loss 6525.11
2021-10-03 13:35:02,753 - INFO - joeynmt.training - EPOCH 5
2021-10-03 13:35:14,051 - INFO - joeynmt.training - Epoch   5: total training loss 6226.43
2021-10-03 13:35:14,052 - INFO - joeynmt.training - EPOCH 6
2021-10-03 13:35:25,267 - INFO - joeynmt.training - Epoch   6: total training loss 6006.04
2021-10-03 13:35:25,267 - INFO - joeynmt.training - EPOCH 7
2021-10-03 13:35:36,280 - INFO - joeynmt.training - Epoch   7: total training loss 5838.53
2021-10-03 13:35:36,281 - INFO - joeynmt.training - EPOCH 8
2021-10-03 13:35:47,891 - INFO - joeynmt.training - Epoch   8: total training loss 5687.62
2021-10-03 13:35:47,891 - INFO - joeynmt.training - EPOCH 9
2021-10-03 13:35:58,875 - INFO - joeynmt.training - Epoch   9: total training loss 5535.44
2021-10-03 13:35:58,875 - INFO - joeynmt.training - EPOCH 10
2021-10-03 13:36:09,616 - INFO - joeynmt.training - Epoch  10: total training loss 5388.35
2021-10-03 13:36:09,616 - INFO - joeynmt.training - EPOCH 11
2021-10-03 13:36:20,407 - INFO - joeynmt.training - Epoch  11: total training loss 5251.55
2021-10-03 13:36:20,408 - INFO - joeynmt.training - EPOCH 12
2021-10-03 13:36:31,313 - INFO - joeynmt.training - Epoch  12: total training loss 5117.31
2021-10-03 13:36:31,313 - INFO - joeynmt.training - EPOCH 13
2021-10-03 13:36:42,137 - INFO - joeynmt.training - Epoch  13: total training loss 4993.34
2021-10-03 13:36:42,138 - INFO - joeynmt.training - EPOCH 14
2021-10-03 13:36:52,988 - INFO - joeynmt.training - Epoch  14: total training loss 4853.34
2021-10-03 13:36:52,988 - INFO - joeynmt.training - EPOCH 15
2021-10-03 13:37:03,945 - INFO - joeynmt.training - Epoch  15: total training loss 4723.79
2021-10-03 13:37:03,945 - INFO - joeynmt.training - EPOCH 16
2021-10-03 13:37:14,825 - INFO - joeynmt.training - Epoch  16: total training loss 4594.45
2021-10-03 13:37:14,825 - INFO - joeynmt.training - EPOCH 17
2021-10-03 13:37:25,215 - INFO - joeynmt.training - Epoch  17, Step:     1000, Batch Loss:    70.812508, Tokens per Sec:     8547, Lr: 0.000300
2021-10-03 13:37:39,113 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:37:39,113 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:37:39,113 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:37:39,118 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:37:40,376 - INFO - joeynmt.training - Example #0
2021-10-03 13:37:40,377 - INFO - joeynmt.training - 	Source:     th
2021-10-03 13:37:40,377 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:37:40,377 - INFO - joeynmt.training - 	Hypothesis: Thank you .
2021-10-03 13:37:40,377 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e15\u0e2d\u0e19@@', '\u0e09\u0e31\u0e19@@', '\u0e2d\u0e32\u0e22\u0e38', '1@@', '1', '\u0e09\u0e31\u0e19@@', '\u0e08\u0e33@@', '\u0e44\u0e14', '\u0e49', '\u0e27', '\u0e48', '\u0e32\u0e15@@', '\u0e37', '\u0e48', '\u0e19@@', '\u0e02\u0e36', '\u0e49', '\u0e19\u0e21\u0e32@@', '\u0e40\u0e0a', '\u0e49', '\u0e32\u0e27@@', '\u0e31\u0e19@@', '\u0e2b\u0e19\u0e36', '\u0e48', '\u0e07', '\u0e41\u0e25\u0e30@@', '\u0e44\u0e14', '\u0e49', '\u0e22\u0e34\u0e19@@', '\u0e40\u0e2a\u0e35\u0e22\u0e07@@', '\u0e23', '\u0e49', '\u0e2d\u0e07@@', '\u0e14', '\u0e49', '\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21@@', '\u0e22\u0e34\u0e19@@', '\u0e14\u0e35@@', '\u0e43\u0e19@@', '\u0e1a', '\u0e49', '\u0e32\u0e19'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19',)
2021-10-03 13:37:40,380 - INFO - joeynmt.training - 	Source:     \u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19
2021-10-03 13:37:40,381 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:37:40,381 - INFO - joeynmt.training - 	Hypothesis: "I was a few years of my day , I was a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very
2021-10-03 13:37:40,381 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e1e', '\u0e48', '\u0e2d\u0e02\u0e2d\u0e07@@', '\u0e09\u0e31\u0e19@@', '\u0e01\u0e33\u0e25\u0e31\u0e07@@', '\u0e1f\u0e31\u0e07@@', '\u0e02', '\u0e48', '\u0e32\u0e27@@', '\u0e1a\u0e35@@', '\u0e1a\u0e35@@', '\u0e0b\u0e35', '\u0e08\u0e32\u0e01@@', '\u0e27\u0e34\u0e17@@', '\u0e22\u0e38@@', '\u0e2a\u0e35@@', '\u0e40\u0e17@@', '\u0e32@@', '\u0e2d\u0e31\u0e19@@', '\u0e40\u0e25', '\u0e47', '\u0e01'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 65: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01',)
2021-10-03 13:37:40,382 - INFO - joeynmt.training - 	Source:     \u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01
2021-10-03 13:37:40,383 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:37:40,383 - INFO - joeynmt.training - 	Hypothesis: I was a picture of my sistent of the Garara .
2021-10-03 13:37:40,383 - INFO - joeynmt.training - Validation result (greedy) at epoch  17, step     1000: bleu:   5.91, loss: 73352.9062, ppl: 116.9725, duration: 15.1673s
2021-10-03 13:37:40,909 - INFO - joeynmt.training - Epoch  17: total training loss 4465.55
2021-10-03 13:37:40,909 - INFO - joeynmt.training - EPOCH 18
2021-10-03 13:37:51,816 - INFO - joeynmt.training - Epoch  18: total training loss 4343.28
2021-10-03 13:37:51,817 - INFO - joeynmt.training - EPOCH 19
2021-10-03 13:38:02,717 - INFO - joeynmt.training - Epoch  19: total training loss 4211.57
2021-10-03 13:38:02,718 - INFO - joeynmt.training - EPOCH 20
2021-10-03 13:38:14,393 - INFO - joeynmt.training - Epoch  20: total training loss 4077.88
2021-10-03 13:38:14,394 - INFO - joeynmt.training - EPOCH 21
2021-10-03 13:38:26,376 - INFO - joeynmt.training - Epoch  21: total training loss 3954.12
2021-10-03 13:38:26,377 - INFO - joeynmt.training - EPOCH 22
2021-10-03 13:38:38,380 - INFO - joeynmt.training - Epoch  22: total training loss 3832.46
2021-10-03 13:38:38,380 - INFO - joeynmt.training - EPOCH 23
2021-10-03 13:38:50,172 - INFO - joeynmt.training - Epoch  23: total training loss 3716.88
2021-10-03 13:38:50,172 - INFO - joeynmt.training - EPOCH 24
2021-10-03 13:39:01,096 - INFO - joeynmt.training - Epoch  24: total training loss 3594.70
2021-10-03 13:39:01,097 - INFO - joeynmt.training - EPOCH 25
2021-10-03 13:39:12,867 - INFO - joeynmt.training - Epoch  25: total training loss 3481.83
2021-10-03 13:39:12,867 - INFO - joeynmt.training - EPOCH 26
2021-10-03 13:39:23,758 - INFO - joeynmt.training - Epoch  26: total training loss 3368.23
2021-10-03 13:39:23,758 - INFO - joeynmt.training - EPOCH 27
2021-10-03 13:39:34,766 - INFO - joeynmt.training - Epoch  27: total training loss 3263.11
2021-10-03 13:39:34,766 - INFO - joeynmt.training - EPOCH 28
2021-10-03 13:39:45,708 - INFO - joeynmt.training - Epoch  28: total training loss 3153.80
2021-10-03 13:39:45,709 - INFO - joeynmt.training - EPOCH 29
2021-10-03 13:39:56,605 - INFO - joeynmt.training - Epoch  29: total training loss 3052.97
2021-10-03 13:39:56,606 - INFO - joeynmt.training - EPOCH 30
2021-10-03 13:40:07,452 - INFO - joeynmt.training - Epoch  30: total training loss 2954.67
2021-10-03 13:40:07,452 - INFO - joeynmt.training - EPOCH 31
2021-10-03 13:40:18,399 - INFO - joeynmt.training - Epoch  31: total training loss 2845.65
2021-10-03 13:40:18,399 - INFO - joeynmt.training - EPOCH 32
2021-10-03 13:40:29,309 - INFO - joeynmt.training - Epoch  32: total training loss 2751.36
2021-10-03 13:40:29,309 - INFO - joeynmt.training - EPOCH 33
2021-10-03 13:40:40,243 - INFO - joeynmt.training - Epoch  33: total training loss 2659.83
2021-10-03 13:40:40,243 - INFO - joeynmt.training - EPOCH 34
2021-10-03 13:40:50,012 - INFO - joeynmt.training - Epoch  34, Step:     2000, Batch Loss:    44.592159, Tokens per Sec:     8576, Lr: 0.000300
2021-10-03 13:41:01,927 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:41:01,928 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:41:01,928 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:41:01,932 - INFO - joeynmt.training - Hooray! New best validation result [eval_metric]!
2021-10-03 13:41:03,172 - INFO - joeynmt.helpers - delete models/TRANS_th/1000.ckpt
2021-10-03 13:41:03,270 - INFO - joeynmt.helpers - delete /home/lcur0008/joeynmt/models/TRANS_th/1000.ckpt
2021-10-03 13:41:03,270 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /home/lcur0008/joeynmt/models/TRANS_th/1000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/home/lcur0008/joeynmt/models/TRANS_th/1000.ckpt')
2021-10-03 13:41:03,272 - INFO - joeynmt.training - Example #0
2021-10-03 13:41:03,273 - INFO - joeynmt.training - 	Source:     th
2021-10-03 13:41:03,273 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:41:03,273 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:41:03,273 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e15\u0e2d\u0e19@@', '\u0e09\u0e31\u0e19@@', '\u0e2d\u0e32\u0e22\u0e38', '1@@', '1', '\u0e09\u0e31\u0e19@@', '\u0e08\u0e33@@', '\u0e44\u0e14', '\u0e49', '\u0e27', '\u0e48', '\u0e32\u0e15@@', '\u0e37', '\u0e48', '\u0e19@@', '\u0e02\u0e36', '\u0e49', '\u0e19\u0e21\u0e32@@', '\u0e40\u0e0a', '\u0e49', '\u0e32\u0e27@@', '\u0e31\u0e19@@', '\u0e2b\u0e19\u0e36', '\u0e48', '\u0e07', '\u0e41\u0e25\u0e30@@', '\u0e44\u0e14', '\u0e49', '\u0e22\u0e34\u0e19@@', '\u0e40\u0e2a\u0e35\u0e22\u0e07@@', '\u0e23', '\u0e49', '\u0e2d\u0e07@@', '\u0e14', '\u0e49', '\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21@@', '\u0e22\u0e34\u0e19@@', '\u0e14\u0e35@@', '\u0e43\u0e19@@', '\u0e1a', '\u0e49', '\u0e32\u0e19'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19',)
2021-10-03 13:41:03,276 - INFO - joeynmt.training - 	Source:     \u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19
2021-10-03 13:41:03,276 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:41:03,276 - INFO - joeynmt.training - 	Hypothesis: "And I was 227 , I had a very good thing that I was going to have been ssive seven in the workkle and Cloved in the work ."
2021-10-03 13:41:03,276 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e1e', '\u0e48', '\u0e2d\u0e02\u0e2d\u0e07@@', '\u0e09\u0e31\u0e19@@', '\u0e01\u0e33\u0e25\u0e31\u0e07@@', '\u0e1f\u0e31\u0e07@@', '\u0e02', '\u0e48', '\u0e32\u0e27@@', '\u0e1a\u0e35@@', '\u0e1a\u0e35@@', '\u0e0b\u0e35', '\u0e08\u0e32\u0e01@@', '\u0e27\u0e34\u0e17@@', '\u0e22\u0e38@@', '\u0e2a\u0e35@@', '\u0e40\u0e17@@', '\u0e32@@', '\u0e2d\u0e31\u0e19@@', '\u0e40\u0e25', '\u0e47', '\u0e01'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 65: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01',)
2021-10-03 13:41:03,277 - INFO - joeynmt.training - 	Source:     \u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01
2021-10-03 13:41:03,277 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:41:03,277 - INFO - joeynmt.training - 	Hypothesis: He was a lot of my dani technologist .
2021-10-03 13:41:03,277 - INFO - joeynmt.training - Validation result (greedy) at epoch  34, step     2000: bleu:   8.69, loss: 76898.0469, ppl: 147.2428, duration: 13.2649s
2021-10-03 13:41:04,345 - INFO - joeynmt.training - Epoch  34: total training loss 2551.96
2021-10-03 13:41:04,346 - INFO - joeynmt.training - EPOCH 35
2021-10-03 13:41:15,221 - INFO - joeynmt.training - Epoch  35: total training loss 2460.64
2021-10-03 13:41:15,222 - INFO - joeynmt.training - EPOCH 36
2021-10-03 13:41:26,037 - INFO - joeynmt.training - Epoch  36: total training loss 2380.95
2021-10-03 13:41:26,037 - INFO - joeynmt.training - EPOCH 37
2021-10-03 13:41:36,818 - INFO - joeynmt.training - Epoch  37: total training loss 2293.85
2021-10-03 13:41:36,818 - INFO - joeynmt.training - EPOCH 38
2021-10-03 13:41:47,681 - INFO - joeynmt.training - Epoch  38: total training loss 2200.91
2021-10-03 13:41:47,681 - INFO - joeynmt.training - EPOCH 39
2021-10-03 13:41:58,780 - INFO - joeynmt.training - Epoch  39: total training loss 2118.44
2021-10-03 13:41:58,780 - INFO - joeynmt.training - EPOCH 40
2021-10-03 13:42:10,492 - INFO - joeynmt.training - Epoch  40: total training loss 2034.98
2021-10-03 13:42:10,492 - INFO - joeynmt.training - EPOCH 41
2021-10-03 13:42:22,389 - INFO - joeynmt.training - Epoch  41: total training loss 1950.38
2021-10-03 13:42:22,389 - INFO - joeynmt.training - EPOCH 42
2021-10-03 13:42:34,209 - INFO - joeynmt.training - Epoch  42: total training loss 1879.18
2021-10-03 13:42:34,209 - INFO - joeynmt.training - EPOCH 43
2021-10-03 13:42:45,602 - INFO - joeynmt.training - Epoch  43: total training loss 1806.46
2021-10-03 13:42:45,603 - INFO - joeynmt.training - EPOCH 44
2021-10-03 13:42:56,547 - INFO - joeynmt.training - Epoch  44: total training loss 1733.03
2021-10-03 13:42:56,548 - INFO - joeynmt.training - EPOCH 45
2021-10-03 13:43:07,666 - INFO - joeynmt.training - Epoch  45: total training loss 1670.33
2021-10-03 13:43:07,666 - INFO - joeynmt.training - EPOCH 46
2021-10-03 13:43:19,462 - INFO - joeynmt.training - Epoch  46: total training loss 1596.90
2021-10-03 13:43:19,463 - INFO - joeynmt.training - EPOCH 47
2021-10-03 13:43:30,483 - INFO - joeynmt.training - Epoch  47: total training loss 1534.08
2021-10-03 13:43:30,484 - INFO - joeynmt.training - EPOCH 48
2021-10-03 13:43:41,321 - INFO - joeynmt.training - Epoch  48: total training loss 1464.34
2021-10-03 13:43:41,322 - INFO - joeynmt.training - EPOCH 49
2021-10-03 13:43:52,179 - INFO - joeynmt.training - Epoch  49: total training loss 1410.66
2021-10-03 13:43:52,180 - INFO - joeynmt.training - EPOCH 50
2021-10-03 13:44:03,964 - INFO - joeynmt.training - Epoch  50: total training loss 1347.19
2021-10-03 13:44:03,964 - INFO - joeynmt.training - EPOCH 51
2021-10-03 13:44:14,093 - INFO - joeynmt.training - Epoch  51, Step:     3000, Batch Loss:    22.864319, Tokens per Sec:     7830, Lr: 0.000300
2021-10-03 13:44:25,947 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:44:25,947 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:44:25,947 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:44:25,953 - INFO - joeynmt.training - Example #0
2021-10-03 13:44:25,953 - INFO - joeynmt.training - 	Source:     th
2021-10-03 13:44:25,953 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:44:25,953 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:44:25,953 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e15\u0e2d\u0e19@@', '\u0e09\u0e31\u0e19@@', '\u0e2d\u0e32\u0e22\u0e38', '1@@', '1', '\u0e09\u0e31\u0e19@@', '\u0e08\u0e33@@', '\u0e44\u0e14', '\u0e49', '\u0e27', '\u0e48', '\u0e32\u0e15@@', '\u0e37', '\u0e48', '\u0e19@@', '\u0e02\u0e36', '\u0e49', '\u0e19\u0e21\u0e32@@', '\u0e40\u0e0a', '\u0e49', '\u0e32\u0e27@@', '\u0e31\u0e19@@', '\u0e2b\u0e19\u0e36', '\u0e48', '\u0e07', '\u0e41\u0e25\u0e30@@', '\u0e44\u0e14', '\u0e49', '\u0e22\u0e34\u0e19@@', '\u0e40\u0e2a\u0e35\u0e22\u0e07@@', '\u0e23', '\u0e49', '\u0e2d\u0e07@@', '\u0e14', '\u0e49', '\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21@@', '\u0e22\u0e34\u0e19@@', '\u0e14\u0e35@@', '\u0e43\u0e19@@', '\u0e1a', '\u0e49', '\u0e32\u0e19'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19',)
2021-10-03 13:44:25,956 - INFO - joeynmt.training - 	Source:     \u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19
2021-10-03 13:44:25,956 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:44:25,957 - INFO - joeynmt.training - 	Hypothesis: "I was 22 that I was just a bad cold , and I was going to fight that plate and pllip in the village ."
2021-10-03 13:44:25,957 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e1e', '\u0e48', '\u0e2d\u0e02\u0e2d\u0e07@@', '\u0e09\u0e31\u0e19@@', '\u0e01\u0e33\u0e25\u0e31\u0e07@@', '\u0e1f\u0e31\u0e07@@', '\u0e02', '\u0e48', '\u0e32\u0e27@@', '\u0e1a\u0e35@@', '\u0e1a\u0e35@@', '\u0e0b\u0e35', '\u0e08\u0e32\u0e01@@', '\u0e27\u0e34\u0e17@@', '\u0e22\u0e38@@', '\u0e2a\u0e35@@', '\u0e40\u0e17@@', '\u0e32@@', '\u0e2d\u0e31\u0e19@@', '\u0e40\u0e25', '\u0e47', '\u0e01'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 65: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01',)
2021-10-03 13:44:25,957 - INFO - joeynmt.training - 	Source:     \u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01
2021-10-03 13:44:25,957 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:44:25,957 - INFO - joeynmt.training - 	Hypothesis: My father worked a tiny bit of psychologist organization .
2021-10-03 13:44:25,957 - INFO - joeynmt.training - Validation result (greedy) at epoch  51, step     3000: bleu:   8.53, loss: 88842.3672, ppl: 319.7332, duration: 11.8639s
2021-10-03 13:44:27,706 - INFO - joeynmt.training - Epoch  51: total training loss 1290.97
2021-10-03 13:44:27,706 - INFO - joeynmt.training - EPOCH 52
2021-10-03 13:44:39,620 - INFO - joeynmt.training - Epoch  52: total training loss 1232.91
2021-10-03 13:44:39,620 - INFO - joeynmt.training - EPOCH 53
2021-10-03 13:44:51,420 - INFO - joeynmt.training - Epoch  53: total training loss 1188.52
2021-10-03 13:44:51,420 - INFO - joeynmt.training - EPOCH 54
2021-10-03 13:45:02,855 - INFO - joeynmt.training - Epoch  54: total training loss 1133.91
2021-10-03 13:45:02,856 - INFO - joeynmt.training - EPOCH 55
2021-10-03 13:45:13,990 - INFO - joeynmt.training - Epoch  55: total training loss 1093.34
2021-10-03 13:45:13,990 - INFO - joeynmt.training - EPOCH 56
2021-10-03 13:45:25,816 - INFO - joeynmt.training - Epoch  56: total training loss 1045.36
2021-10-03 13:45:25,816 - INFO - joeynmt.training - EPOCH 57
2021-10-03 13:45:37,379 - INFO - joeynmt.training - Epoch  57: total training loss 1015.31
2021-10-03 13:45:37,379 - INFO - joeynmt.training - EPOCH 58
2021-10-03 13:45:49,184 - INFO - joeynmt.training - Epoch  58: total training loss 965.08
2021-10-03 13:45:49,185 - INFO - joeynmt.training - EPOCH 59
2021-10-03 13:46:01,056 - INFO - joeynmt.training - Epoch  59: total training loss 929.56
2021-10-03 13:46:01,057 - INFO - joeynmt.training - EPOCH 60
2021-10-03 13:46:12,561 - INFO - joeynmt.training - Epoch  60: total training loss 886.47
2021-10-03 13:46:12,561 - INFO - joeynmt.training - EPOCH 61
2021-10-03 13:46:23,483 - INFO - joeynmt.training - Epoch  61: total training loss 851.48
2021-10-03 13:46:23,484 - INFO - joeynmt.training - EPOCH 62
2021-10-03 13:46:34,408 - INFO - joeynmt.training - Epoch  62: total training loss 819.83
2021-10-03 13:46:34,408 - INFO - joeynmt.training - EPOCH 63
2021-10-03 13:46:45,238 - INFO - joeynmt.training - Epoch  63: total training loss 789.76
2021-10-03 13:46:45,238 - INFO - joeynmt.training - EPOCH 64
2021-10-03 13:46:56,358 - INFO - joeynmt.training - Epoch  64: total training loss 763.98
2021-10-03 13:46:56,358 - INFO - joeynmt.training - EPOCH 65
2021-10-03 13:47:08,158 - INFO - joeynmt.training - Epoch  65: total training loss 730.75
2021-10-03 13:47:08,158 - INFO - joeynmt.training - EPOCH 66
2021-10-03 13:47:19,716 - INFO - joeynmt.training - Epoch  66: total training loss 708.83
2021-10-03 13:47:19,716 - INFO - joeynmt.training - EPOCH 67
2021-10-03 13:47:31,552 - INFO - joeynmt.training - Epoch  67: total training loss 683.85
2021-10-03 13:47:31,553 - INFO - joeynmt.training - EPOCH 68
2021-10-03 13:47:41,012 - INFO - joeynmt.training - Epoch  68, Step:     4000, Batch Loss:     9.533645, Tokens per Sec:     7899, Lr: 0.000300
2021-10-03 13:47:51,578 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:47:51,578 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:47:51,578 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:47:51,585 - INFO - joeynmt.training - Example #0
2021-10-03 13:47:51,585 - INFO - joeynmt.training - 	Source:     th
2021-10-03 13:47:51,585 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:47:51,586 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:47:51,586 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e15\u0e2d\u0e19@@', '\u0e09\u0e31\u0e19@@', '\u0e2d\u0e32\u0e22\u0e38', '1@@', '1', '\u0e09\u0e31\u0e19@@', '\u0e08\u0e33@@', '\u0e44\u0e14', '\u0e49', '\u0e27', '\u0e48', '\u0e32\u0e15@@', '\u0e37', '\u0e48', '\u0e19@@', '\u0e02\u0e36', '\u0e49', '\u0e19\u0e21\u0e32@@', '\u0e40\u0e0a', '\u0e49', '\u0e32\u0e27@@', '\u0e31\u0e19@@', '\u0e2b\u0e19\u0e36', '\u0e48', '\u0e07', '\u0e41\u0e25\u0e30@@', '\u0e44\u0e14', '\u0e49', '\u0e22\u0e34\u0e19@@', '\u0e40\u0e2a\u0e35\u0e22\u0e07@@', '\u0e23', '\u0e49', '\u0e2d\u0e07@@', '\u0e14', '\u0e49', '\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21@@', '\u0e22\u0e34\u0e19@@', '\u0e14\u0e35@@', '\u0e43\u0e19@@', '\u0e1a', '\u0e49', '\u0e32\u0e19'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19',)
2021-10-03 13:47:51,589 - INFO - joeynmt.training - 	Source:     \u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19
2021-10-03 13:47:51,590 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:47:51,590 - INFO - joeynmt.training - 	Hypothesis: "I was 27 that I was a bad cold that traditional , have been stomed in my hand , and I played the fight ."
2021-10-03 13:47:51,590 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e1e', '\u0e48', '\u0e2d\u0e02\u0e2d\u0e07@@', '\u0e09\u0e31\u0e19@@', '\u0e01\u0e33\u0e25\u0e31\u0e07@@', '\u0e1f\u0e31\u0e07@@', '\u0e02', '\u0e48', '\u0e32\u0e27@@', '\u0e1a\u0e35@@', '\u0e1a\u0e35@@', '\u0e0b\u0e35', '\u0e08\u0e32\u0e01@@', '\u0e27\u0e34\u0e17@@', '\u0e22\u0e38@@', '\u0e2a\u0e35@@', '\u0e40\u0e17@@', '\u0e32@@', '\u0e2d\u0e31\u0e19@@', '\u0e40\u0e25', '\u0e47', '\u0e01'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 65: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01',)
2021-10-03 13:47:51,591 - INFO - joeynmt.training - 	Source:     \u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01
2021-10-03 13:47:51,592 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:47:51,592 - INFO - joeynmt.training - 	Hypothesis: My father worked as a topishing of this aths .
2021-10-03 13:47:51,592 - INFO - joeynmt.training - Validation result (greedy) at epoch  68, step     4000: bleu:   8.30, loss: 101843.8594, ppl: 743.6127, duration: 10.5794s
2021-10-03 13:47:53,994 - INFO - joeynmt.training - Epoch  68: total training loss 651.21
2021-10-03 13:47:53,995 - INFO - joeynmt.training - EPOCH 69
2021-10-03 13:48:05,873 - INFO - joeynmt.training - Epoch  69: total training loss 569.76
2021-10-03 13:48:05,874 - INFO - joeynmt.training - EPOCH 70
2021-10-03 13:48:17,735 - INFO - joeynmt.training - Epoch  70: total training loss 535.49
2021-10-03 13:48:17,736 - INFO - joeynmt.training - EPOCH 71
2021-10-03 13:48:28,763 - INFO - joeynmt.training - Epoch  71: total training loss 522.98
2021-10-03 13:48:28,763 - INFO - joeynmt.training - EPOCH 72
2021-10-03 13:48:39,876 - INFO - joeynmt.training - Epoch  72: total training loss 504.89
2021-10-03 13:48:39,877 - INFO - joeynmt.training - EPOCH 73
2021-10-03 13:48:51,738 - INFO - joeynmt.training - Epoch  73: total training loss 495.87
2021-10-03 13:48:51,739 - INFO - joeynmt.training - EPOCH 74
2021-10-03 13:49:03,522 - INFO - joeynmt.training - Epoch  74: total training loss 486.91
2021-10-03 13:49:03,522 - INFO - joeynmt.training - EPOCH 75
2021-10-03 13:49:15,328 - INFO - joeynmt.training - Epoch  75: total training loss 472.87
2021-10-03 13:49:15,328 - INFO - joeynmt.training - EPOCH 76
2021-10-03 13:49:27,170 - INFO - joeynmt.training - Epoch  76: total training loss 474.61
2021-10-03 13:49:27,170 - INFO - joeynmt.training - EPOCH 77
2021-10-03 13:49:39,040 - INFO - joeynmt.training - Epoch  77: total training loss 466.47
2021-10-03 13:49:39,040 - INFO - joeynmt.training - EPOCH 78
2021-10-03 13:49:50,658 - INFO - joeynmt.training - Epoch  78: total training loss 451.82
2021-10-03 13:49:50,658 - INFO - joeynmt.training - EPOCH 79
2021-10-03 13:50:01,545 - INFO - joeynmt.training - Epoch  79: total training loss 449.38
2021-10-03 13:50:01,546 - INFO - joeynmt.training - EPOCH 80
2021-10-03 13:50:12,883 - INFO - joeynmt.training - Epoch  80: total training loss 442.70
2021-10-03 13:50:12,883 - INFO - joeynmt.training - EPOCH 81
2021-10-03 13:50:24,738 - INFO - joeynmt.training - Epoch  81: total training loss 442.34
2021-10-03 13:50:24,739 - INFO - joeynmt.training - EPOCH 82
2021-10-03 13:50:36,562 - INFO - joeynmt.training - Epoch  82: total training loss 437.04
2021-10-03 13:50:36,562 - INFO - joeynmt.training - EPOCH 83
2021-10-03 13:50:48,392 - INFO - joeynmt.training - Epoch  83: total training loss 429.62
2021-10-03 13:50:48,393 - INFO - joeynmt.training - EPOCH 84
2021-10-03 13:50:59,726 - INFO - joeynmt.training - Epoch  84: total training loss 428.49
2021-10-03 13:50:59,726 - INFO - joeynmt.training - EPOCH 85
2021-10-03 13:51:08,577 - INFO - joeynmt.training - Epoch  85, Step:     5000, Batch Loss:     7.152997, Tokens per Sec:     7887, Lr: 0.000030
2021-10-03 13:51:18,652 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:51:18,653 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:51:18,653 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:51:18,659 - INFO - joeynmt.training - Example #0
2021-10-03 13:51:18,660 - INFO - joeynmt.training - 	Source:     th
2021-10-03 13:51:18,660 - INFO - joeynmt.training - 	Reference:  en
2021-10-03 13:51:18,660 - INFO - joeynmt.training - 	Hypothesis: 
2021-10-03 13:51:18,660 - INFO - joeynmt.training - Example #1
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 72-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e15\u0e2d\u0e19@@', '\u0e09\u0e31\u0e19@@', '\u0e2d\u0e32\u0e22\u0e38', '1@@', '1', '\u0e09\u0e31\u0e19@@', '\u0e08\u0e33@@', '\u0e44\u0e14', '\u0e49', '\u0e27', '\u0e48', '\u0e32\u0e15@@', '\u0e37', '\u0e48', '\u0e19@@', '\u0e02\u0e36', '\u0e49', '\u0e19\u0e21\u0e32@@', '\u0e40\u0e0a', '\u0e49', '\u0e32\u0e27@@', '\u0e31\u0e19@@', '\u0e2b\u0e19\u0e36', '\u0e48', '\u0e07', '\u0e41\u0e25\u0e30@@', '\u0e44\u0e14', '\u0e49', '\u0e22\u0e34\u0e19@@', '\u0e40\u0e2a\u0e35\u0e22\u0e07@@', '\u0e23', '\u0e49', '\u0e2d\u0e07@@', '\u0e14', '\u0e49', '\u0e27\u0e22\u0e04\u0e27\u0e32\u0e21@@', '\u0e22\u0e34\u0e19@@', '\u0e14\u0e35@@', '\u0e43\u0e19@@', '\u0e1a', '\u0e49', '\u0e32\u0e19'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode characters in position 65-74: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19',)
2021-10-03 13:51:18,664 - INFO - joeynmt.training - 	Source:     \u0e15\u0e2d\u0e19\u0e09\u0e31\u0e19\u0e2d\u0e32\u0e22\u0e38 11 \u0e09\u0e31\u0e19\u0e08\u0e33\u0e44\u0e14 \u0e49 \u0e27 \u0e48 \u0e32\u0e15\u0e37 \u0e48 \u0e19\u0e02\u0e36 \u0e49 \u0e19\u0e21\u0e32\u0e40\u0e0a \u0e49 \u0e32\u0e27\u0e31\u0e19\u0e2b\u0e19\u0e36 \u0e48 \u0e07 \u0e41\u0e25\u0e30\u0e44\u0e14 \u0e49 \u0e22\u0e34\u0e19\u0e40\u0e2a\u0e35\u0e22\u0e07\u0e23 \u0e49 \u0e2d\u0e07\u0e14 \u0e49 \u0e27\u0e22\u0e04\u0e27\u0e32\u0e21\u0e22\u0e34\u0e19\u0e14\u0e35\u0e43\u0e19\u0e1a \u0e49 \u0e32\u0e19
2021-10-03 13:51:18,664 - INFO - joeynmt.training - 	Reference:  "When I was 11 , I remember waking up one morning to the sound of joy in my house ."
2021-10-03 13:51:18,664 - INFO - joeynmt.training - 	Hypothesis: "I was 22 that I &apos;ve been remarkable , stronomy , have been frusted in the mical and plant factor in my hand ."
2021-10-03 13:51:18,665 - INFO - joeynmt.training - Example #2
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 72: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 731, in _log_examples
    logger.debug("\tRaw source:     %s", sources_raw[p])
Message: '\tRaw source:     %s'
Arguments: (['\u0e1e', '\u0e48', '\u0e2d\u0e02\u0e2d\u0e07@@', '\u0e09\u0e31\u0e19@@', '\u0e01\u0e33\u0e25\u0e31\u0e07@@', '\u0e1f\u0e31\u0e07@@', '\u0e02', '\u0e48', '\u0e32\u0e27@@', '\u0e1a\u0e35@@', '\u0e1a\u0e35@@', '\u0e0b\u0e35', '\u0e08\u0e32\u0e01@@', '\u0e27\u0e34\u0e17@@', '\u0e22\u0e38@@', '\u0e2a\u0e35@@', '\u0e40\u0e17@@', '\u0e32@@', '\u0e2d\u0e31\u0e19@@', '\u0e40\u0e25', '\u0e47', '\u0e01'],)
--- Logging error ---
Traceback (most recent call last):
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/logging/__init__.py", line 1036, in emit
    stream.write(msg)
UnicodeEncodeError: 'latin-1' codec can't encode character '\u0e1e' in position 65: ordinal not in range(256)
Call stack:
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/sw/arch/Debian10/EB_production/2019/software/Anaconda3/2018.12/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 48, in <module>
    main()
  File "/home/lcur0008/joeynmt/joeynmt/__main__.py", line 35, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 846, in train
    trainer.train_and_validate(train_data=train_data, valid_data=dev_data)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 497, in train_and_validate
    valid_duration = self._validate(valid_data, epoch_no)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 642, in _validate
    references=valid_references)
  File "/home/lcur0008/joeynmt/joeynmt/training.py", line 737, in _log_examples
    logger.info("\tSource:     %s", sources[p])
Message: '\tSource:     %s'
Arguments: ('\u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01',)
2021-10-03 13:51:18,666 - INFO - joeynmt.training - 	Source:     \u0e1e \u0e48 \u0e2d\u0e02\u0e2d\u0e07\u0e09\u0e31\u0e19\u0e01\u0e33\u0e25\u0e31\u0e07\u0e1f\u0e31\u0e07\u0e02 \u0e48 \u0e32\u0e27\u0e1a\u0e35\u0e1a\u0e35\u0e0b\u0e35 \u0e08\u0e32\u0e01\u0e27\u0e34\u0e17\u0e22\u0e38\u0e2a\u0e35\u0e40\u0e17\u0e32\u0e2d\u0e31\u0e19\u0e40\u0e25 \u0e47 \u0e01
2021-10-03 13:51:18,666 - INFO - joeynmt.training - 	Reference:  "My father was listening to BBC News on his small , gray radio ."
2021-10-03 13:51:18,666 - INFO - joeynmt.training - 	Hypothesis: My father worked as a topposite of evolution of the mind .
2021-10-03 13:51:18,666 - INFO - joeynmt.training - Validation result (greedy) at epoch  85, step     5000: bleu:   8.25, loss: 103889.7578, ppl: 849.2355, duration: 10.0891s
2021-10-03 13:51:21,379 - INFO - joeynmt.training - Epoch  85: total training loss 425.10
2021-10-03 13:51:21,379 - INFO - joeynmt.training - EPOCH 86
2021-10-03 13:51:32,212 - INFO - joeynmt.training - Epoch  86: total training loss 418.74
2021-10-03 13:51:32,213 - INFO - joeynmt.training - EPOCH 87
2021-10-03 13:51:43,766 - INFO - joeynmt.training - Epoch  87: total training loss 416.38
2021-10-03 13:51:43,766 - INFO - joeynmt.training - EPOCH 88
2021-10-03 13:51:54,608 - INFO - joeynmt.training - Epoch  88: total training loss 406.16
2021-10-03 13:51:54,608 - INFO - joeynmt.training - EPOCH 89
2021-10-03 13:52:05,553 - INFO - joeynmt.training - Epoch  89: total training loss 409.10
2021-10-03 13:52:05,554 - INFO - joeynmt.training - EPOCH 90
2021-10-03 13:52:16,637 - INFO - joeynmt.training - Epoch  90: total training loss 404.82
2021-10-03 13:52:16,638 - INFO - joeynmt.training - EPOCH 91
2021-10-03 13:52:27,529 - INFO - joeynmt.training - Epoch  91: total training loss 400.68
2021-10-03 13:52:27,530 - INFO - joeynmt.training - EPOCH 92
2021-10-03 13:52:38,392 - INFO - joeynmt.training - Epoch  92: total training loss 394.98
2021-10-03 13:52:38,392 - INFO - joeynmt.training - EPOCH 93
2021-10-03 13:52:49,220 - INFO - joeynmt.training - Epoch  93: total training loss 395.04
2021-10-03 13:52:49,220 - INFO - joeynmt.training - EPOCH 94
2021-10-03 13:53:00,049 - INFO - joeynmt.training - Epoch  94: total training loss 385.94
2021-10-03 13:53:00,050 - INFO - joeynmt.training - EPOCH 95
2021-10-03 13:53:11,896 - INFO - joeynmt.training - Epoch  95: total training loss 388.27
2021-10-03 13:53:11,897 - INFO - joeynmt.training - EPOCH 96
2021-10-03 13:53:23,729 - INFO - joeynmt.training - Epoch  96: total training loss 383.52
2021-10-03 13:53:23,729 - INFO - joeynmt.training - EPOCH 97
2021-10-03 13:53:35,605 - INFO - joeynmt.training - Epoch  97: total training loss 379.41
2021-10-03 13:53:35,606 - INFO - joeynmt.training - EPOCH 98
2021-10-03 13:53:47,423 - INFO - joeynmt.training - Epoch  98: total training loss 379.69
2021-10-03 13:53:47,424 - INFO - joeynmt.training - EPOCH 99
2021-10-03 13:53:59,168 - INFO - joeynmt.training - Epoch  99: total training loss 371.98
2021-10-03 13:53:59,168 - INFO - joeynmt.training - EPOCH 100
2021-10-03 13:54:10,818 - INFO - joeynmt.training - Epoch 100: total training loss 373.20
2021-10-03 13:54:10,819 - INFO - joeynmt.training - Training ended after 100 epochs.
2021-10-03 13:54:10,819 - INFO - joeynmt.training - Best validation result (greedy) at step     2000:   8.69 eval_metric.
2021-10-03 13:54:10,847 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 80
2021-10-03 13:54:10,847 - INFO - joeynmt.prediction - Loading model from models/TRANS_th/2000.ckpt
2021-10-03 13:54:11,566 - INFO - joeynmt.model - Building an encoder-decoder model...
2021-10-03 13:54:12,393 - INFO - joeynmt.model - Enc-dec model built.
2021-10-03 13:54:12,544 - INFO - joeynmt.prediction - Decoding on dev set (../2DL4NLP/all_data/th.en_s/val.bpe.en_s)...
2021-10-03 13:54:35,360 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:54:35,360 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:54:35,361 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:54:35,364 - INFO - joeynmt.prediction -  dev bleu[13a]:   9.39 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:54:35,367 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_th/00002000.hyps.dev
2021-10-03 13:54:35,367 - INFO - joeynmt.prediction - Decoding on test set (../2DL4NLP/all_data/th.en_s/test.bpe.en_s)...
2021-10-03 13:55:01,781 - WARNING - sacrebleu - That's 100 lines that end in a tokenized period ('.')
2021-10-03 13:55:01,781 - WARNING - sacrebleu - It looks like you forgot to detokenize your test data, which may hurt your score.
2021-10-03 13:55:01,781 - WARNING - sacrebleu - If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.
2021-10-03 13:55:01,785 - INFO - joeynmt.prediction - test bleu[13a]:   9.39 [Beam search decoding with beam size = 5 and alpha = 1.0]
2021-10-03 13:55:01,788 - INFO - joeynmt.prediction - Translations saved to: models/TRANS_th/00002000.hyps.test
